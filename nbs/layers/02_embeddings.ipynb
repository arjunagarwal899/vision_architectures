{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp layers/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, model_validator\n",
    "from vision_architectures.utils.normalizations import get_norm_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class RelativePositionEmbeddings3DConfig(CustomBaseModel):\n",
    "    num_heads: int = Field(..., description=\"Number of query attention heads\")\n",
    "    grid_size: tuple[int, int, int]\n",
    "\n",
    "    @property\n",
    "    def num_patches(self) -> int:\n",
    "        return np.prod(self.grid_size)\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data):\n",
    "        grid_size = data.get(\"grid_size\")\n",
    "        if isinstance(data[\"grid_size\"], int):\n",
    "            data[\"grid_size\"] = (grid_size, grid_size, grid_size)\n",
    "        return data\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if isinstance(self.grid_size, int):\n",
    "            self.grid_size = (self.grid_size, self.grid_size, self.grid_size)\n",
    "        return self\n",
    "\n",
    "\n",
    "class AbsolutePositionEmbeddings3DConfig(CustomBaseModel):\n",
    "    dim: int\n",
    "    grid_size: tuple[int, int, int] | None = None\n",
    "    learnable: bool = False\n",
    "\n",
    "    @property\n",
    "    def num_patches(self) -> int:\n",
    "        return np.prod(self.grid_size)\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data):\n",
    "        if isinstance(data.get(\"grid_size\"), int):\n",
    "            data[\"grid_size\"] = (\n",
    "                data[\"grid_size\"],\n",
    "                data[\"grid_size\"],\n",
    "                data[\"grid_size\"],\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if isinstance(self.grid_size, int):\n",
    "            self.grid_size = (self.grid_size, self.grid_size, self.grid_size)\n",
    "\n",
    "        if self.learnable and self.grid_size is None:\n",
    "            raise ValueError(\"grid_size must be provided if learnable is True\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class PatchEmbeddings3DConfig(CustomBaseModel):\n",
    "    patch_size: tuple[int, int, int]\n",
    "    in_channels: int\n",
    "    dim: int\n",
    "    norm_layer: str = \"layernorm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_coords_grid(grid_size: tuple[int, int, int]) -> torch.Tensor:\n",
    "    d, h, w = grid_size\n",
    "\n",
    "    grid_d = torch.arange(d, dtype=torch.int32)\n",
    "    grid_h = torch.arange(h, dtype=torch.int32)\n",
    "    grid_w = torch.arange(w, dtype=torch.int32)\n",
    "\n",
    "    grid = torch.meshgrid(grid_d, grid_h, grid_w, indexing=\"ij\")\n",
    "    grid = torch.stack(grid, axis=0)\n",
    "    # (3, d, h, w)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class RelativePositionEmbeddings3D(nn.Module):\n",
    "    def __init__(self, config: RelativePositionEmbeddings3DConfig = {}, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = RelativePositionEmbeddings3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        num_heads = self.config.num_heads\n",
    "        grid_size = self.config.grid_size\n",
    "\n",
    "        # TODO: Add embed_spacing_info functionality\n",
    "\n",
    "        relative_limits = (\n",
    "            2 * grid_size[0] - 1,\n",
    "            2 * grid_size[1] - 1,\n",
    "            2 * grid_size[2] - 1,\n",
    "        )\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(torch.randn(num_heads, np.prod(relative_limits)))\n",
    "        # (num_heads, num_patches_z * num_patches_y * num_patches_x)\n",
    "\n",
    "        # Pair-wise relative position index for each token inside the window\n",
    "        coords = get_coords_grid(grid_size)\n",
    "        coords_flatten = rearrange(coords, \"three d h w -> three (d h w)\", three=3)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += grid_size[0] - 1\n",
    "        relative_coords[:, :, 1] += grid_size[1] - 1\n",
    "        relative_coords[:, :, 2] += grid_size[2] - 1\n",
    "        relative_position_index: torch.Tensor = (\n",
    "            relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "            + relative_coords[:, :, 1] * relative_limits[2]\n",
    "            + relative_coords[:, :, 2]\n",
    "        )\n",
    "        self.relative_position_index = relative_position_index.flatten()\n",
    "        # (num_patches, num_patches)\n",
    "\n",
    "    def forward(self):\n",
    "        relative_position_embeddings = self.relative_position_bias_table[:, self.relative_position_index]\n",
    "        # (num_heads, num_patches, num_patches)\n",
    "        relative_position_embeddings = relative_position_embeddings.reshape(\n",
    "            1, self.config.num_patches, self.config.num_patches, -1\n",
    "        )\n",
    "        # (1, num_patches, num_patches, num_heads)\n",
    "        relative_position_embeddings = relative_position_embeddings.permute(0, 3, 1, 2).contiguous()\n",
    "        # (1, num_heads, num_patches, num_patches)\n",
    "        return relative_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mRelativePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = RelativePositionEmbeddings3D(num_heads=6, grid_size=4)\n",
    "display(test)\n",
    "display(test().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class RelativePositionEmbeddings3DMetaNetwork(nn.Module):\n",
    "    def __init__(self, config: RelativePositionEmbeddings3DConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = RelativePositionEmbeddings3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        num_heads = self.config.num_heads\n",
    "        grid_size = self.config.grid_size\n",
    "\n",
    "        # TODO: Add embed_spacing_info functionality\n",
    "        self.cpb_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_heads, bias=False),\n",
    "        )\n",
    "\n",
    "        relative_limits = (\n",
    "            2 * grid_size[0] - 1,\n",
    "            2 * grid_size[1] - 1,\n",
    "            2 * grid_size[2] - 1,\n",
    "        )\n",
    "\n",
    "        # Relative coordinates table\n",
    "        relative_coords_table = get_coords_grid(relative_limits).float()\n",
    "        for i in range(3):\n",
    "            relative_coords_table[i] = (relative_coords_table[i] - (grid_size[0] - 1)) / (\n",
    "                grid_size[0] - 1 + 1e-8  # small value added to ensure there is no NaN when window size is 1\n",
    "            )\n",
    "        relative_coords_table = rearrange(\n",
    "            relative_coords_table,\n",
    "            \"three num_patches_z num_patches_y num_patches_x -> num_patches_z num_patches_y num_patches_x three\",\n",
    "        ).contiguous()\n",
    "        relative_coords_table *= 8  # Normalize to -8, 8\n",
    "        relative_coords_table = (\n",
    "            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n",
    "        )\n",
    "        # (num_patches_z, num_patches_y, num_patches_x, 3)\n",
    "        # Allow moving this to and from cuda whenever required but don't save to state_dict\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table, persistent=False)\n",
    "\n",
    "        # Pair-wise relative position index for each token inside the window\n",
    "        coords = get_coords_grid(grid_size)\n",
    "        coords_flatten = rearrange(coords, \"three d h w -> three (d h w)\", three=3)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += grid_size[0] - 1\n",
    "        relative_coords[:, :, 1] += grid_size[1] - 1\n",
    "        relative_coords[:, :, 2] += grid_size[2] - 1\n",
    "        relative_position_index: torch.Tensor = (\n",
    "            relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "            + relative_coords[:, :, 1] * relative_limits[2]\n",
    "            + relative_coords[:, :, 2]\n",
    "        )\n",
    "        self.relative_position_index = relative_position_index.flatten()\n",
    "        # (num_patches, num_patches)\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "\n",
    "    def get_relative_position_embeddings_table(self):\n",
    "        # (num_patches_z, num_patches_y, num_patches_x, 3)\n",
    "        relative_position_embeddings_table: torch.Tensor = self.cpb_mlp(self.relative_coords_table)\n",
    "        # (num_patches_z, num_patches_y, num_patches_x, num_heads)\n",
    "        relative_position_embeddings_table = relative_position_embeddings_table.reshape(-1, self.config.num_heads)\n",
    "        # (num_patches, num_heads)\n",
    "        return relative_position_embeddings_table\n",
    "\n",
    "    def forward(self):\n",
    "        relative_position_embeddings_table = self.checkpointing_level1(self.get_relative_position_embeddings_table)\n",
    "        # (num_patches, num_heads)\n",
    "        relative_position_embeddings = relative_position_embeddings_table[self.relative_position_index]\n",
    "        # (num_patches * num_patches, num_heads)\n",
    "        relative_position_embeddings = rearrange(\n",
    "            relative_position_embeddings,\n",
    "            \"(num_patches1 num_patches2) num_heads -> num_heads num_patches1 num_patches2\",\n",
    "            num_patches1=self.config.num_patches,\n",
    "            num_patches2=self.config.num_patches,\n",
    "            num_heads=self.config.num_heads,\n",
    "        ).contiguous()\n",
    "        # (num_heads, num_patches, num_patches)\n",
    "        relative_position_embeddings = 16 * torch.sigmoid(relative_position_embeddings)\n",
    "        # (num_heads, num_patches, num_patches)\n",
    "        return relative_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mRelativePositionEmbeddings3DMetaNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcpb_mlp\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33minplace\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m6\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = RelativePositionEmbeddings3DMetaNetwork(num_heads=6, grid_size=(4, 4, 4))\n",
    "display(test)\n",
    "display(test().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "RelativePositionEmbeddings = Union[RelativePositionEmbeddings3D, RelativePositionEmbeddings3DMetaNetwork]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_absolute_position_embeddings_3d(\n",
    "    dim: int,\n",
    "    grid_size: tuple[int, int, int],\n",
    "    spacing: tuple[float, float, float] = (1.0, 1.0, 1.0),\n",
    "    # Used if the embeddings required are of a crop of a larger image\n",
    "    full_image_size: tuple[int, int, int] = None,\n",
    "    crop_center: tuple[int, int, int] = None,\n",
    ") -> torch.Tensor:\n",
    "    if dim % 6 != 0:\n",
    "        raise ValueError(\"embed_dim must be divisible by 6\")\n",
    "\n",
    "    if full_image_size is None:\n",
    "        full_image_size = grid_size\n",
    "\n",
    "    grid = get_coords_grid(grid_size)\n",
    "    # (3, d, h, w)\n",
    "\n",
    "    # Apply offset if crop parameters are provided\n",
    "    if crop_center is not None:\n",
    "        # Calculate crop boundaries\n",
    "        crop_start = tuple(max(0, center - size // 2) for center, size in zip(crop_center, grid_size))\n",
    "\n",
    "        # Offset the grid coordinates to represent their position in the full volume\n",
    "        for i in range(3):\n",
    "            grid[i] = grid[i] + crop_start[i]\n",
    "\n",
    "    grid = rearrange(grid, \"x d h w -> x 1 d h w\")\n",
    "    # (3, 1, d, h, w)\n",
    "\n",
    "    omega = torch.arange(dim // 6, dtype=torch.float32)\n",
    "    omega /= dim / 6.0\n",
    "    omega = 1.0 / 10000**omega\n",
    "    # (dim // 6)\n",
    "\n",
    "    patch_multiplier = torch.Tensor(spacing) / min(spacing)\n",
    "\n",
    "    position_embeddings = []\n",
    "    for i, grid_subset in enumerate(grid):\n",
    "        grid_subset = grid_subset.reshape(-1)\n",
    "\n",
    "        out = torch.einsum(\"m,d->md\", grid_subset, omega)\n",
    "\n",
    "        emb_sin = torch.sin(out)\n",
    "        emb_cos = torch.cos(out)\n",
    "\n",
    "        emb = torch.cat([emb_sin, emb_cos], axis=1) * patch_multiplier[i]\n",
    "        position_embeddings.append(emb)\n",
    "\n",
    "    position_embeddings = torch.cat(position_embeddings, axis=1)\n",
    "    # (dim, d * h * w)\n",
    "    position_embeddings = rearrange(\n",
    "        position_embeddings,\n",
    "        \"(d h w) e -> 1 e d h w\",\n",
    "        d=grid_size[0],\n",
    "        h=grid_size[1],\n",
    "        w=grid_size[2],\n",
    "    )\n",
    "    # (1, dim, d, h, w)\n",
    "\n",
    "    return position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AbsolutePositionEmbeddings3D(nn.Module):\n",
    "    def __init__(self, config: AbsolutePositionEmbeddings3DConfig = {}, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AbsolutePositionEmbeddings3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        grid_size = self.config.grid_size\n",
    "        learnable = self.config.learnable\n",
    "\n",
    "        # TODO: Add getting absolute position embeddings of subsection of the grid\n",
    "\n",
    "        self.position_embeddings_cache = {}\n",
    "        self.position_embeddings = None\n",
    "        if grid_size is not None:\n",
    "            self.position_embeddings_cache[grid_size] = get_absolute_position_embeddings_3d(dim, grid_size)\n",
    "            self.position_embeddings = nn.Parameter(self.position_embeddings_cache[grid_size], requires_grad=learnable)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_size=None,\n",
    "        grid_size=None,\n",
    "        spacings: torch.Tensor = None,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        # Used if the embeddings required are of a crop of a larger image\n",
    "        full_image_size: tuple[int, int, int] = None,\n",
    "        crop_center: tuple[int, int, int] = None,\n",
    "    ):\n",
    "        assert self.position_embeddings is not None or grid_size is not None, \"grid_size must be provided\"\n",
    "        assert batch_size is not None or spacings is not None, \"Either batch_size or spacings must be provided\"\n",
    "\n",
    "        if isinstance(grid_size, int):\n",
    "            grid_size = (grid_size, grid_size, grid_size)\n",
    "\n",
    "        if self.position_embeddings is not None:\n",
    "            position_embeddings = self.position_embeddings\n",
    "        else:\n",
    "            cache_key = (grid_size, full_image_size, crop_center)\n",
    "            if cache_key not in self.position_embeddings_cache:\n",
    "                self.position_embeddings_cache[cache_key] = get_absolute_position_embeddings_3d(\n",
    "                    self.config.dim, grid_size, full_image_size=full_image_size, crop_center=crop_center\n",
    "                )\n",
    "            position_embeddings = self.position_embeddings_cache[cache_key].to(device)\n",
    "        # (1, dim, d, h, w)\n",
    "\n",
    "        if batch_size is not None:\n",
    "            b = batch_size\n",
    "        else:\n",
    "            assert spacings.ndim == 2 and spacings.shape[1] == 3, \"spacings must be of shape (batch_size, 3)\"\n",
    "            assert self.config.dim % 3 == 0, \"embed_dim must be divisible by 3\"\n",
    "\n",
    "            b = spacings.shape[0]\n",
    "\n",
    "        position_embeddings = repeat(position_embeddings, \"1 e d h w -> b e d h w\", b=b)\n",
    "\n",
    "        if spacings is not None:\n",
    "            # (b, 3)\n",
    "            spacings = repeat(\n",
    "                spacings,\n",
    "                \"b three -> b (three dim_by_three) 1 1 1\",\n",
    "                three=3,\n",
    "                dim_by_three=self.config.dim // 3,\n",
    "            )\n",
    "            # (b, dim, 1, 1, 1)\n",
    "\n",
    "            position_embeddings = position_embeddings * spacings.to(position_embeddings.device)\n",
    "            # (b, dim, d, h, w)\n",
    "\n",
    "        return position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = AbsolutePositionEmbeddings3D(dim=6, grid_size=4, learnable=True)\n",
    "display(test)\n",
    "display(test(batch_size=2).shape)\n",
    "display(test(spacings=torch.randn(4, 3)).shape)\n",
    "\n",
    "test = AbsolutePositionEmbeddings3D(dim=6)\n",
    "display(test(grid_size=(3, 3, 3), batch_size=2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class PatchEmbeddings3D(nn.Module):\n",
    "    def __init__(self, config: PatchEmbeddings3DConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = PatchEmbeddings3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        patch_size = self.config.patch_size\n",
    "        in_channels = self.config.in_channels\n",
    "        dim = self.config.dim\n",
    "        norm_layer = self.config.norm_layer\n",
    "\n",
    "        self.patch_embeddings = nn.Conv3d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "        if norm_layer is None:\n",
    "            self.normalization = nn.Identity()\n",
    "        elif isinstance(norm_layer, nn.Module):\n",
    "            self.normalization = norm_layer(dim)\n",
    "        else:\n",
    "            self.normalization = get_norm_layer(norm_layer, dim)\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "\n",
    "    def _forward(self, pixel_values: torch.Tensor):\n",
    "        # pixel_values: (b, c, z, y, x)\n",
    "\n",
    "        embeddings = self.patch_embeddings(pixel_values)\n",
    "        # (b, dim, num_patches_z, num_patches_y, num_patches_x)\n",
    "        embeddings = rearrange(embeddings, \"b d z y x -> b z y x d\")\n",
    "        # (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "        embeddings = self.normalization(embeddings)\n",
    "        # (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "        embeddings = rearrange(embeddings, \"b z y x d -> b d z y x\")\n",
    "        # (b, dim, num_patches_z, num_patches_y, num_patches_x)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor):\n",
    "        return self.checkpointing_level1(self._forward, pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPatchEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpatch_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnormalization\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m12\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = PatchEmbeddings3D(patch_size=(1, 8, 8), in_channels=1, dim=12)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 1, 32, 512, 512))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "coords_h = torch.arange(4)\n",
    "coords_w = torch.arange(4)\n",
    "coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "coords_flatten = torch.flatten(coords, 1)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += 4 - 1\n",
    "relative_coords[:, :, 1] += 4 - 1\n",
    "relative_coords[:, :, 0] *= 2 * 4 - 1\n",
    "relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "window_size = (4, 4, 4)\n",
    "relative_limits = (7, 7, 7)\n",
    "\n",
    "coords = get_coords_grid(window_size)\n",
    "coords_flatten = rearrange(coords, \"three_dimensional d h w -> three_dimensional (d h w)\", three_dimensional=3)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += window_size[0] - 1\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "relative_coords[:, :, 2] += window_size[2] - 1\n",
    "relative_position_index: torch.Tensor = (\n",
    "    relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 2]\n",
    ")\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "window_size = (2, 2, 2)\n",
    "relative_limits = (2 * window_size[0] - 1, 2 * window_size[1] - 1, 2 * window_size[2] - 1)\n",
    "\n",
    "relative_coords_table = get_coords_grid(relative_limits)\n",
    "relative_coords_table[0] -= window_size[0] - 1\n",
    "relative_coords_table[1] -= window_size[1] - 1\n",
    "relative_coords_table[2] -= window_size[2] - 1\n",
    "relative_coords_table = relative_coords_table.permute(1, 2, 3, 0).contiguous()\n",
    "relative_coords_table[0, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
