{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp layers/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Literal, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.blocks.cnn import CNNBlock3D, CNNBlockConfig\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, model_validator\n",
    "from vision_architectures.utils.rearrange import rearrange_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class RelativePositionEmbeddings3DConfig(CustomBaseModel):\n",
    "    num_heads: int = Field(..., description=\"Number of query attention heads\")\n",
    "    grid_size: tuple[int, int, int] = Field(..., description=\"Size of entire patch matrix.\")\n",
    "\n",
    "    @property\n",
    "    def num_patches(self) -> int:\n",
    "        \"\"\"Number of patches.\"\"\"\n",
    "        return np.prod(self.grid_size)\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data):\n",
    "        grid_size = data.get(\"grid_size\")\n",
    "        if isinstance(data[\"grid_size\"], int):\n",
    "            data[\"grid_size\"] = (grid_size, grid_size, grid_size)\n",
    "        return data\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if isinstance(self.grid_size, int):\n",
    "            self.grid_size = (self.grid_size, self.grid_size, self.grid_size)\n",
    "        return self\n",
    "\n",
    "\n",
    "class AbsolutePositionEmbeddings3DConfig(CustomBaseModel):\n",
    "    dim: int | None = Field(None, description=\"Dimension of the position embeddings\")\n",
    "    grid_size: tuple[int, int, int] | None = Field(None, description=\"Size of entire patch matrix.\")\n",
    "    learnable: bool = Field(False, description=\"Whether the position embeddings are learnable.\")\n",
    "\n",
    "    @property\n",
    "    def num_patches(self) -> int:\n",
    "        \"\"\"Number of patches.\"\"\"\n",
    "        if self.grid_size is None:\n",
    "            return None\n",
    "        return np.prod(self.grid_size)\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data):\n",
    "        if isinstance(data.get(\"grid_size\"), int):\n",
    "            data[\"grid_size\"] = (\n",
    "                data[\"grid_size\"],\n",
    "                data[\"grid_size\"],\n",
    "                data[\"grid_size\"],\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if self.learnable and (self.dim is None or self.grid_size is None):\n",
    "            raise ValueError(\"dim and grid_size must be provided if learnable is True\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class AbsolutePositionEmbeddings1DConfig(CustomBaseModel):\n",
    "    dim: int | None = Field(None, description=\"Dimension of the position embeddings\")\n",
    "    length: int | None = Field(None, description=\"Length of the sequence.\")\n",
    "    learnable: bool = Field(False, description=\"Whether the position embeddings are learnable.\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if self.learnable and (self.dim is None or self.length is None):\n",
    "            raise ValueError(\"dim and length must be provided if learnable is True\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class PatchEmbeddings3DConfig(CNNBlockConfig):\n",
    "    patch_size: tuple[int, int, int] = Field(..., description=\"Size of the patches to extract from the input.\")\n",
    "    in_channels: int = Field(..., description=\"Number of input channels.\")\n",
    "    dim: int = Field(..., description=\"Dimension of the embeddings.\")\n",
    "    norm_layer: str = Field(\"layernorm\", description=\"Normalization layer to use.\")\n",
    "\n",
    "    out_channels: None = None\n",
    "    kernel_size: None = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data: dict):\n",
    "        data.setdefault(\"patch_size\", data.pop(\"kernel_size\", None))\n",
    "        data.setdefault(\"dim\", data.pop(\"out_channels\", None))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_coords_grid(grid_size: tuple[int, int, int]) -> torch.Tensor:\n",
    "    \"\"\"Get a coordinate grid of shape (3, d, h, w) for a given grid size.\n",
    "\n",
    "    Args:\n",
    "        grid_size: Size of the grid (d, h, w).\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (3, d, h, w) containing the coordinates.\n",
    "    \"\"\"\n",
    "    d, h, w = grid_size\n",
    "\n",
    "    grid_d = torch.arange(d, dtype=torch.int32)\n",
    "    grid_h = torch.arange(h, dtype=torch.int32)\n",
    "    grid_w = torch.arange(w, dtype=torch.int32)\n",
    "\n",
    "    grid = torch.meshgrid(grid_d, grid_h, grid_w, indexing=\"ij\")\n",
    "    grid = torch.stack(grid, axis=0)\n",
    "    # (3, d, h, w)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class RelativePositionEmbeddings3D(nn.Module):\n",
    "    \"\"\"Learnable 3D Relative Position Embeddings. This can be passed directly to the attention layers.\n",
    "    {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: RelativePositionEmbeddings3DConfig = {}, **kwargs):\n",
    "        \"\"\"Initialize RelativePositionEmbeddings3D.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = RelativePositionEmbeddings3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        num_heads = self.config.num_heads\n",
    "        grid_size = self.config.grid_size\n",
    "\n",
    "        # TODO: Add embed_spacing_info functionality\n",
    "\n",
    "        relative_limits = (\n",
    "            2 * grid_size[0] - 1,\n",
    "            2 * grid_size[1] - 1,\n",
    "            2 * grid_size[2] - 1,\n",
    "        )\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(torch.randn(num_heads, np.prod(relative_limits)))\n",
    "        # (num_heads, num_patches_z * num_patches_y * num_patches_x)\n",
    "\n",
    "        # Pair-wise relative position index for each token inside the window\n",
    "        coords = get_coords_grid(grid_size)\n",
    "        coords_flatten = rearrange(coords, \"three d h w -> three (d h w)\", three=3).contiguous()\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += grid_size[0] - 1\n",
    "        relative_coords[:, :, 1] += grid_size[1] - 1\n",
    "        relative_coords[:, :, 2] += grid_size[2] - 1\n",
    "        relative_position_index: torch.Tensor = (\n",
    "            relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "            + relative_coords[:, :, 1] * relative_limits[2]\n",
    "            + relative_coords[:, :, 2]\n",
    "        )\n",
    "        self.relative_position_index = relative_position_index.flatten()\n",
    "        # (num_patches, num_patches)\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        \"\"\"Get relative position embeddings as specified by the config.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (1, num_heads, num_patches, num_patches) containing the relative position embeddings.\n",
    "        \"\"\"\n",
    "        relative_position_embeddings = self.relative_position_bias_table[:, self.relative_position_index].contiguous()\n",
    "        # (num_heads, num_patches, num_patches)\n",
    "        relative_position_embeddings = relative_position_embeddings.reshape(\n",
    "            1, self.config.num_patches, self.config.num_patches, -1\n",
    "        )\n",
    "        # (1, num_patches, num_patches, num_heads)\n",
    "        relative_position_embeddings = rearrange(\n",
    "            relative_position_embeddings,\n",
    "            \"1 num_patches1 num_patches2 num_heads -> 1 num_heads num_patches1 num_patches2\",\n",
    "        ).contiguous()\n",
    "        # (1, num_heads, num_patches, num_patches)\n",
    "        return relative_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mRelativePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = RelativePositionEmbeddings3D(num_heads=6, grid_size=4)\n",
    "display(test)\n",
    "display(test().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class RelativePositionEmbeddings3DMetaNetwork(nn.Module):\n",
    "    \"\"\"3D Relative Position Embeddings obtained from a meta network (inspired by SwinV2). This can be passed directly\n",
    "    to the attention layers. {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: RelativePositionEmbeddings3DConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize RelativePositionEmbeddings3DMetaNetwork.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = RelativePositionEmbeddings3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        num_heads = self.config.num_heads\n",
    "        grid_size = self.config.grid_size\n",
    "\n",
    "        # TODO: Add embed_spacing_info functionality\n",
    "        self.cpb_mlp = nn.Sequential(\n",
    "            nn.Linear(3, 512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_heads, bias=False),\n",
    "        )\n",
    "\n",
    "        relative_limits = (\n",
    "            2 * grid_size[0] - 1,\n",
    "            2 * grid_size[1] - 1,\n",
    "            2 * grid_size[2] - 1,\n",
    "        )\n",
    "\n",
    "        # Relative coordinates table\n",
    "        relative_coords_table = get_coords_grid(relative_limits).float()\n",
    "        for i in range(3):\n",
    "            relative_coords_table[i] = (relative_coords_table[i] - (grid_size[0] - 1)) / (\n",
    "                grid_size[0] - 1 + 1e-5  # small value added to ensure there is no NaN when window size is 1\n",
    "            )\n",
    "        relative_coords_table = rearrange(\n",
    "            relative_coords_table,\n",
    "            \"three num_patches_z num_patches_y num_patches_x -> num_patches_z num_patches_y num_patches_x three\",\n",
    "        ).contiguous()\n",
    "        relative_coords_table *= 8  # Normalize to -8, 8\n",
    "        relative_coords_table = (\n",
    "            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n",
    "        )\n",
    "        # (num_patches_z, num_patches_y, num_patches_x, 3)\n",
    "        # Allow moving this to and from cuda whenever required but don't save to state_dict\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table, persistent=False)\n",
    "\n",
    "        # Pair-wise relative position index for each token inside the window\n",
    "        coords = get_coords_grid(grid_size)\n",
    "        coords_flatten = rearrange(coords, \"three d h w -> three (d h w)\", three=3).contiguous()\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = rearrange(\n",
    "            relative_coords, \"three num_patches1 num_patches2 -> num_patches1 num_patches2 three\"\n",
    "        ).contiguous()\n",
    "        relative_coords[:, :, 0] += grid_size[0] - 1\n",
    "        relative_coords[:, :, 1] += grid_size[1] - 1\n",
    "        relative_coords[:, :, 2] += grid_size[2] - 1\n",
    "        relative_position_index: torch.Tensor = (\n",
    "            relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "            + relative_coords[:, :, 1] * relative_limits[2]\n",
    "            + relative_coords[:, :, 2]\n",
    "        )\n",
    "        self.relative_position_index = relative_position_index.flatten()\n",
    "        # (num_patches, num_patches)\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "\n",
    "    def get_relative_position_embeddings_table(self) -> torch.Tensor:\n",
    "        \"\"\"Get the relative position embeddings table from the meta network.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (num_patches, num_heads) containing the relative position embeddings table.\n",
    "        \"\"\"\n",
    "        # (num_patches_z, num_patches_y, num_patches_x, 3)\n",
    "        relative_position_embeddings_table: torch.Tensor = self.cpb_mlp(self.relative_coords_table)\n",
    "        # (num_patches_z, num_patches_y, num_patches_x, num_heads)\n",
    "        relative_position_embeddings_table = relative_position_embeddings_table.reshape(-1, self.config.num_heads)\n",
    "        # (num_patches, num_heads)\n",
    "        return relative_position_embeddings_table\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        \"\"\"Get relative position embeddings as specified by the config.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (num_heads, num_patches, num_patches) containing the relative position embeddings.\n",
    "        \"\"\"\n",
    "        relative_position_embeddings_table = self.checkpointing_level1(self.get_relative_position_embeddings_table)\n",
    "        # (num_patches, num_heads)\n",
    "        relative_position_embeddings = relative_position_embeddings_table[self.relative_position_index]\n",
    "        # (num_patches * num_patches, num_heads)\n",
    "        relative_position_embeddings = rearrange(\n",
    "            relative_position_embeddings,\n",
    "            \"(num_patches1 num_patches2) num_heads -> num_heads num_patches1 num_patches2\",\n",
    "            num_patches1=self.config.num_patches,\n",
    "            num_patches2=self.config.num_patches,\n",
    "            num_heads=self.config.num_heads,\n",
    "        ).contiguous()\n",
    "        # (num_heads, num_patches, num_patches)\n",
    "        relative_position_embeddings = 16 * torch.sigmoid(relative_position_embeddings)\n",
    "        # (num_heads, num_patches, num_patches)\n",
    "        return relative_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mRelativePositionEmbeddings3DMetaNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcpb_mlp\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33minplace\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m6\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = RelativePositionEmbeddings3DMetaNetwork(num_heads=6, grid_size=(4, 4, 4))\n",
    "display(test)\n",
    "display(test().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "RelativePositionEmbeddings = Union[RelativePositionEmbeddings3D, RelativePositionEmbeddings3DMetaNetwork]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "def get_sinusoidal_embeddings_3d(\n",
    "    dim: int,\n",
    "    grid_size: tuple[int, int, int],\n",
    "    spacing: tuple[float, float, float] = (1.0, 1.0, 1.0),\n",
    "    crop_offset: tuple[int, int, int] = None,\n",
    "    channels_first: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Get 3D sinusoidal position embeddings.\n",
    "\n",
    "    Args:\n",
    "        dim: Embedding dimension. Must be divisible by 6.\n",
    "        grid_size: Size of the patch grid (d, h, w).\n",
    "        spacing: Spacing between patches in each dimension. Useful for medical images.\n",
    "        crop_offset: Used if the embeddings required are of a crop of a larger image. If provided, the grid coordinates\n",
    "            will be offset accordingly.\n",
    "        channels_first: {CHANNELS_FIRST_DOC}\n",
    "\n",
    "    Returns:\n",
    "        {OUTPUT_3D_DOC}\n",
    "    \"\"\"\n",
    "    if dim % 6 != 0:\n",
    "        raise ValueError(\"dim must be divisible by 6\")\n",
    "\n",
    "    grid = get_coords_grid(grid_size)\n",
    "    # (3, d, h, w)\n",
    "\n",
    "    # Apply offset if crop parameters are provided\n",
    "    if crop_offset is not None:\n",
    "        # Offset the grid coordinates to represent their position in the full volume\n",
    "        for i in range(3):\n",
    "            grid[i] = grid[i] + crop_offset[i]\n",
    "\n",
    "    grid = rearrange(grid, \"x d h w -> x 1 d h w\").contiguous()\n",
    "    # (3, 1, d, h, w)\n",
    "\n",
    "    omega = torch.arange(dim // 6, dtype=torch.float32)\n",
    "    omega /= dim / 6.0\n",
    "    omega = 1.0 / (10000**omega)\n",
    "    # (dim // 6)\n",
    "\n",
    "    patch_multiplier = torch.Tensor(spacing) / min(spacing)\n",
    "\n",
    "    embeddings = []\n",
    "    for i, grid_subset in enumerate(grid):\n",
    "        grid_subset = grid_subset.reshape(-1)\n",
    "\n",
    "        out = torch.einsum(\"m,d->md\", grid_subset, omega)\n",
    "\n",
    "        emb_sin = torch.sin(out)\n",
    "        emb_cos = torch.cos(out)\n",
    "\n",
    "        emb = torch.cat([emb_sin, emb_cos], axis=1) * patch_multiplier[i]\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    embeddings = torch.cat(embeddings, axis=1)\n",
    "    # (dim, d * h * w)\n",
    "    embeddings = rearrange(\n",
    "        embeddings,\n",
    "        \"(d h w) e -> 1 e d h w\",\n",
    "        d=grid_size[0],\n",
    "        h=grid_size[1],\n",
    "        w=grid_size[2],\n",
    "    ).contiguous()\n",
    "    # (1, dim, d, h, w)\n",
    "\n",
    "    embeddings = rearrange_channels(embeddings, True, channels_first)\n",
    "    # (1, [dim], d, h, w, [dim])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "get_absolute_position_embeddings_3d = get_sinusoidal_embeddings_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class AbsolutePositionEmbeddings3D(nn.Module):\n",
    "    \"\"\"3D Absolute Position Embeddings. May or may not learnable. These have to be applied on the input manually and\n",
    "    cannot be passed to attention layers directly. {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: AbsolutePositionEmbeddings3DConfig = {}, **kwargs):\n",
    "        \"\"\"Initialize AbsolutePositionEmbeddings3D.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AbsolutePositionEmbeddings3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        grid_size = self.config.grid_size\n",
    "        learnable = self.config.learnable\n",
    "\n",
    "        self.position_embeddings_cache = {}\n",
    "        self.position_embeddings = None\n",
    "        if dim is not None and grid_size is not None:\n",
    "            self.position_embeddings = nn.Parameter(\n",
    "                get_absolute_position_embeddings_3d(dim, grid_size), requires_grad=learnable\n",
    "            )\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        embedding_type: Literal[\"add\", \"concat\"] = \"add\",\n",
    "        spacings: torch.Tensor = None,\n",
    "        channels_first: bool = True,\n",
    "        crop_offsets: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Apply absolute position embeddings to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x: {INPUT_3D_DOC}\n",
    "            embedding_type: Type of embedding to apply. 'add' to add the position embeddings to the input,\n",
    "                'concat' to concatenate them along the channel dimension.\n",
    "            spacings: {SPACINGS_DOC}\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "            crop_offsets: Used if the embeddings required are of a crop of a larger image. If provided, the grid\n",
    "                coordinates will be offset accordingly.\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_3D_DOC}\n",
    "        \"\"\"\n",
    "        assert x.ndim == 5, \"Input tensor must be of shape (b, [d], z, y, x, [d])\"\n",
    "        # Check if sufficient information has been provided\n",
    "        if self.position_embeddings is None:\n",
    "            dim = self.config.dim\n",
    "            if dim is None:\n",
    "                dim = x.shape[1] if channels_first else x.shape[-1]\n",
    "            grid_size = self.config.grid_size\n",
    "            if grid_size is None:\n",
    "                grid_size = tuple(x.shape[2:5] if channels_first else x.shape[1:4])\n",
    "        else:\n",
    "            dim = self.config.dim\n",
    "            grid_size = self.config.grid_size\n",
    "\n",
    "        # Estimate batch size\n",
    "        b = x.shape[0]\n",
    "\n",
    "        # Get position embeddings, adjust based on crop offsets if applicable\n",
    "        if self.position_embeddings is not None:\n",
    "            position_embeddings = rearrange_channels(self.position_embeddings, True, channels_first)\n",
    "            position_embeddings = repeat(position_embeddings, \"1 ... -> b ...\", b=b)\n",
    "        else:\n",
    "            if isinstance(grid_size, int):\n",
    "                grid_size = (grid_size, grid_size, grid_size)\n",
    "\n",
    "            if crop_offsets is None:\n",
    "                cache_key = (dim, grid_size, None)\n",
    "                if cache_key not in self.position_embeddings_cache:\n",
    "                    self.position_embeddings_cache[cache_key] = get_absolute_position_embeddings_3d(\n",
    "                        dim, grid_size, channels_first=channels_first\n",
    "                    )\n",
    "                position_embeddings = self.position_embeddings_cache[cache_key]\n",
    "                position_embeddings = repeat(position_embeddings, \"1 ... -> b ...\", b=b)\n",
    "            else:\n",
    "                if crop_offsets.ndim == 1:\n",
    "                    crop_offsets = crop_offsets.unsqueeze(0)\n",
    "\n",
    "                position_embeddings = []\n",
    "                for crop_offset in crop_offsets:\n",
    "                    position_embeddings.append(\n",
    "                        get_absolute_position_embeddings_3d(\n",
    "                            dim, grid_size, crop_offset=crop_offset.tolist(), channels_first=channels_first\n",
    "                        )\n",
    "                    )\n",
    "                position_embeddings = torch.cat(position_embeddings, dim=0)\n",
    "            position_embeddings = position_embeddings.to(x.device)\n",
    "        # (b, [dim], d, h, w, [dim])\n",
    "\n",
    "        # Incorporate spacing information\n",
    "        if spacings is not None:\n",
    "            assert spacings.shape == (b, 3), \"spacings must be of shape (batch_size, 3)\"\n",
    "            assert dim % 3 == 0, \"dim must be divisible by 3\"\n",
    "            # (b, 3)\n",
    "            spacings = repeat(spacings, \"b three -> b (three dim_by_three) 1 1 1\", three=3, dim_by_three=dim // 3)\n",
    "            # (b, dim, 1, 1, 1)\n",
    "            spacings = rearrange_channels(spacings, True, channels_first)\n",
    "            # (b, [dim], 1, 1, 1, [dim])\n",
    "\n",
    "            position_embeddings = position_embeddings * spacings.to(position_embeddings.device)\n",
    "            # (b, [dim], d, h, w, [dim])\n",
    "\n",
    "        if embedding_type == \"add\":\n",
    "            x = x + position_embeddings\n",
    "        elif embedding_type == \"concat\":\n",
    "            x = torch.cat([x, position_embeddings], dim=1 if channels_first else -1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only 'add' and 'concat' are supported for embedding_type\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_input = torch.randn(2, 6, 4, 4, 4)\n",
    "\n",
    "test = AbsolutePositionEmbeddings3D(dim=6, grid_size=4, learnable=True)\n",
    "display(test)\n",
    "display(test(sample_input).shape)\n",
    "display(test(sample_input, spacings=torch.randn(2, 3)).shape)\n",
    "\n",
    "test = AbsolutePositionEmbeddings3D(dim=6)\n",
    "display(test(sample_input).shape)\n",
    "\n",
    "test = AbsolutePositionEmbeddings3D()\n",
    "display(test(sample_input, crop_offsets=torch.Tensor([(0, 0, 0), (10, 10, 10)])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_input = torch.randn(2, 4, 4, 4, 6)\n",
    "\n",
    "test = AbsolutePositionEmbeddings3D(dim=6, grid_size=4, learnable=True)\n",
    "display(test)\n",
    "display(test(sample_input, channels_first=False).shape)\n",
    "\n",
    "test = AbsolutePositionEmbeddings3D(dim=6)\n",
    "display(test(sample_input, channels_first=False).shape)\n",
    "\n",
    "test = AbsolutePositionEmbeddings3D()\n",
    "display(test(sample_input, channels_first=False).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_specific_sinusoidal_embeddings_1d(dim: int, indices: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Get 1D sinusoidal position embeddings for specific indices.\n",
    "\n",
    "    Args:\n",
    "        dim: Embedding dimension. Must be divisible by 2.\n",
    "        indices: Indices for which to get the embeddings. Shape: (length,).\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (1, length, dim) containing the position embeddings.\n",
    "    \"\"\"\n",
    "    if dim % 2 != 0:\n",
    "        raise ValueError(\"dim must be divisible by 2\")\n",
    "\n",
    "    # Create frequency bands\n",
    "    omega = torch.arange(dim // 2, dtype=torch.float32, device=indices.device)\n",
    "    omega /= dim / 2.0\n",
    "    omega = 1.0 / (10000**omega)\n",
    "    # (dim // 2)\n",
    "\n",
    "    # Outer product of positions / timesteps and frequencies\n",
    "    out = torch.einsum(\"n,d->nd\", indices, omega)\n",
    "    # (length, dim//2)\n",
    "\n",
    "    # Apply sin and cos functions\n",
    "    emb_sin = torch.sin(out)\n",
    "    emb_cos = torch.cos(out)\n",
    "\n",
    "    # Interleave sin and cos embeddings\n",
    "    embeddings = torch.stack([emb_sin, emb_cos], dim=2)\n",
    "    embeddings = embeddings.flatten(1)\n",
    "    # (length, dim)\n",
    "\n",
    "    # Reshape to expected output format\n",
    "    embeddings = rearrange(embeddings, \"length dim -> 1 length dim\").contiguous()\n",
    "    # (1, length, dim)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_sinusoidal_embeddings_1d(dim: int, length: int, device=torch.device(\"cpu\")) -> torch.Tensor:\n",
    "    \"\"\"Get 1D sinusoidal position embeddings.\n",
    "\n",
    "    Args:\n",
    "        dim: Embedding dimension. Must be divisible by 2.\n",
    "        length: Length of the sequence.\n",
    "        device: Device to create the embeddings on.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (1, length, dim) containing the position embeddings.\n",
    "    \"\"\"\n",
    "    # Create position / timestep indices\n",
    "    indices = torch.arange(length, dtype=torch.int32, device=device)\n",
    "    # (length,)\n",
    "\n",
    "    return get_specific_sinusoidal_embeddings_1d(dim, indices)\n",
    "\n",
    "\n",
    "get_timestep_embeddings_1d = get_specific_sinusoidal_embeddings_1d\n",
    "get_all_timestep_embeddings_1d = get_sinusoidal_embeddings_1d\n",
    "get_absolute_position_embeddings_1d = get_sinusoidal_embeddings_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_timestep_embeddings_1d(2, torch.tensor([1, 5, 11])).shape, get_all_timestep_embeddings_1d(2, 10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class AbsolutePositionEmbeddings1D(nn.Module):\n",
    "    \"\"\"1D Absolute Position Embeddings. May or may not learnable. These have to be applied on the input manually and\n",
    "    cannot be passed to attention layers directly. {CLASS_DESCRIPTION_1D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: AbsolutePositionEmbeddings1DConfig = {}, **kwargs):\n",
    "        \"\"\"Initialize AbsolutePositionEmbeddings1D.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AbsolutePositionEmbeddings1DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        length = self.config.length\n",
    "        learnable = self.config.learnable\n",
    "\n",
    "        self.position_embeddings_cache = {}\n",
    "        self.position_embeddings = None\n",
    "        if dim is not None and length is not None:\n",
    "            self.position_embeddings = nn.Parameter(\n",
    "                get_absolute_position_embeddings_1d(dim, length), requires_grad=learnable\n",
    "            )\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        embedding_type: Literal[\"add\", \"concat\"] = \"add\",\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Apply absolute position embeddings to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x: {INPUT_1D_DOC}\n",
    "            embedding_type: Type of embedding to apply. 'add' to add the position embeddings to the input,\n",
    "                'concat' to concatenate them along the last dimension.\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_1D_DOC}\n",
    "        \"\"\"\n",
    "        assert x.ndim == 3, \"Input tensor must be of shape (b, length, dim)\"\n",
    "        # Check if sufficient information has been provided\n",
    "        if self.position_embeddings is None:\n",
    "            dim = self.config.dim\n",
    "            if dim is None:\n",
    "                dim = x.shape[2]\n",
    "            length = self.config.length\n",
    "            if length is None:\n",
    "                length = x.shape[1]\n",
    "        else:\n",
    "            dim = self.config.dim\n",
    "            length = self.config.length\n",
    "\n",
    "        # Estimate batch size\n",
    "        b = x.shape[0]\n",
    "\n",
    "        # Get position embeddings, adjust based on crop offsets if applicable\n",
    "        if self.position_embeddings is not None:\n",
    "            position_embeddings = self.position_embeddings\n",
    "            position_embeddings = repeat(position_embeddings, \"1 l d-> b l d\", b=b)\n",
    "        else:\n",
    "            cache_key = (dim, length)\n",
    "            if cache_key not in self.position_embeddings_cache:\n",
    "                self.position_embeddings_cache[cache_key] = get_absolute_position_embeddings_1d(dim, length)\n",
    "            position_embeddings = self.position_embeddings_cache[cache_key]\n",
    "            position_embeddings = repeat(position_embeddings, \"1 l d -> b l d\", b=b).to(x.device)\n",
    "        # (b, length, dim)\n",
    "\n",
    "        if embedding_type == \"add\":\n",
    "            x = x + position_embeddings\n",
    "        elif embedding_type == \"concat\":\n",
    "            x = torch.cat([x, position_embeddings], dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only 'add' and 'concat' are supported for embedding_type\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mAbsolutePositionEmbeddings1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_input = torch.randn(3, 6, 2)\n",
    "\n",
    "test = AbsolutePositionEmbeddings1D(dim=2, length=6, learnable=True)\n",
    "display(test)\n",
    "display(test(sample_input).shape)\n",
    "\n",
    "test = AbsolutePositionEmbeddings1D()\n",
    "display(test(sample_input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class PatchEmbeddings3D(CNNBlock3D):\n",
    "    \"\"\"3D Patch Embeddings using a convolutional layer. {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: PatchEmbeddings3DConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize PatchEmbeddings3D.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        self.config = PatchEmbeddings3DConfig.model_validate(config | kwargs)\n",
    "        config = self.config.model_dump() | {\n",
    "            \"kernel_size\": self.config.get(\"patch_size\"),\n",
    "            \"stride\": self.config.get(\"patch_size\"),\n",
    "            \"padding\": 0,\n",
    "            \"out_channels\": self.config.get(\"dim\"),\n",
    "        }\n",
    "        super().__init__(config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPatchEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m12\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = PatchEmbeddings3D(patch_size=(1, 8, 8), in_channels=1, dim=12)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 1, 32, 512, 512))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPatchEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m12\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = PatchEmbeddings3D(patch_size=(1, 8, 8), in_channels=1, dim=12)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 32, 512, 512, 1), channels_first=False)\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "coords_h = torch.arange(4)\n",
    "coords_w = torch.arange(4)\n",
    "coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "coords_flatten = torch.flatten(coords, 1)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += 4 - 1\n",
    "relative_coords[:, :, 1] += 4 - 1\n",
    "relative_coords[:, :, 0] *= 2 * 4 - 1\n",
    "relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "window_size = (4, 4, 4)\n",
    "relative_limits = (7, 7, 7)\n",
    "\n",
    "coords = get_coords_grid(window_size)\n",
    "coords_flatten = rearrange(coords, \"three_dimensional d h w -> three_dimensional (d h w)\", three_dimensional=3)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += window_size[0] - 1\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "relative_coords[:, :, 2] += window_size[2] - 1\n",
    "relative_position_index: torch.Tensor = (\n",
    "    relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 2]\n",
    ")\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "window_size = (2, 2, 2)\n",
    "relative_limits = (2 * window_size[0] - 1, 2 * window_size[1] - 1, 2 * window_size[2] - 1)\n",
    "\n",
    "relative_coords_table = get_coords_grid(relative_limits)\n",
    "relative_coords_table[0] -= window_size[0] - 1\n",
    "relative_coords_table[1] -= window_size[1] - 1\n",
    "relative_coords_table[2] -= window_size[2] - 1\n",
    "relative_coords_table = relative_coords_table.permute(1, 2, 3, 0).contiguous()\n",
    "relative_coords_table[0, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
