{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp layers/latent_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "\n",
    "from vision_architectures.blocks.cnn import CNNBlock3D, CNNBlockConfig\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, model_validator\n",
    "from vision_architectures.utils.rearrange import rearrange_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class LatentEncoderConfig(CNNBlockConfig):\n",
    "    init_low_var: bool = Field(False, description=\"Whether to initialize weights such that output variance is low\")\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data: dict):\n",
    "        data.setdefault(\"in_channels\", data.get(\"dim\"))\n",
    "        data.setdefault(\"out_channels\", data.get(\"latent_dim\"))\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.in_channels\n",
    "\n",
    "    @property\n",
    "    def latent_dim(self):\n",
    "        return self.out_channels\n",
    "\n",
    "\n",
    "class LatentDecoderConfig(CNNBlockConfig):\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data: dict):\n",
    "        data.setdefault(\"in_channels\", data.get(\"latent_dim\"))\n",
    "        data.setdefault(\"out_channels\", data.get(\"dim\"))\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def latent_dim(self):\n",
    "        return self.in_channels\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.out_channels\n",
    "\n",
    "\n",
    "class GaussianLatentSpaceConfig(CustomBaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class LatentEncoder(nn.Module):\n",
    "    def __init__(self, config: LatentEncoderConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = LatentEncoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        latent_dim = self.config.latent_dim\n",
    "\n",
    "        self.dim_mapper = CNNBlock3D(self.config, checkpointing_level)\n",
    "        self.quant_conv_mu = nn.Conv3d(latent_dim, latent_dim, 1)\n",
    "        self.quant_conv_log_var = nn.Conv3d(latent_dim, latent_dim, 1)\n",
    "\n",
    "        if self.config.init_low_var:\n",
    "            self.init_low_var()\n",
    "\n",
    "    def init_low_var(self, bias_constant: float = -1.0):\n",
    "        nn.init.normal_(self.quant_conv_log_var.weight, std=0.001)\n",
    "        nn.init.constant_(self.quant_conv_log_var.bias, bias_constant)\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        prior_mu: torch.Tensor | None = None,\n",
    "        prior_log_var: torch.Tensor | None = None,\n",
    "        return_logvar: bool = False,\n",
    "        channels_first: bool = True,\n",
    "    ):\n",
    "        \"\"\"Get latent space representation of the input by mapping it to the latent dimension and then extracting the\n",
    "        mean and standard deviation of the latent space. If a prior distribution is provided, the input is expected to\n",
    "        predict the deviation from the prior. If it is not provided, one can think of it as the deviation from a\n",
    "        standard normal distribution is being predicted. The output is the mean and standard deviation of the latent\n",
    "        space.\n",
    "\n",
    "        Args:\n",
    "            x: The input feature tensor.\n",
    "            prior_mu: The mean of the prior distribution. If None, it is assumed to be the mean of a standard normal\n",
    "                distribution. Defaults to None.\n",
    "            prior_log_var: The log-variance of the prior distribution. If None, it is assumed to be log-variance of a\n",
    "                standard normal distribution. Defaults to None.\n",
    "            return_logvar: Whether to return the log-variance too. Defaults to False.\n",
    "            channels_first: {CHANNELS_FIRST_DOC}. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            z_mu: The mean of the latent space.\n",
    "            z_sigma: The standard deviation of the latent space.\n",
    "        \"\"\"\n",
    "        # x: (b, [dim], z, y, x, [dim])\n",
    "        # z_mu_prior: (b, [latent_dim], z, y, x, [latent_dim])\n",
    "        # z_logvar_prior: (b, [latent_dim], z, y, x, [latent_dim])\n",
    "\n",
    "        assert (\n",
    "            prior_mu is None and prior_log_var is None or (prior_mu is not None and prior_log_var is not None)\n",
    "        ), \"If prior_mu or prior_log_var are provided, both must be provided.\"\n",
    "\n",
    "        x = rearrange_channels(x, channels_first, True)\n",
    "        # (b, dim, z, y, x)\n",
    "        if prior_mu is not None:\n",
    "            prior_mu = rearrange_channels(prior_mu, channels_first, True)\n",
    "            prior_log_var = rearrange_channels(prior_log_var, channels_first, True)\n",
    "            # (b, latent_dim, z, y, x)\n",
    "\n",
    "        x = self.dim_mapper(x, channels_first=True)\n",
    "        # (b, latent_dim, z, y, x)\n",
    "\n",
    "        z_mu = self.quant_conv_mu(x)\n",
    "        z_log_var = self.quant_conv_log_var(x)\n",
    "        # (b, latent_dim, z, y, x)\n",
    "\n",
    "        if prior_mu is not None:\n",
    "            z_mu = prior_mu + z_mu\n",
    "            z_log_var = prior_log_var + z_log_var\n",
    "            # (b, latent_dim, z, y, x)\n",
    "\n",
    "        z_log_var = torch.clamp(z_log_var, -10.0, 10.0)\n",
    "        z_sigma = torch.exp(z_log_var / 2).clamp(min=1e-6)\n",
    "        # (b, latent_dim, z, y, x)\n",
    "\n",
    "        z_mu = rearrange_channels(z_mu, True, channels_first)\n",
    "        z_sigma = rearrange_channels(z_sigma, True, channels_first)\n",
    "        if return_logvar:\n",
    "            z_log_var = rearrange_channels(z_log_var, True, channels_first)\n",
    "        # (b, [latent_dim], z, y, x, [latent_dim])\n",
    "\n",
    "        if return_logvar:\n",
    "            return z_mu, z_sigma, z_log_var\n",
    "\n",
    "        return z_mu, z_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mLatentEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdim_mapper\u001b[1m)\u001b[0m: \u001b[1;35mCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mquant_conv_mu\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mquant_conv_log_var\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = LatentEncoder(dim=32, latent_dim=4, kernel_size=3)\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 32, 16, 16, 16)\n",
    "z_mu, z_sigma = test(sample_input)\n",
    "z_mu.shape, z_sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mLatentEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdim_mapper\u001b[1m)\u001b[0m: \u001b[1;35mCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mquant_conv_mu\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mquant_conv_log_var\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = LatentEncoder(dim=32, latent_dim=4, kernel_size=3)\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 32, 16, 16, 16)\n",
    "sample_prior_mu = torch.randn(2, 4, 16, 16, 16)\n",
    "sample_prior_log_var = torch.randn(2, 4, 16, 16, 16)\n",
    "z_mu, z_sigma = test(sample_input, sample_prior_mu, sample_prior_log_var)\n",
    "z_mu.shape, z_sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class LatentDecoder(nn.Module):\n",
    "    def __init__(self, config: LatentDecoderConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = LatentDecoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        latent_dim = self.config.latent_dim\n",
    "\n",
    "        self.post_quant_conv = nn.Conv3d(latent_dim, latent_dim, 1)\n",
    "        self.dim_mapper = CNNBlock3D(self.config, checkpointing_level)\n",
    "\n",
    "    def forward(self, z: torch.Tensor, channels_first: bool = True):\n",
    "        # z: (b, [latent_dim], z, y, x, [latent_dim])\n",
    "\n",
    "        z = rearrange_channels(z, channels_first, True)\n",
    "        # (b, latent_dim, z, y, x)\n",
    "\n",
    "        z = self.post_quant_conv(z)\n",
    "        # (b, latent_dim, z, y, x)\n",
    "\n",
    "        x = self.dim_mapper(z, channels_first=True)\n",
    "        # (b, dim, z, y, x)\n",
    "\n",
    "        x = rearrange_channels(x, True, channels_first)\n",
    "        # (b, [dim], z, y, x, [dim])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mLatentDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpost_quant_conv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdim_mapper\u001b[1m)\u001b[0m: \u001b[1;35mCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = LatentDecoder(latent_dim=4, dim=32, kernel_size=3)\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class GaussianLatentSpace(nn.Module):\n",
    "    def __init__(self, config: GaussianLatentSpaceConfig = {}, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = GaussianLatentSpaceConfig.model_validate(config | kwargs)\n",
    "\n",
    "    def sample(self, z_mu: torch.Tensor, z_sigma: torch.Tensor, force_sampling: bool = False):\n",
    "        if not self.training and not force_sampling:\n",
    "            return z_mu\n",
    "        eps = torch.randn_like(z_sigma)\n",
    "        z_vae = z_mu + eps * z_sigma\n",
    "        return z_vae\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_divergence(\n",
    "        z_mu: torch.Tensor,\n",
    "        z_sigma: torch.Tensor,\n",
    "        prior_mu: torch.Tensor | None = None,\n",
    "        prior_sigma: torch.Tensor | None = None,\n",
    "        reduction: Literal[\"allsum\", \"channelssum\"] | None = \"allsum\",\n",
    "        channels_first: bool = True,\n",
    "    ):\n",
    "        if prior_mu is None:\n",
    "            prior_mu = torch.zeros_like(z_mu)\n",
    "        if prior_sigma is None:\n",
    "            prior_sigma = torch.ones_like(z_sigma)\n",
    "\n",
    "        prior = Normal(prior_mu, prior_sigma)\n",
    "        posterior = Normal(z_mu, z_sigma)\n",
    "        kl_div = kl_divergence(posterior, prior)\n",
    "\n",
    "        if reduction is None:\n",
    "            pass\n",
    "        elif reduction == \"allsum\":\n",
    "            kl_div = kl_div.sum(dim=list(range(1, kl_div.ndim))).mean()\n",
    "        elif reduction == \"channelssum\":\n",
    "            if channels_first:\n",
    "                channels_dim = 1\n",
    "            else:\n",
    "                channels_dim = -1\n",
    "            kl_div = kl_div.sum(dim=channels_dim).mean()\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction {reduction} not implemented\")\n",
    "\n",
    "        return kl_div\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z_mu: torch.Tensor,\n",
    "        z_sigma: torch.Tensor,\n",
    "        prior_mu: torch.Tensor | None = None,\n",
    "        prior_sigma: torch.Tensor | None = None,\n",
    "        kl_divergence_reduction: Literal[\"allsum\", \"channelssum\"] | None = \"allsum\",\n",
    "        force_sampling: bool = False,\n",
    "        channels_first: bool = True,\n",
    "    ):\n",
    "        return self.sample(z_mu, z_sigma, force_sampling), self.kl_divergence(\n",
    "            z_mu, z_sigma, prior_mu, prior_sigma, kl_divergence_reduction, channels_first\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mGaussianLatentSpace\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m19114.0234\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = GaussianLatentSpace()\n",
    "display(test)\n",
    "\n",
    "sample_input_mu = torch.randn(2, 16, 16, 16, 4)\n",
    "sample_input_sigma = torch.rand(2, 16, 16, 16, 4)\n",
    "o = test(sample_input_mu, sample_input_sigma, channels_first=False)\n",
    "o[0].shape, o[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
