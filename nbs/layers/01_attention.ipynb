{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp layers/attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from functools import partial\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.layers.embeddings import RelativePositionEmbeddings\n",
    "from vision_architectures.utils.activations import get_act_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs attention (MHA, GQA, and MQA) on 1D sequences\n",
    "    Parameters:\n",
    "        - dim: Input dimensions of k, q, and v. Attention happens at dim=dim_qk.\n",
    "            int: dimension of q, k, and v\n",
    "            tuple[int, int]: first int is dimension of q and k, second int is dimension of v\n",
    "        - num_q_heads: number of query heads\n",
    "        - ratio_q_to_kv_heads: number of query heads per key and value head\n",
    "        - relative_position_bias: RelativePositionEmbeddings object\n",
    "        - logit_scale: scale of the logits. Defaults to 1/sqrt(d)\n",
    "        - logit_scale_learnable: whether the logit scale is learnable\n",
    "        - attn_drop_prob: dropout probability for attention weights\n",
    "        - proj_drop_prob: dropout probability for projection layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int | tuple[int, int],\n",
    "        num_q_heads: int,\n",
    "        ratio_q_to_kv_heads: int = 1,  # num q heads per kv head\n",
    "        relative_position_bias: RelativePositionEmbeddings | None = None,\n",
    "        logit_scale=None,\n",
    "        logit_scale_learnable: bool = False,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        gqa_mqa_enabled = ratio_q_to_kv_heads != 1\n",
    "        if gqa_mqa_enabled:\n",
    "            assert torch.__version__ >= \"2.5\", \"Need PyTorch version >= 2.5 for GQA and MQA\"\n",
    "\n",
    "        if isinstance(dim, int):\n",
    "            dim_qk, dim_v = dim, dim\n",
    "        else:\n",
    "            dim_qk, dim_v = dim\n",
    "\n",
    "        num_kv_heads = num_q_heads // ratio_q_to_kv_heads\n",
    "\n",
    "        assert dim_qk % num_q_heads == 0, \"dimension must be divisible by number of heads\"\n",
    "        assert (\n",
    "            num_q_heads % num_kv_heads == 0\n",
    "        ), \"number of query heads must be divisible by number of key and value heads\"\n",
    "\n",
    "        self.num_q_heads = num_q_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.gqa_mqa_enabled = gqa_mqa_enabled\n",
    "\n",
    "        self.per_head_dim = int(dim_qk // num_q_heads)\n",
    "\n",
    "        self.W_q = nn.Linear(dim_qk, dim_qk)\n",
    "        self.W_k = nn.Linear(dim_qk, dim_qk // ratio_q_to_kv_heads)\n",
    "        self.W_v = nn.Linear(dim_v, dim_qk // ratio_q_to_kv_heads)\n",
    "        self.attn_drop_prob = attn_drop_prob\n",
    "        self.proj = nn.Linear(dim_qk, dim_qk)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_prob)\n",
    "\n",
    "        if logit_scale is None:\n",
    "            self.logit_scale = nn.Parameter(\n",
    "                torch.tensor([self.per_head_dim**-0.5]), requires_grad=logit_scale_learnable\n",
    "            )\n",
    "        else:\n",
    "            self.logit_scale = logit_scale\n",
    "\n",
    "        self.relative_position_bias = relative_position_bias\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Parameters: T => number of tokens, b => batch size\n",
    "            - query: (b, T_q, dim_qk)\n",
    "            - key: (b, T_kv, dim_qk)\n",
    "            - value: (b, T_kv, dim_v)\n",
    "        \"\"\"\n",
    "\n",
    "        query = self.W_q(query)\n",
    "        key = self.W_k(key)\n",
    "        value = self.W_v(value)\n",
    "\n",
    "        rearrange_partial = partial(rearrange, pattern=\"b T (num_heads d) -> b num_heads T d\")\n",
    "        query = rearrange_partial(query, num_heads=self.num_q_heads)\n",
    "        key = rearrange_partial(key, num_heads=self.num_kv_heads)\n",
    "        value = rearrange_partial(value, num_heads=self.num_kv_heads)\n",
    "        # query: (b, num_q_heads, T, per_head_dim)\n",
    "        # key: (b, num_kv_heads, T, per_head_dim)\n",
    "        # value: (b, num_kv_heads, T, per_head_dim)\n",
    "\n",
    "        if isinstance(self.logit_scale, nn.Module):\n",
    "            logit_scale = self.logit_scale()\n",
    "        else:\n",
    "            logit_scale = self.logit_scale\n",
    "\n",
    "        query_normalized = F.normalize(query, dim=-1)\n",
    "        key_normalized = F.normalize(key, dim=-1)\n",
    "\n",
    "        query_normalized_and_scaled = query_normalized * logit_scale  # Scale the query beforehand\n",
    "\n",
    "        relative_position_bias = None\n",
    "        if self.relative_position_bias is not None:\n",
    "            relative_position_bias = self.relative_position_bias()\n",
    "\n",
    "        output = F.scaled_dot_product_attention(\n",
    "            query_normalized_and_scaled,\n",
    "            key_normalized,\n",
    "            value,\n",
    "            attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias\n",
    "            dropout_p=self.attn_drop_prob,\n",
    "            is_causal=False,\n",
    "            scale=1.0,  # Already scaled the vectors\n",
    "            enable_gqa=self.gqa_mqa_enabled,\n",
    "        )\n",
    "        # (b, num_q_heads, T, per_head_dim)\n",
    "\n",
    "        output = rearrange(output, \"b num_heads T d -> b T (num_heads d)\")\n",
    "        # (b, T, dim_qk)\n",
    "\n",
    "        output = self.proj(output)\n",
    "        output = self.proj_drop(output)\n",
    "        # (b, T, dim_qk)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m30\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention1D((30, 60), 6, ratio_q_to_kv_heads=2, logit_scale=4.0)\n",
    "q = torch.randn(2, 64, 30)\n",
    "k = torch.randn(2, 32, 30)\n",
    "v = torch.randn(2, 32, 60)\n",
    "\n",
    "display(test)\n",
    "display(test(q, k, v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention3D(Attention1D):\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, channels_first: bool = True):\n",
    "        \"\"\"\n",
    "        Parameters: z => depth, y => height, x => width, b => batch size\n",
    "            - query: (b, [dim_qk], z_q, y_q, x_q, [dim_qk])\n",
    "            - key: (b, [dim_qk], z_k, y_k, x_k, [dim_qk])\n",
    "            - value: (b, [dim_v], z_k, y_k, x_k, [dim_v])\n",
    "            - channels_first: if True, BCDHW expected, else BDHWC\n",
    "\n",
    "        Constraints:\n",
    "            - d_q * h_q * w_q = d_k * h_k * w_k\n",
    "        \"\"\"\n",
    "\n",
    "        if channels_first:\n",
    "            z_q, y_q, x_q = query.shape[2:5]\n",
    "            forward_pattern = \"b d z y x -> b (z y x) d\"\n",
    "            reverse_pattern = \"b (z y x) d -> b d z y x\"\n",
    "        else:\n",
    "            z_q, y_q, x_q = query.shape[1:4]\n",
    "            forward_pattern = \"b z y x d -> b (z y x) d\"\n",
    "            reverse_pattern = \"b (z y x) d -> b z y x d\"\n",
    "\n",
    "        query = rearrange(query, forward_pattern)\n",
    "        key = rearrange(key, forward_pattern)\n",
    "        value = rearrange(value, forward_pattern)\n",
    "\n",
    "        output = super().forward(query, key, value)\n",
    "\n",
    "        output = rearrange(output, reverse_pattern, z=z_q, y=y_q, x=x_q)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention3D((30, 60), 6, ratio_q_to_kv_heads=2, logit_scale=4.0)\n",
    "q = torch.randn(2, 30, 4, 4, 4)\n",
    "k = torch.randn(2, 30, 2, 4, 4)\n",
    "v = torch.randn(2, 60, 2, 4, 4)\n",
    "\n",
    "display(test)\n",
    "display(test(q, k, v, channels_first=True).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention1DMLP(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio, activation='gelu', mlp_drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(dim, dim * mlp_ratio)\n",
    "\n",
    "        if isinstance(activation, nn.Module):\n",
    "            self.act = activation\n",
    "        else:\n",
    "            self.act = get_act_layer(activation)\n",
    "\n",
    "        self.dense2 = nn.Linear(dim * mlp_ratio, dim)\n",
    "        self.dropout = nn.Dropout(mlp_drop_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        # hidden_states: (b, T, dim)\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m28\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention1DMLP(64, 4, \"relu\", 0.2)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 28, 64)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention3DMLP(Attention1DMLP):\n",
    "    def forward(self, hidden_states: torch.Tensor, channels_first: bool = True):\n",
    "        # hidden_states: (b, dim, z, y, x) or (b, z, y, x, dim)\n",
    "\n",
    "        if channels_first:\n",
    "            hidden_states = rearrange(hidden_states, \"b d z y x -> b z y x d\")\n",
    "\n",
    "        hidden_states = super().forward(hidden_states)\n",
    "\n",
    "        if channels_first:\n",
    "            hidden_states = rearrange(hidden_states, \"b z y x d -> b d z y x\")\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention3DMLP(64, 4, \"relu\", 0.2)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 64, 4, 4, 4)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention1DWithMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int | tuple[int, int],\n",
    "        num_q_heads: int,\n",
    "        ratio_q_to_kv_heads: int = 1,\n",
    "        mlp_ratio: int = 4,\n",
    "        qkv_relative_position_bias=None,\n",
    "        qk_scale: float = None,\n",
    "        qk_scale_learnable: bool = False,\n",
    "        activation=\"gelu\",\n",
    "        norm_location: Literal[\"pre\", \"post\"] = \"post\",\n",
    "        layer_norm_eps: float = 1e-6,\n",
    "        attn_drop_prob: float = 0.0,\n",
    "        proj_drop_prob: float = 0.0,\n",
    "        mlp_drop_prob: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_location = norm_location\n",
    "\n",
    "        if isinstance(dim, int):\n",
    "            dim_qk = dim\n",
    "        else:\n",
    "            dim_qk = dim[0]\n",
    "\n",
    "        self.attn = Attention1D(\n",
    "            dim=dim,\n",
    "            num_q_heads=num_q_heads,\n",
    "            ratio_q_to_kv_heads=ratio_q_to_kv_heads,\n",
    "            relative_position_bias=qkv_relative_position_bias,\n",
    "            logit_scale=qk_scale,\n",
    "            logit_scale_learnable=qk_scale_learnable,\n",
    "            attn_drop_prob=attn_drop_prob,\n",
    "            proj_drop_prob=proj_drop_prob,\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)\n",
    "        self.mlp = Attention1DMLP(\n",
    "            dim_qk, mlp_ratio=mlp_ratio, activation=activation, mlp_drop_prob=mlp_drop_prob\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        # Each is (b, T, dim)\n",
    "        res_connection1 = query\n",
    "        # (b, T, dim)\n",
    "\n",
    "        if self.norm_location == \"pre\":\n",
    "            query = self.layernorm1(query)\n",
    "            key = self.layernorm1(key)\n",
    "            value = self.layernorm1(value)\n",
    "            # (b, T, dim)\n",
    "\n",
    "        hidden_states = self.attn(query, key, value)\n",
    "        # (b, T, dim)\n",
    "\n",
    "        if self.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm1(hidden_states)\n",
    "            # (b, T, dim)\n",
    "\n",
    "        hidden_states = hidden_states + res_connection1\n",
    "        res_connection2 = hidden_states\n",
    "        # (b, T, dim)\n",
    "\n",
    "        if self.norm_location == \"pre\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, T, dim)\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # (b, T, dim)\n",
    "\n",
    "        if self.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, T, dim)\n",
    "\n",
    "        hidden_states = hidden_states + res_connection2\n",
    "        # (b, T, dim)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention1DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m216\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m216\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention1DWithMLP(54, 3)\n",
    "output = torch.randn(2, 64, 54)\n",
    "\n",
    "display(test)\n",
    "display(test(output, output, output).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention3DWithMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int | tuple[int, int],\n",
    "        num_q_heads: int,\n",
    "        ratio_q_to_kv_heads: int = 1,\n",
    "        mlp_ratio: int = 4,\n",
    "        qkv_relative_position_bias=None,\n",
    "        qk_scale: float = None,\n",
    "        qk_scale_learnable: bool = False,\n",
    "        activation=\"gelu\",\n",
    "        norm_location: Literal[\"pre\", \"post\"] = \"post\",\n",
    "        layer_norm_eps: float = 1e-6,\n",
    "        attn_drop_prob: float = 0.0,\n",
    "        proj_drop_prob: float = 0.0,\n",
    "        mlp_drop_prob: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_location = norm_location\n",
    "\n",
    "        if isinstance(dim, int):\n",
    "            dim_qk = dim\n",
    "        else:\n",
    "            dim_qk = dim[0]\n",
    "\n",
    "        self.attn = Attention3D(\n",
    "            dim=dim,\n",
    "            num_q_heads=num_q_heads,\n",
    "            ratio_q_to_kv_heads=ratio_q_to_kv_heads,\n",
    "            relative_position_bias=qkv_relative_position_bias,\n",
    "            logit_scale=qk_scale,\n",
    "            logit_scale_learnable=qk_scale_learnable,\n",
    "            attn_drop_prob=attn_drop_prob,\n",
    "            proj_drop_prob=proj_drop_prob,\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)\n",
    "        self.mlp = Attention3DMLP(\n",
    "            dim_qk, mlp_ratio=mlp_ratio, activation=activation, mlp_drop_prob=mlp_drop_prob\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, channels_first: bool = True):\n",
    "        # Each is (b, [dim], tokens_z, tokens_y, tokens_x, [dim])\n",
    "\n",
    "        if channels_first:\n",
    "            query = rearrange(query, \"b d z y x -> b z y x d\")\n",
    "            key = rearrange(key, \"b d z y x -> b z y x d\")\n",
    "            value = rearrange(value, \"b d z y x -> b z y x d\")\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        res_connection1 = query\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.norm_location == \"pre\":\n",
    "            query = self.layernorm1(query)\n",
    "            key = self.layernorm1(key)\n",
    "            value = self.layernorm1(value)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.attn(query, key, value, channels_first=False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm1(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = hidden_states + res_connection1\n",
    "        res_connection2 = hidden_states\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.norm_location == \"pre\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states, channels_first=False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = hidden_states + res_connection2\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if channels_first:\n",
    "            hidden_states = rearrange(hidden_states, \"b z y x d -> b d z y x\")\n",
    "            # (b, dim, tokens_z, tokens_y, tokens_x)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention3DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m216\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m216\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m54\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention3DWithMLP(54, 3)\n",
    "output = torch.randn(2, 54, 4, 4, 4)\n",
    "\n",
    "display(test)\n",
    "display(test(output, output, output, channels_first=True).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
