{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp layers/attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.layers.embeddings import RelativePositionEmbeddings\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, model_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention1DConfig(CustomBaseModel):\n",
    "    dim: int | tuple[int, int]\n",
    "    num_heads: int = Field(..., description=\"Number of query heads\")\n",
    "    ratio_q_to_kv_heads: int = 1\n",
    "    logit_scale_learnable: bool = False\n",
    "    attn_drop_prob: float = 0.0\n",
    "    proj_drop_prob: float = 0.0\n",
    "    max_attention_batch_size: int = Field(\n",
    "        -1,\n",
    "        description=(\n",
    "            \"Runs attention by splitting the inputs into chunks of this size. 0 means no chunking. \"\n",
    "            \"Useful for large inputs during inference.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def num_q_heads(self) -> int:\n",
    "        return self.num_heads\n",
    "\n",
    "    @property\n",
    "    def num_kv_heads(self) -> int:\n",
    "        return self.num_heads // self.ratio_q_to_kv_heads\n",
    "\n",
    "    @property\n",
    "    def gqa_mqa_enabled(self) -> bool:\n",
    "        return self.ratio_q_to_kv_heads != 1\n",
    "\n",
    "    @property\n",
    "    def dim_qk(self) -> int:\n",
    "        if isinstance(self.dim, tuple):\n",
    "            return self.dim[0]\n",
    "        return self.dim\n",
    "\n",
    "    @property\n",
    "    def dim_v(self) -> int:\n",
    "        if isinstance(self.dim, tuple):\n",
    "            return self.dim[1]\n",
    "        return self.dim\n",
    "\n",
    "    @property\n",
    "    def per_head_dim_qk(self) -> int:\n",
    "        return self.dim_qk // self.num_heads\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if self.gqa_mqa_enabled:\n",
    "            assert torch.__version__ >= \"2.5\", \"Need PyTorch version >= 2.5 for GQA and MQA\"\n",
    "\n",
    "        assert self.dim_qk % self.num_heads == 0, \"dimension must be divisible by number of heads\"\n",
    "        assert (\n",
    "            self.num_heads % self.num_kv_heads == 0\n",
    "        ), \"number of query heads must be divisible by number of key and value heads\"\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class Attention3DConfig(Attention1DConfig):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention1D(nn.Module):\n",
    "    \"\"\"Performs attention (MHA, GQA, and MQA) on 1D sequences\"\"\"\n",
    "\n",
    "    _warn_relative_position_bias: bool = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Attention1DConfig = {},\n",
    "        relative_position_bias: RelativePositionEmbeddings | None = None,\n",
    "        logit_scale: float | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Attention1DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim_qk = self.config.dim_qk\n",
    "        dim_v = self.config.dim_v\n",
    "        ratio_q_to_kv_heads = self.config.ratio_q_to_kv_heads\n",
    "        per_head_dim = self.config.per_head_dim_qk\n",
    "        logit_scale_learnable = self.config.logit_scale_learnable\n",
    "        attn_drop_prob = self.config.attn_drop_prob\n",
    "        proj_drop_prob = self.config.proj_drop_prob\n",
    "\n",
    "        self.W_q = nn.Linear(dim_qk, dim_qk)\n",
    "        self.W_k = nn.Linear(dim_qk, dim_qk // ratio_q_to_kv_heads)\n",
    "        self.W_v = nn.Linear(dim_v, dim_qk // ratio_q_to_kv_heads)\n",
    "        self.attn_drop_prob = attn_drop_prob\n",
    "        self.proj = nn.Linear(dim_qk, dim_qk)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_prob)\n",
    "\n",
    "        if logit_scale is None:\n",
    "            self.logit_scale = nn.Parameter(\n",
    "                torch.tensor([per_head_dim**-0.5]),\n",
    "                requires_grad=logit_scale_learnable,\n",
    "            )\n",
    "        else:\n",
    "            self.logit_scale = logit_scale\n",
    "\n",
    "        if self._warn_relative_position_bias and relative_position_bias is not None:\n",
    "            print(\n",
    "                \"Warning: Relative position bias is not used in Attention1D. \"\n",
    "                \"Use Attention3D for relative position bias.\"\n",
    "            )\n",
    "        self.relative_position_bias = relative_position_bias\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)\n",
    "\n",
    "    def _forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Parameters: T => number of tokens, b => batch size\n",
    "            - query: (b, T_q, dim_qk)\n",
    "            - key: (b, T_kv, dim_qk)\n",
    "            - value: (b, T_kv, dim_v)\n",
    "        \"\"\"\n",
    "\n",
    "        def get_final_query_key_value(query, key, value):\n",
    "            query = self.W_q(query)\n",
    "            key = self.W_k(key)\n",
    "            value = self.W_v(value)\n",
    "\n",
    "            rearrange_partial = partial(rearrange, pattern=\"b T (num_heads d) -> b num_heads T d\")\n",
    "            query = rearrange_partial(query, num_heads=self.config.num_heads).contiguous()\n",
    "            key = rearrange_partial(key, num_heads=self.config.num_kv_heads).contiguous()\n",
    "            value = rearrange_partial(value, num_heads=self.config.num_kv_heads).contiguous()\n",
    "            # query: (b, num_heads, T, per_head_dim)\n",
    "            # key: (b, num_kv_heads, T, per_head_dim)\n",
    "            # value: (b, num_kv_heads, T, per_head_dim)\n",
    "\n",
    "            if isinstance(self.logit_scale, nn.Module):\n",
    "                logit_scale = self.logit_scale()\n",
    "            else:\n",
    "                logit_scale = self.logit_scale\n",
    "\n",
    "            query_normalized = F.normalize(query, dim=-1)\n",
    "            key_normalized = F.normalize(key, dim=-1)\n",
    "\n",
    "            query_normalized_and_scaled = query_normalized * logit_scale  # Scale the query beforehand\n",
    "\n",
    "            return query_normalized_and_scaled, key_normalized, value\n",
    "\n",
    "        query_normalized_and_scaled, key_normalized, value = self.checkpointing_level1(\n",
    "            get_final_query_key_value, query, key, value\n",
    "        )\n",
    "\n",
    "        relative_position_bias = None\n",
    "        if self.relative_position_bias is not None:\n",
    "            relative_position_bias = self.relative_position_bias()\n",
    "\n",
    "        # Split tensors into batches and perform attention\n",
    "        output = []\n",
    "        chunk_size = self.config.max_attention_batch_size\n",
    "        if chunk_size == -1:\n",
    "            chunk_size = query_normalized_and_scaled.size(0)\n",
    "        for query_normalized_and_scaled_chunk, key_normalized_chunk, value_chunk in zip(\n",
    "            torch.split(query_normalized_and_scaled, chunk_size, dim=0),\n",
    "            torch.split(key_normalized, chunk_size, dim=0),\n",
    "            torch.split(value, chunk_size, dim=0),\n",
    "        ):\n",
    "            torch250plus_kwargs = {}\n",
    "            if torch.__version__ >= \"2.5\":\n",
    "                torch250plus_kwargs[\"enable_gqa\"] = self.config.gqa_mqa_enabled\n",
    "\n",
    "            output_chunk = F.scaled_dot_product_attention(\n",
    "                query_normalized_and_scaled_chunk,\n",
    "                key_normalized_chunk,\n",
    "                value_chunk,\n",
    "                attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias\n",
    "                dropout_p=self.attn_drop_prob,\n",
    "                is_causal=False,\n",
    "                scale=1.0,  # Already scaled the vectors\n",
    "                **torch250plus_kwargs,\n",
    "            )\n",
    "            output.append(output_chunk)\n",
    "            # (chunk_size, num_heads, T, per_head_dim)\n",
    "        output = torch.cat(output, dim=0)\n",
    "        # (b, num_heads, T, per_head_dim)\n",
    "\n",
    "        output = rearrange(output, \"b num_heads T d -> b T (num_heads d)\").contiguous()\n",
    "        # (b, T, dim_qk)\n",
    "\n",
    "        def get_final_output(output):\n",
    "            output = self.proj(output)\n",
    "            output = self.proj_drop(output)\n",
    "            return output\n",
    "\n",
    "        output = self.checkpointing_level1(get_final_output, output)\n",
    "        # (b, T, dim_qk)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        return self.checkpointing_level2(self._forward, query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m6\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m30\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention1D(dim=(30, 60), num_heads=6, ratio_q_to_kv_heads=2, logit_scale=4.0, max_attention_batch_size=2)\n",
    "q = torch.randn(6, 64, 30)\n",
    "k = torch.randn(6, 32, 30)\n",
    "v = torch.randn(6, 32, 60)\n",
    "\n",
    "display(test)\n",
    "display(test(q, k, v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention3D(Attention1D):\n",
    "    _warn_relative_position_bias: bool = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Attention3DConfig = {},\n",
    "        relative_position_bias: RelativePositionEmbeddings | None = None,\n",
    "        logit_scale: float | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, relative_position_bias, logit_scale, checkpointing_level, **kwargs)\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        channels_first: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters: z => depth, y => height, x => width, b => batch size\n",
    "            - query: (b, [dim_qk], z_q, y_q, x_q, [dim_qk])\n",
    "            - key: (b, [dim_qk], z_k, y_k, x_k, [dim_qk])\n",
    "            - value: (b, [dim_v], z_k, y_k, x_k, [dim_v])\n",
    "            - channels_first: if True, BCDHW expected, else BDHWC\n",
    "\n",
    "        Constraints:\n",
    "            - d_q * h_q * w_q = d_k * h_k * w_k\n",
    "        \"\"\"\n",
    "\n",
    "        if channels_first:\n",
    "            z_q, y_q, x_q = query.shape[2:5]\n",
    "            forward_pattern = \"b d z y x -> b (z y x) d\"\n",
    "            reverse_pattern = \"b (z y x) d -> b d z y x\"\n",
    "        else:\n",
    "            z_q, y_q, x_q = query.shape[1:4]\n",
    "            forward_pattern = \"b z y x d -> b (z y x) d\"\n",
    "            reverse_pattern = \"b (z y x) d -> b z y x d\"\n",
    "\n",
    "        query = rearrange(query, forward_pattern).contiguous()\n",
    "        key = rearrange(key, forward_pattern).contiguous()\n",
    "        value = rearrange(value, forward_pattern).contiguous()\n",
    "\n",
    "        output = super()._forward(query, key, value)\n",
    "\n",
    "        output = rearrange(output, reverse_pattern, z=z_q, y=y_q, x=x_q).contiguous()\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        channels_first: bool = True,\n",
    "    ):\n",
    "        return self.checkpointing_level2(self._forward, query, key, value, channels_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention3D(\n",
    "    config={\"dim\": (30, 60), \"num_heads\": 6, \"ratio_q_to_kv_heads\": 2},\n",
    "    checkpointing_level=5,\n",
    ")\n",
    "q = torch.randn(2, 30, 4, 4, 4)\n",
    "k = torch.randn(2, 30, 2, 4, 4)\n",
    "v = torch.randn(2, 60, 2, 4, 4)\n",
    "\n",
    "display(test)\n",
    "display(test(q, k, v, channels_first=True).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/bin/nbdev_export\", line 8, in <module>\n",
      "    sys.exit(nbdev_export())\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/fastcore/script.py\", line 125, in _f\n",
      "    return tfunc(**merge(args, args_from_prog(func, xtra)))\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/nbdev/doclinks.py\", line 155, in nbdev_export\n",
      "    for f in files: nb_export(f, procs=procs)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/nbdev/export.py\", line 81, in nb_export\n",
      "    nb = NBProcessor(nbname, [exp]+L(procs), debug=debug)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/nbdev/process.py\", line 95, in __init__\n",
      "    self.nb = read_nb(path) if nb is None else nb\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/execnb/nbio.py\", line 59, in read_nb\n",
      "    res = dict2nb(_read_json(path, encoding='utf-8'))\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/execnb/nbio.py\", line 18, in _read_json\n",
      "    return loads(Path(self).read_text(encoding=encoding, errors=errors))\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
