{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp layers/attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from functools import partial, wraps\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.layers.embeddings import RelativePositionEmbeddings\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, model_validator\n",
    "from vision_architectures.utils.rearrange import rearrange_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention1DConfig(CustomBaseModel):\n",
    "    dim: int | tuple[int, int] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Dimension of the input features. If tuple, (dim_qk, dim_v). \"\n",
    "            \"Otherwise it is assumed to be dim of both qk and v.\"\n",
    "        ),\n",
    "    )\n",
    "    num_heads: int = Field(..., description=\"Number of query heads\")\n",
    "    ratio_q_to_kv_heads: int = Field(1, description=\"Ratio of query heads to key/value heads. Useful for MQA/GQA.\")\n",
    "    logit_scale_learnable: bool = Field(False, description=\"Whether the logit scale is learnable.\")\n",
    "    attn_drop_prob: float = Field(0.0, description=\"Dropout probability for attention weights.\")\n",
    "    proj_drop_prob: float = Field(0.0, description=\"Dropout probability for the projection layer.\")\n",
    "    max_attention_batch_size: int = Field(\n",
    "        -1,\n",
    "        description=(\n",
    "            \"Runs attention by splitting the inputs into chunks of this size. 0 means no chunking. \"\n",
    "            \"Useful for large inputs during inference. (This happens along batch dimension).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def num_q_heads(self) -> int:\n",
    "        return self.num_heads\n",
    "\n",
    "    @property\n",
    "    def num_kv_heads(self) -> int:\n",
    "        return self.num_heads // self.ratio_q_to_kv_heads\n",
    "\n",
    "    @property\n",
    "    def gqa_mqa_enabled(self) -> bool:\n",
    "        return self.ratio_q_to_kv_heads != 1\n",
    "\n",
    "    @property\n",
    "    def dim_qk(self) -> int:\n",
    "        if isinstance(self.dim, tuple):\n",
    "            return self.dim[0]\n",
    "        return self.dim\n",
    "\n",
    "    @property\n",
    "    def dim_v(self) -> int:\n",
    "        if isinstance(self.dim, tuple):\n",
    "            return self.dim[1]\n",
    "        return self.dim\n",
    "\n",
    "    @property\n",
    "    def per_head_dim_qk(self) -> int:\n",
    "        return self.dim_qk // self.num_heads\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if self.gqa_mqa_enabled:\n",
    "            assert torch.__version__ >= \"2.5\", \"Need PyTorch version >= 2.5 for GQA and MQA\"\n",
    "\n",
    "        assert self.dim_qk % self.num_heads == 0, \"dimension must be divisible by number of heads\"\n",
    "        assert (\n",
    "            self.num_heads % self.num_kv_heads == 0\n",
    "        ), \"number of query heads must be divisible by number of key and value heads\"\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class Attention3DConfig(Attention1DConfig):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class Attention1D(nn.Module):\n",
    "    \"\"\"Performs attention (MHA, GQA, and MQA) on 1D sequences. {CLASS_DESCRIPTION_1D_DOC}\"\"\"\n",
    "\n",
    "    _warn_relative_position_bias: bool = True\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Attention1DConfig = {},\n",
    "        relative_position_bias: RelativePositionEmbeddings | None = None,\n",
    "        logit_scale: float | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initializes the Attention1D module.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            relative_position_bias: Relative position embeddings to be considered during attention. Should be callable.\n",
    "            logit_scale: Logit scale to be used for attention. If None, it will be initialized based on per-head\n",
    "                dimension.\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Attention1DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim_qk = self.config.dim_qk\n",
    "        dim_v = self.config.dim_v\n",
    "        ratio_q_to_kv_heads = self.config.ratio_q_to_kv_heads\n",
    "        per_head_dim = self.config.per_head_dim_qk\n",
    "        logit_scale_learnable = self.config.logit_scale_learnable\n",
    "        attn_drop_prob = self.config.attn_drop_prob\n",
    "        proj_drop_prob = self.config.proj_drop_prob\n",
    "\n",
    "        self.W_q = nn.Linear(dim_qk, dim_qk)\n",
    "        self.W_k = nn.Linear(dim_qk, dim_qk // ratio_q_to_kv_heads)\n",
    "        self.W_v = nn.Linear(dim_v, dim_qk // ratio_q_to_kv_heads)\n",
    "        self.attn_drop_prob = attn_drop_prob\n",
    "        self.proj = nn.Linear(dim_qk, dim_qk)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_prob)\n",
    "\n",
    "        if logit_scale is None:\n",
    "            self.logit_scale = nn.Parameter(\n",
    "                torch.tensor([per_head_dim**-0.5]),\n",
    "                requires_grad=logit_scale_learnable,\n",
    "            )\n",
    "        else:\n",
    "            self.logit_scale = logit_scale\n",
    "\n",
    "        if self._warn_relative_position_bias and relative_position_bias is not None:\n",
    "            print(\n",
    "                \"Warning: Relative position bias is not used in Attention1D. \"\n",
    "                \"Use Attention3D for relative position bias.\"\n",
    "            )\n",
    "        self.relative_position_bias = relative_position_bias\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)\n",
    "\n",
    "    def _forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        \"\"\"Forward pass of the Attention1D module.\n",
    "\n",
    "        Terminology: T => number of tokens, b => batch size\n",
    "\n",
    "        Args:\n",
    "            query: Tensor of shape (b, T_q, dim_qk) representing the input to the query matrix.\n",
    "            key: Tensor of shape (b, T_kv, dim_qk) representing the input to the key matrix.\n",
    "            value: Tensor of shape (b, T_kv, dim_v) representing the input to the value matrix.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (b, T_q, dim_qk) representing output tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        def get_final_query_key_value(query, key, value):\n",
    "            \"\"\"Computing query, key, and value tokens after passing to the weight matrices. Useful for activation\n",
    "            checkpointing\"\"\"\n",
    "            query = self.W_q(query)\n",
    "            key = self.W_k(key)\n",
    "            value = self.W_v(value)\n",
    "\n",
    "            rearrange_partial = partial(rearrange, pattern=\"b T (num_heads d) -> b num_heads T d\")\n",
    "            query = rearrange_partial(query, num_heads=self.config.num_heads).contiguous()\n",
    "            key = rearrange_partial(key, num_heads=self.config.num_kv_heads).contiguous()\n",
    "            value = rearrange_partial(value, num_heads=self.config.num_kv_heads).contiguous()\n",
    "            # query: (b, num_heads, T, per_head_dim)\n",
    "            # key: (b, num_kv_heads, T, per_head_dim)\n",
    "            # value: (b, num_kv_heads, T, per_head_dim)\n",
    "\n",
    "            if isinstance(self.logit_scale, nn.Module):\n",
    "                logit_scale = self.logit_scale()\n",
    "            else:\n",
    "                logit_scale = self.logit_scale\n",
    "\n",
    "            query_normalized = F.normalize(query, dim=-1)\n",
    "            key_normalized = F.normalize(key, dim=-1)\n",
    "\n",
    "            query_normalized_and_scaled = query_normalized * logit_scale  # Scale the query beforehand\n",
    "\n",
    "            return query_normalized_and_scaled, key_normalized, value\n",
    "\n",
    "        query_normalized_and_scaled, key_normalized, value = self.checkpointing_level1(\n",
    "            get_final_query_key_value, query, key, value\n",
    "        )\n",
    "\n",
    "        relative_position_bias = None\n",
    "        if self.relative_position_bias is not None:\n",
    "            relative_position_bias = self.relative_position_bias()\n",
    "\n",
    "        # Split tensors into batches and perform attention\n",
    "        output = []\n",
    "        chunk_size = self.config.max_attention_batch_size\n",
    "        if chunk_size == -1:\n",
    "            chunk_size = query_normalized_and_scaled.size(0)\n",
    "        for query_normalized_and_scaled_chunk, key_normalized_chunk, value_chunk in zip(\n",
    "            torch.split(query_normalized_and_scaled, chunk_size, dim=0),\n",
    "            torch.split(key_normalized, chunk_size, dim=0),\n",
    "            torch.split(value, chunk_size, dim=0),\n",
    "        ):\n",
    "            torch250plus_kwargs = {}\n",
    "            if torch.__version__ >= \"2.5\":\n",
    "                torch250plus_kwargs[\"enable_gqa\"] = self.config.gqa_mqa_enabled\n",
    "\n",
    "            output_chunk = F.scaled_dot_product_attention(\n",
    "                query_normalized_and_scaled_chunk,\n",
    "                key_normalized_chunk,\n",
    "                value_chunk,\n",
    "                attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias\n",
    "                dropout_p=self.attn_drop_prob,\n",
    "                is_causal=False,\n",
    "                scale=1.0,  # Already scaled the vectors\n",
    "                **torch250plus_kwargs,\n",
    "            )\n",
    "            output.append(output_chunk)\n",
    "            # (chunk_size, num_heads, T, per_head_dim)\n",
    "        output = torch.cat(output, dim=0)\n",
    "        # (b, num_heads, T, per_head_dim)\n",
    "\n",
    "        output = rearrange(output, \"b num_heads T d -> b T (num_heads d)\").contiguous()\n",
    "        # (b, T, dim_qk)\n",
    "\n",
    "        def get_final_output(output):\n",
    "            \"\"\"Computing final output after projection. Useful for activation checkpointing\"\"\"\n",
    "            output = self.proj(output)\n",
    "            output = self.proj_drop(output)\n",
    "            return output\n",
    "\n",
    "        output = self.checkpointing_level1(get_final_output, output)\n",
    "        # (b, T, dim_qk)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level2(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m6\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m30\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention1D(dim=(30, 60), num_heads=6, ratio_q_to_kv_heads=2, logit_scale=4.0, max_attention_batch_size=2)\n",
    "q = torch.randn(6, 64, 30)\n",
    "k = torch.randn(6, 32, 30)\n",
    "v = torch.randn(6, 32, 60)\n",
    "\n",
    "display(test)\n",
    "display(test(q, k, v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class Attention3D(Attention1D):\n",
    "    \"\"\"Performs attention (MHA, GQA, and MQA) on 3D sequences. {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    _warn_relative_position_bias: bool = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Attention3DConfig = {},\n",
    "        relative_position_bias: RelativePositionEmbeddings | None = None,\n",
    "        logit_scale: float | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, relative_position_bias, logit_scale, checkpointing_level, **kwargs)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        channels_first: bool = True,\n",
    "    ):\n",
    "        \"\"\"Forward pass of the Attention3D module.\n",
    "\n",
    "        Terminology: z => depth, y => height, x => width, b => batch size\n",
    "\n",
    "        Args:\n",
    "            query: Tensor of shape (b, [dim_qk], z_q, y_q, x_q, [dim_qk]) representing the input to the query matrix.\n",
    "            key: Tensor of shape (b, [dim_qk], z_kv, y_kv, x_kv, [dim_qk]) representing the input to the key matrix.\n",
    "            value: Tensor of shape (b, [dim_v], z_kv, y_kv, x_kv, [dim_v]) representing the input to the value matrix.\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (b, [dim_qk], z_q, y_q, x_q, [dim_qk]) representing output tokens.\n",
    "\n",
    "        Constraints:\n",
    "            z_q * y_q * x_q = z_kv * y_kv * x_kv\n",
    "        \"\"\"\n",
    "        query = rearrange_channels(query, channels_first, False)\n",
    "        key = rearrange_channels(key, channels_first, False)\n",
    "        value = rearrange_channels(value, channels_first, False)\n",
    "        # Each is now (b, z, y, x, d)\n",
    "\n",
    "        z_q, y_q, x_q = query.shape[1:4]\n",
    "\n",
    "        query = rearrange(query, \"b z y x d -> b (z y x) d\").contiguous()\n",
    "        key = rearrange(key, \"b z y x d -> b (z y x) d\").contiguous()\n",
    "        value = rearrange(value, \"b z y x d -> b (z y x) d\").contiguous()\n",
    "        # Each is now (b, T, d)\n",
    "\n",
    "        output = super()._forward(query, key, value)\n",
    "        # (b, T, d)\n",
    "\n",
    "        output = rearrange(output, \"b (z y x) d -> b z y x d\", z=z_q, y=y_q, x=x_q).contiguous()\n",
    "        # (b, z, y, x, d)\n",
    "\n",
    "        output = rearrange_channels(output, False, channels_first)\n",
    "        # (b, [d], z, y, x, [d])\n",
    "\n",
    "        return output\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level2(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m60\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m30\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention3D(\n",
    "    config={\"dim\": (30, 60), \"num_heads\": 6, \"ratio_q_to_kv_heads\": 2},\n",
    "    checkpointing_level=5,\n",
    ")\n",
    "q = torch.randn(2, 30, 4, 4, 4)\n",
    "k = torch.randn(2, 30, 2, 4, 4)\n",
    "v = torch.randn(2, 60, 2, 4, 4)\n",
    "\n",
    "display(test)\n",
    "display(test(q, k, v, channels_first=True).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
