{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/maxvit_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.blocks.cnn import CNNBlock3D, CNNBlockConfig\n",
    "from vision_architectures.blocks.mbconv_3d import MBConv3D, MBConv3DConfig\n",
    "from vision_architectures.blocks.transformer import Attention3DWithMLPConfig\n",
    "from vision_architectures.nets.swinv2_3d import SwinV23DLayer\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, model_validator\n",
    "from vision_architectures.utils.rearrange import rearrange_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DStem0Config(CNNBlockConfig):\n",
    "    in_channels: int\n",
    "    kernel_size: int = 3\n",
    "    dim: int\n",
    "    depth: int = 2\n",
    "\n",
    "    out_channels: None = Field(None, description=\"This is defined by dim\")\n",
    "\n",
    "\n",
    "class MaxViT3DBlockConfig(MBConv3DConfig, Attention3DWithMLPConfig):\n",
    "    window_size: tuple[int, int, int]\n",
    "    modify_dims: bool = False  # Used in the last block of stems\n",
    "    out_dim_ratio: int = 2  # Used only if modify_dims is True\n",
    "\n",
    "\n",
    "class MaxViT3DStemConfig(MaxViT3DBlockConfig):\n",
    "    depth: int\n",
    "\n",
    "\n",
    "class MaxViT3DEncoderConfig(CustomBaseModel):\n",
    "    stem0: MaxViT3DStem0Config\n",
    "    stems: list[MaxViT3DStemConfig]\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.stem0.dim == self.stems[0].dim, \"Stem0 dim should be equal to the first stem dim\"\n",
    "        for i in range(1, len(self.stems)):\n",
    "            assert (\n",
    "                self.stems[i - 1].dim * self.stems[i - 1].out_dim_ratio == self.stems[i].dim\n",
    "            ), \"Stem dims should match\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DStem0(nn.Module):\n",
    "    def __init__(self, config: MaxViT3DStem0Config = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = MaxViT3DStem0Config.model_validate(config | kwargs)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            CNNBlock3D(\n",
    "                self.config.model_dump() | dict(out_channels=self.config.dim),\n",
    "                checkpointing_level,\n",
    "            )\n",
    "        )\n",
    "        for i in range(self.config.depth - 1):\n",
    "            self.layers.append(\n",
    "                CNNBlock3D(\n",
    "                    self.config.model_dump()\n",
    "                    | dict(\n",
    "                        in_channels=self.config.dim,\n",
    "                        out_channels=self.config.dim,\n",
    "                        normalization=self.config.normalization if i < self.config.depth - 1 else None,\n",
    "                        activation=self.config.activation if i < self.config.depth - 1 else None,\n",
    "                    ),\n",
    "                    checkpointing_level,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)\n",
    "\n",
    "    def _forward(self, x: torch.Tensor, channels_first: bool = True):\n",
    "        # x: (b, [in_channels], z, y, x, [in_channels])\n",
    "\n",
    "        x = rearrange_channels(x, channels_first, True)\n",
    "        # Now x is (b, in_channels, z, y, x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            # (b, dim, z1, y1, x1)\n",
    "\n",
    "        x = rearrange_channels(x, True, channels_first)\n",
    "        # (b, [dim], z1, y1, x1, [dim])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level2(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MaxViT3DStem0(in_channels=4, dim=8)\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(1, 4, 32, 32, 32)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DBlockAttention(SwinV23DLayer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MaxViT3DBlockAttention(\n",
    "    dim=12,\n",
    "    num_heads=2,\n",
    "    window_size=(4, 4, 4),\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(1, 12, 20, 20, 20)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DGridAttention(SwinV23DLayer):\n",
    "    \"\"\"Perform grid attention on the input tensor.\n",
    "\n",
    "    Note that the grid attention implementation differs from the paper where the image is being partitioned based on the\n",
    "    window size and not based on the number of windows. For example:\n",
    "\n",
    "    Let us say the input is\n",
    "\n",
    "    .. code-block:: text\n",
    "\n",
    "        A1 A2 A3 A4 A5 A6\n",
    "        B1 B2 B3 B4 B5 B6\n",
    "        C1 C2 C3 C4 C5 C6\n",
    "        D1 D2 D3 D4 D5 D6\n",
    "\n",
    "    Let us say the window size is 2x2. The grid attention will be performed on the following 6 windows:\n",
    "\n",
    "    .. code-block:: text\n",
    "\n",
    "        A1 A4  A2 A5  A3 A6\n",
    "        C1 C4  C2 C5  C3 C6\n",
    "\n",
    "        B1 B4  B2 B5  B3 B6\n",
    "        D1 D4  D2 D5  D3 D6\n",
    "\n",
    "    According to the paper, my understanding is that attention should have been applied on the following 4 windows:\n",
    "\n",
    "    .. code-block:: text\n",
    "\n",
    "        A1 A3 A5  A2 A4 A6\n",
    "        B1 B3 B5  B2 B4 B6\n",
    "\n",
    "        C1 C3 C5  C2 C4 C6\n",
    "        D1 D3 D5  D2 D4 D6\n",
    "\n",
    "    i.e. the first token of all 2x2 windows in block attention, the second token of all 2x2 windows in block attention\n",
    "    and so on.\n",
    "\n",
    "    This has been implemented different so as to limit the number of tokens to be attended to in a window, as if\n",
    "    utilized as per the paper, since 3D inputs are usually very large, the number of total windows in block attention\n",
    "    would be very large, leading to a very large number of tokens to attend to in each window in grid attention.\n",
    "\n",
    "    It would also cause problems when estimating the position embeddings as the grid size of the position embeddings\n",
    "    would vary very with every input size.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_rearrange_patterns():\n",
    "        forward_pattern = (\n",
    "            \"b (window_size_z num_windows_z) (window_size_y num_windows_y) (window_size_x num_windows_x) dim -> \"\n",
    "            \"(b num_windows_z num_windows_y num_windows_x) window_size_z window_size_y window_size_x dim \"\n",
    "        )\n",
    "        reverse_pattern = (\n",
    "            \"(b num_windows_z num_windows_y num_windows_x) window_size_z window_size_y window_size_x dim -> \"\n",
    "            \"b (window_size_z num_windows_z) (window_size_y num_windows_y) (window_size_x num_windows_x) dim\"\n",
    "        )\n",
    "        return forward_pattern, reverse_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MaxViT3DGridAttention(\n",
    "    dim=12,\n",
    "    num_heads=2,\n",
    "    window_size=(4, 4, 4),\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(1, 12, 20, 20, 20)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DBlock(nn.Module):\n",
    "    def __init__(self, config: MaxViT3DBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = MaxViT3DBlockConfig.model_validate(config | kwargs)\n",
    "\n",
    "        mbconv_kwargs = {}\n",
    "        out_dim = self.config.dim\n",
    "        if self.config.modify_dims:\n",
    "            out_dim = self.config.dim * self.config.out_dim_ratio\n",
    "            mbconv_kwargs[\"stride\"] = 2\n",
    "            mbconv_kwargs[\"padding\"] = 1\n",
    "            mbconv_kwargs[\"out_dim\"] = out_dim\n",
    "\n",
    "        self.mbconv = MBConv3D(self.config.model_dump(), checkpointing_level=checkpointing_level, **mbconv_kwargs)\n",
    "\n",
    "        # modify dim in the config\n",
    "        self.config = MaxViT3DBlockConfig.model_validate(self.config | {\"dim\": out_dim})\n",
    "\n",
    "        self.block_attention = MaxViT3DBlockAttention(self.config.model_dump(), checkpointing_level=checkpointing_level)\n",
    "        self.grid_attention = MaxViT3DGridAttention(self.config.model_dump(), checkpointing_level=checkpointing_level)\n",
    "\n",
    "        self.checkpointing_level3 = ActivationCheckpointing(3, checkpointing_level)\n",
    "\n",
    "    def _forward(self, x: torch.Tensor, channels_first: bool = True):\n",
    "        # x: (b, [dim], z, y, x, [dim])\n",
    "\n",
    "        x = self.mbconv(x, channels_first)  # this runs in channels_first format internally\n",
    "        # (b, [dim], z1, y1, x1, [dim])\n",
    "\n",
    "        x = rearrange_channels(x, channels_first, False)\n",
    "        # (b, z1, y1, x1, dim)\n",
    "\n",
    "        x = self.block_attention(x, channels_first=False)\n",
    "        # (b, z1, y1, x1, dim)\n",
    "        x = self.grid_attention(x, channels_first=False)\n",
    "        # (b, z1, y1, x1, dim)\n",
    "\n",
    "        x = rearrange_channels(x, False, channels_first)\n",
    "        # (b, [dim], z1, y1, x1, [dim])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level3(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MaxViT3DBlock(dim=12, num_heads=3, window_size=(4, 4, 4))\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 12, 20, 20, 20)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MaxViT3DBlock(dim=12, num_heads=3, window_size=(2, 2, 2), modify_dims=True)\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 12, 20, 20, 20)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DStem(nn.Module):\n",
    "    def __init__(\n",
    "        self, config: MaxViT3DStemConfig = {}, checkpointing_level: int = 0, dont_downsample: bool = False, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = MaxViT3DStemConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            MaxViT3DBlock(\n",
    "                self.config.model_dump(),\n",
    "                checkpointing_level=checkpointing_level,\n",
    "                modify_dims=True if i == self.config.depth - 1 and not dont_downsample else False,\n",
    "            )\n",
    "            for i in range(self.config.depth)\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    def _forward(self, x: torch.Tensor, channels_first: bool = True):\n",
    "        # x: (b, [dim], z, y, x, [dim])\n",
    "        for layer in self.blocks:\n",
    "            x = layer(x, channels_first)\n",
    "        return x\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level4(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MaxViT3DStem(\n",
    "    dim=12,\n",
    "    num_heads=3,\n",
    "    depth=3,\n",
    "    window_size=(2, 2, 2),\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 12, 20, 20, 20)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MaxViT3DStem(\n",
    "    dim=12,\n",
    "    num_heads=3,\n",
    "    depth=3,\n",
    "    window_size=(2, 2, 2),\n",
    "    dont_downsample=True,\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 12, 20, 20, 20)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DEncoder(nn.Module):\n",
    "    def __init__(self, config: MaxViT3DEncoderConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = MaxViT3DEncoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.stems = nn.ModuleList([])\n",
    "        self.stems.append(MaxViT3DStem0(self.config.stem0, checkpointing_level))\n",
    "        for i, stem_config in enumerate(self.config.stems):\n",
    "            self.stems.append(\n",
    "                MaxViT3DStem(stem_config, checkpointing_level, dont_downsample=i == len(self.config.stems) - 1)\n",
    "            )\n",
    "\n",
    "        self.checkpointing_level5 = ActivationCheckpointing(5, checkpointing_level)\n",
    "\n",
    "    def _forward(self, x: torch.Tensor, return_intermediates: bool = False, channels_first: bool = True):\n",
    "        # x: (b, [in_channels], z, y, x, [in_channels])\n",
    "        features = []\n",
    "        for stem in self.stems:\n",
    "            x = stem(x, channels_first)\n",
    "            features.append(x)\n",
    "        if return_intermediates:\n",
    "            return x, features\n",
    "        return x\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level5(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MaxViT3DEncoder(\n",
    "    stem0={\n",
    "        \"in_channels\": 1,\n",
    "        \"dim\": 12,\n",
    "    },\n",
    "    stems=[\n",
    "        {\n",
    "            \"dim\": 12,\n",
    "            \"num_heads\": 3,\n",
    "            \"window_size\": (2, 2, 2),\n",
    "            \"depth\": 3,\n",
    "        },\n",
    "        {\n",
    "            \"dim\": 24,\n",
    "            \"num_heads\": 3,\n",
    "            \"window_size\": (2, 2, 2),\n",
    "            \"depth\": 3,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 1, 32, 32, 32)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
