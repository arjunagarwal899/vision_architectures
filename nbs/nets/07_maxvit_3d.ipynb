{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/maxvit_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "# from vision_architectures.blocks.se import SEBlock3D\n",
    "# from vision_architectures.blocks.cnn import CNNBlock3D\n",
    "# from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "# from vision_architectures.utils.residuals import StochasticDepthResidual\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DConfig(CustomBaseModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViTMBConv3D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding,\n",
    "        is_first=False,\n",
    "        expand_ratio=6,\n",
    "        reduction=4,  # squeeze excitation 1/4 = 0.25\n",
    "        survival_prob=0.8,  # for stocastic depth\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        stride = 2 if is_first else 1\n",
    "        survival_prob = 0.8\n",
    "        self.use_residual = True if not is_first else False\n",
    "        hidden_dim = int(out_channels * expand_ratio)\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "        padding = padding\n",
    "\n",
    "        # expansion phase\n",
    "        self.expand = nn.Identity() if (expand_ratio == 1) else MaxViTCNNBlock3D(in_channels, hidden_dim, kernel_size=1)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        self.depthwise_conv = MaxViTCNNBlock3D(\n",
    "            hidden_dim,\n",
    "            hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=hidden_dim,\n",
    "        )\n",
    "\n",
    "        # Squeeze Excitation phase\n",
    "        self.se = MaxViTSqueezeExcitation(hidden_dim, reduced_dim=reduced_dim)\n",
    "\n",
    "        # output phase\n",
    "        self.pointwise_conv = MaxViTCNNBlock3D(hidden_dim, out_channels, kernel_size=1, stride=1, act=False, padding=0)\n",
    "        # add Sigmoid Activation as mentioned in the paper\n",
    "\n",
    "        # drop connect\n",
    "        self.drop_layers = MaxViTStochasticDepth(survival_prob=survival_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Not 1st MBConv | 1st MBConv\n",
    "        residual = x\n",
    "        # (b, d, x, y, z) | (b, d, x, y, z)\n",
    "        x = self.expand(x)\n",
    "        # (b, 6d, x, y, z) | (b, 6d, x, y, z)\n",
    "        x = self.depthwise_conv(x)\n",
    "        # (b, 6d, x/2, y/2, z/2) | (b, 6d, x, y, z)\n",
    "        x = self.se(x)\n",
    "        # (b, 6d, x/2, y/2, z/2) | (b, 6d, x, y, z)\n",
    "        x = self.pointwise_conv(x)\n",
    "        # b, d,x,y,z | b,2d,x/2,y/2,z/2\n",
    "        if self.use_residual:\n",
    "            x = self.drop_layers(x)\n",
    "            x += residual\n",
    "        # b, d,x,y,z | b,2d,x/2,y/2,z/2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DMHSA(nn.Module):\n",
    "    def __init__(self, dim, dim_per_head=32, dropout=0.0, window_size=(7, 7, 7), bias=False):\n",
    "        super().__init__()\n",
    "        assert (dim % dim_per_head) == 0, \"dimension should be divisible by dimension per head\"\n",
    "\n",
    "        self.heads = dim // dim_per_head\n",
    "        self.scale = dim_per_head**-0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=bias)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Linear(dim, dim, bias=bias), nn.Dropout(dropout))\n",
    "\n",
    "        # relative positional bias\n",
    "        w1, w2, w3 = window_size\n",
    "        self.rel_pos_bias = nn.Embedding((2 * w1 - 1) * (2 * w2 - 1) * (2 * w3 - 1), self.heads)\n",
    "        pos1 = torch.arange(w1, dtype=torch.int32)\n",
    "        pos2 = torch.arange(w2, dtype=torch.int32)\n",
    "        pos3 = torch.arange(w3, dtype=torch.int32)\n",
    "        # First we use the torch.arange and torch.meshgrid functions to generate the corresponding coordinates, [3,H,W,D]\n",
    "        # and then stack them up and expand them into a two-dimensional vector to get the absolute position index.\n",
    "        grid = torch.stack(torch.meshgrid(pos1, pos2, pos3, indexing=\"ij\"))\n",
    "        grid = rearrange(grid, \"c i j k -> (i j k) c\").contiguous()\n",
    "        # insert a dimension in the first dimension and the second dimension respectively, perform broadcast subtraction, and obtain the tensor of 3, whd*ww, whd*ww\n",
    "        rel_pos = rearrange(grid, \"i ... -> i 1 ...\").contiguous() - rearrange(grid, \"j ... -> 1 j ...\").contiguous()\n",
    "        rel_pos[..., 0] += w1 - 1\n",
    "        rel_pos[..., 1] += w2 - 1\n",
    "        rel_pos[..., 2] += w3 - 1\n",
    "        # Do a multiplication operation to distinguish, sum up the last dimension, and expand it into a one-dimensional coordinate   a*x1 + b*x2 + c*x3  (a= hd b=d c =1)\n",
    "        rel_pos_indices = (rel_pos * torch.tensor([(2 * w2 - 1) * (2 * w3 - 1), (2 * w3 - 1), 1])).sum(dim=-1)\n",
    "\n",
    "        # Register as a variable that does not participate in learning\n",
    "        self.register_buffer(\"rel_pos_indices\", rel_pos_indices, persistent=False)\n",
    "        self.dropout_prob = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, height, width, depth, window_height, window_width, window_depth, _ = x.shape\n",
    "        h = self.heads\n",
    "\n",
    "        # b, x/w1, y/w2, z/w3, w1, w2, w3, d\n",
    "        x = rearrange(x, \"b x y z w1 w2 w3 d -> (b x y z) (w1 w2 w3) d\").contiguous()\n",
    "        # total_b, total_w, d\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d ) -> b h n d\", h=h).contiguous(), (q, k, v))  # split_heads\n",
    "\n",
    "        # Calculate rel_pos_bias\n",
    "        rel_pos_bias = rearrange(self.rel_pos_bias(self.rel_pos_indices), \"i j h -> h i j\").contiguous()\n",
    "\n",
    "        # total_b, num_heads, total_w, d/num_heads\n",
    "        context = F.scaled_dot_product_attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            attn_mask=rel_pos_bias,  # Use this as a way to introduce relative position bias\n",
    "            dropout_p=self.dropout_prob,\n",
    "            is_causal=False,\n",
    "            scale=self.scale,  # Already scaled the vectors\n",
    "        )\n",
    "\n",
    "        # total_b, num_heads, total_w, d/num_heads\n",
    "        out = rearrange(\n",
    "            context,\n",
    "            \"b h (w1 w2 w3) d -> b w1 w2 w3 (h d)\",\n",
    "            w1=window_height,\n",
    "            w2=window_width,\n",
    "            w3=window_depth,\n",
    "        ).contiguous()  # merge heads\n",
    "\n",
    "        # total_b, w1, w2 ,w3, d\n",
    "        out = self.to_out(out)  # combine heads out\n",
    "        out = rearrange(out, \"(b x y z) ... -> b x y z ...\", x=height, y=width, z=depth).contiguous()\n",
    "        # b, x/w1, y/w2, z/w3, w1, w2, w3, d\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DStem0(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.conv_stem = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                self.config[\"in_channels\"],\n",
    "                self.config[\"hidden_dim\"],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=self.config[\"bias\"],\n",
    "            ),\n",
    "            nn.Conv3d(\n",
    "                self.config[\"hidden_dim\"],\n",
    "                self.config[\"out_channels\"],\n",
    "                3,\n",
    "                padding=1,\n",
    "                bias=self.config[\"bias\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 54, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_stem0_config = {\n",
    "    \"in_channels\": 54,  # dimension of first layer, doubles every layer\n",
    "    \"hidden_dim\": 98,  # dimension of attention heads, kept at 32 in paper`\n",
    "    \"out_channels\": 54,  # window size for block and grids\n",
    "    \"bias\": True,\n",
    "}\n",
    "\n",
    "maxvit_block = MaxViT3DStem0(test_stem0_config)\n",
    "img = torch.randn(2, 54, 32, 32, 24)\n",
    "preds = maxvit_block(img)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DBlock(nn.Module):\n",
    "    def __init__(self, block_config, is_first=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1, self.w2, self.w3 = block_config[\"window_size\"]\n",
    "        stem_dim_in = block_config[\"stem_dim_in\"]\n",
    "        dim = block_config[\"dim\"]\n",
    "        dim_per_head = block_config[\"dim_per_head\"]\n",
    "        dropout = block_config[\"dropout\"]\n",
    "        window_size = block_config[\"window_size\"]\n",
    "        expansion_rate = block_config[\"expansion_rate\"]\n",
    "        shrinkage_rate = block_config[\"shrinkage_rate\"]\n",
    "        bias = block_config[\"bias\"]\n",
    "\n",
    "        self.MBConv = MaxViTMBConv3D(\n",
    "            in_channels=stem_dim_in if is_first else dim,\n",
    "            out_channels=dim,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            is_first=is_first,\n",
    "            expand_ratio=expansion_rate if expansion_rate is not None else 4,\n",
    "            reduction=shrinkage_rate if shrinkage_rate is not None else 4,  # squeeze excitation 1/4 = 0.25\n",
    "            survival_prob=1 - dropout,  # for stocastic depth\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(dim)\n",
    "        self.blockAttn = MaxViT3DMHSA(\n",
    "            dim=dim,\n",
    "            dim_per_head=dim_per_head,\n",
    "            dropout=dropout,\n",
    "            window_size=window_size,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        self.layernorm2 = nn.LayerNorm(dim)\n",
    "        self.FFN1 = MaxViT3DMLP(dim=dim, dropout=dropout, bias=bias)\n",
    "\n",
    "        self.layernorm3 = nn.LayerNorm(dim)\n",
    "        self.gridAttn = MaxViT3DMHSA(\n",
    "            dim=dim,\n",
    "            dim_per_head=dim_per_head,\n",
    "            dropout=dropout,\n",
    "            window_size=window_size,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.layernorm4 = nn.LayerNorm(dim)\n",
    "        self.FFN2 = MaxViT3DMLP(dim=dim, dropout=dropout, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # b,d,x,y,z | b,d/2,2x,2y,2z for first MBConv of stem\n",
    "        x = self.MBConv(x)\n",
    "        # b,d,x,y,z\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b d (x w1) (y w2) (z w3) -> b x y z w1 w2 w3 d\",\n",
    "            w1=self.w1,\n",
    "            w2=self.w2,\n",
    "            w3=self.w3,\n",
    "        ).contiguous()  # block-like attention\n",
    "        # b,x/w1,y/w2,z/w3,w1,w2,w3,d\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.blockAttn(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.FFN1(x)\n",
    "        # b,x/w1,y/w2,z/w3,w1,w2,w3,d\n",
    "        x = rearrange(x, \"b x y z w1 w2 w3 d -> b d (x w1) (y w2) (z w3)\").contiguous()\n",
    "        # b,d,x,y,z\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b d (w1 x) (w2 y) (w3 z) -> b x y z w1 w2 w3 d\",\n",
    "            w1=self.w1,\n",
    "            w2=self.w2,\n",
    "            w3=self.w3,\n",
    "        ).contiguous()  # grid-like attention\n",
    "        # b,x/w1,y/w2,z/w3,w1,w2,w3,d\n",
    "        x = self.layernorm3(x)\n",
    "        x = x + self.gridAttn(x)\n",
    "        x = self.layernorm4(x)\n",
    "        x = x + self.FFN2(x)\n",
    "        # b,x/w1,y/w2,z/w3,w1,w2,w3,d\n",
    "        x = rearrange(x, \"b x y z w1 w2 w3 d -> b d (w1 x) (w2 y) (w3 z)\").contiguous()\n",
    "        # b,d,x,y,z\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_block_config = {\n",
    "    \"stem_dim_in\": 64,  # used in first block to downsample z,x,y\n",
    "    \"dim\": 128,  # dimension of whole layer, doubles every layer\n",
    "    \"dim_per_head\": 8,  # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4, 4, 4),  # window size for block and grids\n",
    "    \"dropout\": 0.1,  # dropout\n",
    "    \"expansion_rate\": None,  # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,  # (squeeze) and excitation\n",
    "    \"bias\": True,\n",
    "}\n",
    "\n",
    "maxvit_block = MaxViT3DBlock(test_block_config, is_first=True)\n",
    "img = torch.randn(2, 64, 32, 32, 24)\n",
    "preds = maxvit_block(img)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 32, 32, 24])\n"
     ]
    }
   ],
   "source": [
    "test_block_config = {\n",
    "    \"stem_dim_in\": 64,  # used in first block to downsample z,x,y\n",
    "    \"dim\": 128,  # dimension of whole layer, doubles every layer\n",
    "    \"dim_per_head\": 8,  # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4, 4, 4),  # window size for block and grids\n",
    "    \"dropout\": 0.1,  # dropout\n",
    "    \"expansion_rate\": None,  # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,  # (squeeze) and excitation\n",
    "    \"bias\": False,\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    maxvit_block = MaxViT3DBlock(test_block_config, is_first=False)\n",
    "    img = torch.randn(2, 128, 32, 32, 24)\n",
    "    preds = maxvit_block(img)\n",
    "    print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DStem(nn.Module):\n",
    "    def __init__(self, stem_config):\n",
    "        super().__init__()\n",
    "        self.stem = nn.ModuleList(\n",
    "            [MaxViT3DBlock(stem_config, is_first=(i == 0)) for i in range(stem_config[\"num_maxvit_blocks\"])]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        for layer in self.stem:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_stem_config = {\n",
    "    \"num_maxvit_blocks\": 1,\n",
    "    \"stem_dim_in\": 32,\n",
    "    \"dim\": 64,  # dimension of stem, doubles every stem\n",
    "    \"dim_per_head\": 8,  # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4, 4, 4),  # window size for block and grids\n",
    "    \"dropout\": 0.1,  # dropout\n",
    "    \"expansion_rate\": None,  # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,  # (squeeze) and excitation\n",
    "    \"bias\": True,\n",
    "}\n",
    "\n",
    "test = MaxViT3DStem(test_stem_config)\n",
    "img = torch.randn(2, 32, 32, 32, 24)\n",
    "preds = test(img)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MaxViT3DEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.stems = nn.ModuleList([])\n",
    "\n",
    "        self.stems.append(MaxViT3DStem0(config[\"stem0\"]))\n",
    "\n",
    "        for stage_config in config[\"stems\"]:\n",
    "            self.stems.append(MaxViT3DStem(stage_config))\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        for stem in self.stems:\n",
    "            hidden_states = stem(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "test_encoder_config = {\n",
    "    \"stem0\": {\"in_channels\": 1, \"hidden_dim\": 32, \"out_channels\": 32, \"bias\": True},\n",
    "    \"stems\": [\n",
    "        {\n",
    "            \"num_maxvit_blocks\": 2,\n",
    "            \"stem_dim_in\": 32,\n",
    "            \"dim\": 64,  # dimension of first layer, doubles every layer\n",
    "            \"dim_per_head\": 8,  # dimension of attention heads, kept at 32 in paper`\n",
    "            \"window_size\": (4, 4, 4),  # window size for block and grids\n",
    "            \"dropout\": 0.1,  # dropout\n",
    "            \"expansion_rate\": None,  # squeeze and (excitation)\n",
    "            \"shrinkage_rate\": None,  # (squeeze) and excitation\n",
    "            \"bias\": True,\n",
    "        },\n",
    "        {\n",
    "            \"num_maxvit_blocks\": 2,\n",
    "            \"stem_dim_in\": 64,\n",
    "            \"dim\": 128,  # dimension of first layer, doubles every layer\n",
    "            \"dim_per_head\": 8,  # dimension of attention heads, kept at 32 in paper`\n",
    "            \"window_size\": (4, 4, 4),  # window size for block and grids\n",
    "            \"dropout\": 0.1,  # dropout\n",
    "            \"expansion_rate\": None,  # squeeze and (excitation)\n",
    "            \"shrinkage_rate\": None,  # (squeeze) and excitation }\n",
    "            \"bias\": True,\n",
    "        },\n",
    "        {\n",
    "            \"num_maxvit_blocks\": 2,\n",
    "            \"stem_dim_in\": 128,\n",
    "            \"dim\": 256,  # dimension of first layer, doubles every layer\n",
    "            \"dim_per_head\": 8,  # dimension of attention heads, kept at 32 in paper`\n",
    "            \"window_size\": (2, 2, 2),  # window size for block and grids\n",
    "            \"dropout\": 0.1,  # dropout\n",
    "            \"expansion_rate\": None,  # squeeze and (excitation)\n",
    "            \"shrinkage_rate\": None,  # (squeeze) and excitation }\n",
    "            \"bias\": True,\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MaxViT3DEncoder(test_encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.randn(2, 1, 32, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
