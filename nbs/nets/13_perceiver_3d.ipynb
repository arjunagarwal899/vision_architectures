{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/perceiver_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.blocks.transformer import Attention3DWithMLP, Attention3DWithMLPConfig\n",
    "from vision_architectures.layers.embeddings import (\n",
    "    AbsolutePositionEmbeddings3D,\n",
    "    RelativePositionEmbeddings3D,\n",
    "    RelativePositionEmbeddings3DConfig,\n",
    ")\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, model_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DChannelMappingConfig(CustomBaseModel):\n",
    "    in_channels: int | set[int]\n",
    "    out_channels: int\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderEncodeConfig(Attention3DWithMLPConfig):\n",
    "    dim: int\n",
    "    num_layers: int\n",
    "    latent_grid_size: tuple[int, int, int]\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderProcessConfig(Attention3DWithMLPConfig):\n",
    "    dim: int\n",
    "    num_layers: int\n",
    "    latent_grid_size: tuple[int, int, int] | None\n",
    "    use_relative_position_embeddings: bool = True  # can help with self attention\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if self.use_relative_position_embeddings:\n",
    "            assert (\n",
    "                self.latent_grid_size is not None\n",
    "            ), \"latent_grid_size must be provided if using relative position embeddings\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderConfig(CustomBaseModel):\n",
    "    encode: Perceiver3DEncoderEncodeConfig\n",
    "    process: Perceiver3DEncoderProcessConfig\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.encode.dim\n",
    "\n",
    "    @property\n",
    "    def latent_grid_size(self):\n",
    "        return self.encode.latent_grid_size\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data):\n",
    "        super().validate_before(data)\n",
    "        if isinstance(data, dict):\n",
    "            data.setdefault(\"encode\", {})\n",
    "            data.setdefault(\"process\", {})\n",
    "            for key, value in data.items():\n",
    "                if key in {\"encode\", \"process\", \"decode\"}:\n",
    "                    continue\n",
    "                data[\"encode\"].setdefault(key, value)\n",
    "                data[\"process\"].setdefault(key, value)\n",
    "        return data\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.encode.dim == self.process.dim, \"encode and process dims must be equal\"\n",
    "        assert (\n",
    "            self.encode.latent_grid_size == self.process.latent_grid_size\n",
    "        ), \"encode and process latent_grid_size must be equal\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class Perceiver3DDecoderConfig(Attention3DWithMLPConfig):\n",
    "    dim: int\n",
    "    num_layers: int\n",
    "    out_channels: int\n",
    "    use_absolute_position_embeddings: bool = True\n",
    "\n",
    "\n",
    "class Perceiver3DConfig(Perceiver3DEncoderConfig):\n",
    "    decode: Perceiver3DDecoderConfig\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def validate_before(cls, data):\n",
    "        super().validate_before(data)\n",
    "        if isinstance(data, dict):\n",
    "            data.setdefault(\"decode\", {})\n",
    "            for key, value in data.items():\n",
    "                if key in {\"encode\", \"process\", \"decode\"}:\n",
    "                    continue\n",
    "                data[\"decode\"].setdefault(key, value)\n",
    "        return data\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.encode.dim == self.decode.dim, \"encode and decode dims must be equal\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m384\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_config = Perceiver3DConfig.model_validate(\n",
    "    {\n",
    "        \"dim\": 384,\n",
    "        \"latent_grid_size\": (8, 8, 8),\n",
    "        \"num_heads\": 16,\n",
    "        \"encode\": {\n",
    "            \"num_layers\": 1,\n",
    "        },\n",
    "        \"process\": {\n",
    "            \"num_layers\": 3,\n",
    "        },\n",
    "        \"decode\": {\n",
    "            \"out_channels\": 1,\n",
    "            \"num_layers\": 3,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "test_config.dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def unfold_with_roll_3d(\n",
    "    ten: torch.Tensor,\n",
    "    window_size: tuple[int, int, int] | None,\n",
    "    stride: tuple[int, int, int] | None,\n",
    "    raise_large_window_error: bool = False,\n",
    "    raise_large_stride_error: bool = True,\n",
    "):\n",
    "    if window_size is None or stride is None:\n",
    "        return ten.unsqueeze(0), torch.tensor([[0, 0, 0]], device=ten.device)\n",
    "\n",
    "    window_size = list(window_size)\n",
    "    for i in range(3):\n",
    "        if window_size[i] > ten.shape[i + 2]:\n",
    "            if raise_large_window_error:\n",
    "                raise ValueError(f\"window_size[{i}] must be less than or equal to {ten.shape[i+2]}\")\n",
    "            window_size[i] = ten.shape[i + 2]\n",
    "        if stride[i] > window_size[i] and raise_large_stride_error:\n",
    "            raise ValueError(f\"stride[{i}] must be less than or equal to window_size[{i}]\")\n",
    "    window_size = tuple(window_size)\n",
    "\n",
    "    _, _, z, y, x = ten.shape\n",
    "    wz, wy, wx = window_size\n",
    "    sz, sy, sx = stride\n",
    "\n",
    "    positions_z = torch.arange(0, z, sz, device=ten.device)\n",
    "    positions_y = torch.arange(0, y, sy, device=ten.device)\n",
    "    positions_x = torch.arange(0, x, sx, device=ten.device)\n",
    "    positions = torch.stack(torch.meshgrid(positions_z, positions_y, positions_x, indexing=\"ij\"), dim=-1)\n",
    "    positions = rearrange(positions, \"z y x three -> (z y x) three\").contiguous()\n",
    "\n",
    "    # If required, number of patches along each dimension is calculated here:\n",
    "    # nz = positions_z.shape[0]\n",
    "    # ny = positions_y.shape[0]\n",
    "    # nx = positions_x.shape[0]\n",
    "    # n = nz * ny * nx\n",
    "\n",
    "    pad_z = positions_z[-1] + wz - z\n",
    "    pad_y = positions_y[-1] + wy - y\n",
    "    pad_x = positions_x[-1] + wx - x\n",
    "    if pad_z > 0:\n",
    "        ten = torch.cat([ten, ten[:, :, :pad_z]], dim=2)\n",
    "    if pad_y > 0:\n",
    "        ten = torch.cat([ten, ten[:, :, :, :pad_y]], dim=3)\n",
    "    if pad_x > 0:\n",
    "        ten = torch.cat([ten, ten[:, :, :, :, :pad_x]], dim=4)\n",
    "\n",
    "    windows = []\n",
    "    for i, j, k in positions:\n",
    "        window = ten[:, :, i : i + wz, j : j + wy, k : k + wx]\n",
    "        windows.append(window)\n",
    "    windows = torch.stack(windows, dim=0)\n",
    "\n",
    "    return windows, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten = torch.arange(64).reshape(1, 1, 4, 4, 4)\n",
    "window_size = (5, 3, 2)\n",
    "stride = (2, 3, 2)\n",
    "\n",
    "unfolded, positions = unfold_with_roll_3d(ten, window_size, stride)\n",
    "unfolded.shape, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def fold_back_3d(\n",
    "    windows: torch.Tensor,\n",
    "    positions: torch.Tensor,\n",
    "    output_shape: tuple[int, int, int],\n",
    "    reduction=\"mean\",\n",
    "):\n",
    "    z, y, x = output_shape\n",
    "    b, d, wz, wy, wx = windows.shape[1:]\n",
    "\n",
    "    ez, ey, ex = positions[-1] + torch.tensor([wz, wy, wx], device=windows.device)\n",
    "\n",
    "    output = torch.zeros(b, d, ez, ey, ex, dtype=windows.dtype, device=windows.device)\n",
    "    count = torch.zeros(b, d, ez, ey, ex, dtype=windows.dtype, device=windows.device)\n",
    "\n",
    "    for (pz, py, px), window in zip(positions, windows):\n",
    "        output[:, :, pz : pz + wz, py : py + wy, px : px + wx] += window\n",
    "        count[:, :, pz : pz + wz, py : py + wy, px : px + wx] += 1\n",
    "\n",
    "    if ez > z:\n",
    "        pad_z = ez - z\n",
    "        output[:, :, :pad_z] += output[:, :, -pad_z:]\n",
    "        count[:, :, :pad_z] += count[:, :, -pad_z:]\n",
    "        output = output[:, :, :z]\n",
    "        count = count[:, :, :z]\n",
    "    if ey > y:\n",
    "        pad_y = ey - y\n",
    "        output[:, :, :, :pad_y] += output[:, :, :, -pad_y:]\n",
    "        count[:, :, :, :pad_y] += count[:, :, :, -pad_y:]\n",
    "        output = output[:, :, :, :y]\n",
    "        count = count[:, :, :, :y]\n",
    "    if ex > x:\n",
    "        pad_x = ex - x\n",
    "        output[:, :, :, :, :pad_x] += output[:, :, :, :, -pad_x:]\n",
    "        count[:, :, :, :, :pad_x] += count[:, :, :, :, -pad_x:]\n",
    "        output = output[:, :, :, :, :x]\n",
    "        count = count[:, :, :, :, :x]\n",
    "\n",
    "    if reduction == \"sum\":\n",
    "        pass\n",
    "    elif reduction == \"mean\":\n",
    "        output = output / count\n",
    "    else:\n",
    "        raise NotImplementedError(f\"reduction={reduction} is not implemented\")\n",
    "\n",
    "    output = output.type_as(windows)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folded = fold_back_3d(unfolded, positions, ten.shape[2:])\n",
    "\n",
    "assert torch.allclose(folded, ten), \"Fold-back is not working\"\n",
    "folded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DChannelMapping(nn.Module):\n",
    "    def __init__(self, config: Perceiver3DChannelMappingConfig = {}, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Perceiver3DChannelMappingConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.in_channels = self.config.in_channels\n",
    "        self.out_channels = self.config.out_channels\n",
    "\n",
    "        if isinstance(self.in_channels, int):\n",
    "            self.in_channels = {self.in_channels}\n",
    "\n",
    "        self.mappers = nn.ModuleDict()\n",
    "        for in_channels in self.in_channels:\n",
    "            self.mappers[str(in_channels)] = nn.Conv3d(in_channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (b, in_channels, z, y, x)\n",
    "\n",
    "        in_channels = x.shape[1]\n",
    "        if in_channels not in self.in_channels:\n",
    "            raise ValueError(f\"Input channels {in_channels} not in {self.in_channels}\")\n",
    "\n",
    "        mapper = self.mappers[str(in_channels)]\n",
    "        x = mapper(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 4, 4, 4])\n",
      "torch.Size([1, 24, 4, 4, 4])\n",
      "torch.Size([1, 24, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "test = Perceiver3DChannelMapping(in_channels={12, 24, 48}, out_channels=24)\n",
    "for in_channels in [12, 24, 48]:\n",
    "    ten = torch.randn(1, in_channels, 4, 4, 4)\n",
    "    print(test(ten).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderEncode(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Perceiver3DEncoderEncodeConfig | Perceiver3DEncoderConfig = {},\n",
    "        channel_mapping: Perceiver3DChannelMapping | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(config, Perceiver3DEncoderConfig):\n",
    "            config = config.encode\n",
    "        if \"encode\" in config:\n",
    "            config = config[\"encode\"]\n",
    "\n",
    "        self.config = Perceiver3DEncoderEncodeConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        latent_grid_size = self.config.latent_grid_size\n",
    "        num_layers = self.config.num_layers\n",
    "\n",
    "        self.latent_tokens = nn.Parameter(torch.empty(dim, *latent_grid_size), requires_grad=True)\n",
    "        nn.init.xavier_uniform_(self.latent_tokens)\n",
    "\n",
    "        self.position_embeddings = AbsolutePositionEmbeddings3D(dim=dim, grid_size=latent_grid_size, learnable=False)\n",
    "\n",
    "        self.channel_mapping = channel_mapping\n",
    "\n",
    "        self.cross_attention = nn.ModuleList(\n",
    "            [\n",
    "                Attention3DWithMLP(self.config.model_dump(), checkpointing_level=checkpointing_level)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        x: torch.Tensor | list[torch.Tensor],\n",
    "        sliding_window: tuple[int, int, int] | None = None,\n",
    "        sliding_stride: tuple[int, int, int] | None = None,\n",
    "        return_all: bool = False,\n",
    "    ) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "        # x: [(b, in_channels, z, y, x), ...]\n",
    "\n",
    "        # Prepare keys and values\n",
    "        def prepare_keys_values(x: torch.Tensor | list[torch.Tensor]):\n",
    "            if not isinstance(x, list):\n",
    "                x = [x]\n",
    "            # x is now a list of tensors\n",
    "            kvs = []\n",
    "            for i in range(len(x)):\n",
    "                if self.channel_mapping is not None:\n",
    "                    mapped = self.channel_mapping(x[i])  # modifying in-place leads to errors when checkpointing\n",
    "                else:\n",
    "                    mapped = x[i]\n",
    "                # (b, dim, z, y, x)\n",
    "                mapped_windows, _ = unfold_with_roll_3d(mapped, sliding_window, sliding_stride)\n",
    "                # (num_windows, b, dim, *sliding_window])\n",
    "                kvs.append(mapped_windows)\n",
    "            return kvs\n",
    "\n",
    "        kvs = self.checkpointing_level1(prepare_keys_values, x)\n",
    "        # list of (num_windows, b, dim, *sliding_window)\n",
    "\n",
    "        # Prepare queries\n",
    "        b = kvs[0].shape[1]\n",
    "        q = repeat(self.latent_tokens, \"d zl yl xl -> b d zl yl xl\", b=b)\n",
    "        if self.position_embeddings is not None:\n",
    "            q = self.position_embeddings(q, device=q.device)\n",
    "        # (b, dim, zl, yl, xl)\n",
    "\n",
    "        # Perform attention\n",
    "        embeddings = []\n",
    "        for cross_attention_layer in self.cross_attention:\n",
    "            embedding = torch.zeros_like(q)\n",
    "            for kv_windows in kvs:\n",
    "                for kv_window in kv_windows:\n",
    "                    embedding_window = cross_attention_layer(q, kv_window, kv_window)\n",
    "                    embedding = embedding + embedding_window\n",
    "            q = embedding  # To pass to the next layer\n",
    "            embeddings.append(embedding)\n",
    "        # (b, latent_grid_size, dim)\n",
    "\n",
    "        return_value = embeddings[-1]\n",
    "        if return_all:\n",
    "            return_value = {\n",
    "                \"embeddings\": return_value,\n",
    "                \"all_embeddings\": embeddings,\n",
    "            }\n",
    "        return return_value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor | list[torch.Tensor],\n",
    "        sliding_window: int | None = None,  # Sliding window may be beneficial during inference time\n",
    "        sliding_stride: int | None = None,\n",
    "        return_all: bool = False,\n",
    "    ):\n",
    "        return self.checkpointing_level4(self._forward, x, sliding_window, sliding_stride, return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPerceiver3DEncoderEncode\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mchannel_mapping\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DChannelMapping\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mmappers\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcross_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.37 s, sys: 74.5 ms, total: 1.45 s\n",
      "Wall time: 37.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test = Perceiver3DEncoderEncode(test_config, Perceiver3DChannelMapping(in_channels=512, out_channels=384))\n",
    "display(test)\n",
    "o = test(torch.randn(2, 512, 4, 4, 4))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.83 s, sys: 245 ms, total: 3.08 s\n",
      "Wall time: 57.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test.eval()\n",
    "with torch.no_grad():\n",
    "    o = test(torch.randn(2, 512, 12, 12, 12), sliding_window=(8, 8, 8), sliding_stride=(8, 8, 8))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderProcess(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Perceiver3DEncoderProcessConfig | Perceiver3DEncoderConfig = {},\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(config, Perceiver3DEncoderConfig):\n",
    "            config = config.process\n",
    "        if \"process\" in config:\n",
    "            config = config[\"process\"]\n",
    "\n",
    "        self.config = Perceiver3DEncoderProcessConfig.model_validate(config | kwargs)\n",
    "\n",
    "        num_layers = self.config.num_layers\n",
    "\n",
    "        self.self_attention = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            relative_position_embeddings = None\n",
    "            if self.config.use_relative_position_embeddings:\n",
    "                relative_position_embeddings_config = RelativePositionEmbeddings3DConfig(\n",
    "                    num_heads=self.config.num_heads, grid_size=self.config.latent_grid_size\n",
    "                )\n",
    "                relative_position_embeddings = RelativePositionEmbeddings3D(relative_position_embeddings_config)\n",
    "\n",
    "            self.self_attention.append(\n",
    "                Attention3DWithMLP(\n",
    "                    self.config.model_dump(),\n",
    "                    relative_position_bias=relative_position_embeddings,\n",
    "                    checkpointing_level=checkpointing_level,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    def _forward(self, qkv, return_all: bool = False) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "        # qkv: (b, dim, zl, yl, xl)\n",
    "\n",
    "        embeddings = []\n",
    "        embedding = qkv\n",
    "        for self_attention_layer in self.self_attention:\n",
    "            embedding = self_attention_layer(embedding, embedding, embedding)\n",
    "            embeddings.append(embedding)\n",
    "        # (b, dim, zl, yl, xl)\n",
    "\n",
    "        return_value = embeddings[-1]\n",
    "        if return_all:\n",
    "            return_value = {\n",
    "                \"embeddings\": return_value,\n",
    "                \"all_embeddings\": embeddings,\n",
    "            }\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    def forward(self, q: torch.Tensor, return_all: bool = False):\n",
    "        return self.checkpointing_level4(self._forward, q, return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPerceiver3DEncoderProcess\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mself_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m2\u001b[0m x \u001b[1;35mAttention3DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mrelative_position_bias\u001b[1m)\u001b[0m: \u001b[1;35mRelativePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4096\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4096\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Perceiver3DEncoderProcess(dim=1024, num_heads=16, num_layers=2, latent_grid_size=(8, 8, 8))\n",
    "display(test)\n",
    "o = test(torch.randn(2, 1024, 8, 8, 8))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DEncoder(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Perceiver3DEncoderConfig = {},\n",
    "        channel_mapping: Perceiver3DChannelMapping | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Perceiver3DEncoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.encode = Perceiver3DEncoderEncode(self.config.encode, channel_mapping, checkpointing_level)\n",
    "        self.process = Perceiver3DEncoderProcess(self.config.process, checkpointing_level)\n",
    "\n",
    "        self.checkpointing_level5 = ActivationCheckpointing(5, checkpointing_level)\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        x,\n",
    "        sliding_window: int | None = None,\n",
    "        sliding_stride: int | None = None,\n",
    "        return_all: bool = False,\n",
    "    ) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "        # x: (b, in_channels, z, y, x)\n",
    "\n",
    "        return_value = {}\n",
    "\n",
    "        encode_embeddings = self.encode(x, sliding_window, sliding_stride, return_all=True)[\"all_embeddings\"]\n",
    "        return_value[\"encode_embeddings\"] = encode_embeddings\n",
    "        embeddings = encode_embeddings[-1]\n",
    "        # (b, dim, zl, yl, xl)\n",
    "\n",
    "        process_embeddings = self.process(embeddings, return_all=True)[\"all_embeddings\"]\n",
    "        return_value[\"process_embeddings\"] = process_embeddings\n",
    "        embeddings = process_embeddings[-1]\n",
    "        # (b, dim, zl, yl, xl)\n",
    "\n",
    "        return_value[\"embeddings\"] = embeddings\n",
    "\n",
    "        if not return_all:\n",
    "            return_value = embeddings\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        sliding_window: int | None = None,\n",
    "        sliding_stride: int | None = None,\n",
    "        return_all: bool = False,\n",
    "    ):\n",
    "        return self.checkpointing_level5(self._forward, x, sliding_window, sliding_stride, return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPerceiver3DEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mencode\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DEncoderEncode\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mchannel_mapping\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DChannelMapping\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmappers\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m256\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcross_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mprocess\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DEncoderProcess\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mself_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m x \u001b[1;35mAttention3DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mrelative_position_bias\u001b[1m)\u001b[0m: \u001b[1;35mRelativePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Perceiver3DEncoder(test_config, Perceiver3DChannelMapping(in_channels={512, 256}, out_channels=384))\n",
    "display(test)\n",
    "o = test(torch.randn(2, 256, 4, 4, 4))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DDecoder(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Perceiver3DDecoderConfig | Perceiver3DConfig = {},\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(config, Perceiver3DConfig):\n",
    "            config = config.decode\n",
    "        if \"decode\" in config:\n",
    "            config = config[\"decode\"]\n",
    "\n",
    "        self.config = Perceiver3DDecoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        num_layers = self.config.num_layers\n",
    "\n",
    "        self.empty_token = nn.Parameter(torch.randn(dim, 1) * 0.02, requires_grad=True)\n",
    "        # Initialized with gaussian for robust training stability\n",
    "\n",
    "        self.position_embeddings = None\n",
    "        if self.config.use_absolute_position_embeddings:\n",
    "            self.position_embeddings = AbsolutePositionEmbeddings3D()\n",
    "\n",
    "        self.cross_attention = nn.ModuleList(\n",
    "            [\n",
    "                Attention3DWithMLP(self.config.model_dump(), checkpointing_level=checkpointing_level)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.channel_mapping = Perceiver3DChannelMapping(in_channels=dim, out_channels=self.config.out_channels)\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        kv: torch.Tensor,\n",
    "        out_shape: tuple[int, int, int],\n",
    "        sliding_window: tuple[int, int, int] | None = None,\n",
    "        sliding_stride: tuple[int, int, int] | None = None,\n",
    "        crop_offsets: torch.Tensor = None,\n",
    "        return_all: bool = False,\n",
    "    ) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "        # kv: (b, dim, zl, yl, xl)\n",
    "\n",
    "        # Prepare queries\n",
    "        b = kv.shape[0]\n",
    "        z, y, x = out_shape\n",
    "        q = repeat(self.empty_token, \"d 1 -> b d z y x\", b=b, z=z, y=y, x=x)\n",
    "        # (b, dim, z, y, x)\n",
    "        if self.position_embeddings is not None:\n",
    "            q = self.position_embeddings(q, device=q.device, crop_offsets=crop_offsets)\n",
    "        # (b, dim, z, y, x)\n",
    "\n",
    "        # Perform attention\n",
    "        outputs = []\n",
    "        for cross_attention_layer in self.cross_attention:\n",
    "            q_windows, q_positions = unfold_with_roll_3d(q, sliding_window, sliding_stride)\n",
    "            # (num_windows, b, dim, *sliding_window)\n",
    "            new_q_windows = []\n",
    "            for q_window in q_windows:\n",
    "                output_window = cross_attention_layer(q_window, kv, kv)\n",
    "                new_q_windows.append(output_window)\n",
    "            new_q_windows = torch.stack(new_q_windows, dim=0)\n",
    "            # (num_windows, b, dim, *sliding_window)\n",
    "            q = fold_back_3d(new_q_windows, q_positions, q.shape[2:])\n",
    "            outputs.append(q)\n",
    "        # list of (b, dim, z, y, x)\n",
    "\n",
    "        output = outputs[-1]\n",
    "        # (b, dim, z, y, x)\n",
    "\n",
    "        output = self.channel_mapping(output)\n",
    "        # (b, out_channels, z, y, x)\n",
    "\n",
    "        return_value = output\n",
    "        if return_all:\n",
    "            return_value = {\n",
    "                \"output\": output,\n",
    "                \"all_outputs\": outputs,\n",
    "            }\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        kv: torch.Tensor,\n",
    "        out_shape: tuple[int, int, int],\n",
    "        sliding_window: tuple[int, int, int] | None = None,\n",
    "        sliding_stride: tuple[int, int, int] | None = None,\n",
    "        crop_offsets: torch.Tensor = None,\n",
    "        return_all: bool = False,\n",
    "    ):\n",
    "        return self.checkpointing_level4(\n",
    "            self._forward, kv, out_shape, sliding_window, sliding_stride, crop_offsets, return_all\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPerceiver3DDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcross_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m x \u001b[1;35mAttention3DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mchannel_mapping\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DChannelMapping\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mmappers\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Perceiver3DDecoder(test_config)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 384, 8, 8, 8), (20, 20, 20), sliding_window=(8, 8, 8), sliding_stride=(8, 8, 8))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "config = Perceiver3DConfig.model_validate(\n",
    "    {\n",
    "        \"dim\": 120,\n",
    "        \"latent_grid_size\": (10, 10, 10),\n",
    "        \"num_heads\": 10,\n",
    "        \"encode\": {\n",
    "            \"num_layers\": 5,\n",
    "        },\n",
    "        \"process\": {\n",
    "            \"num_layers\": 5,\n",
    "        },\n",
    "        \"decode\": {\n",
    "            \"out_channels\": 1,\n",
    "            \"num_layers\": 5,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "sample_input = torch.randn(5, 1, 10, 10, 10, device=device)\n",
    "channel_mapping = Perceiver3DChannelMapping(in_channels=1, out_channels=120)\n",
    "encoder = Perceiver3DEncoder(config, channel_mapping)\n",
    "decoder = Perceiver3DDecoder(config)\n",
    "encoder.to(device), decoder.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 4.105795383453369\n",
      "Epoch: 1\tLoss: 4.533168792724609\n",
      "Epoch: 2\tLoss: 3.2169480323791504\n",
      "Epoch: 3\tLoss: 0.9737862944602966\n",
      "Epoch: 4\tLoss: 1.81355619430542\n",
      "Epoch: 5\tLoss: 2.1641390323638916\n",
      "Epoch: 6\tLoss: 1.5708163976669312\n",
      "Epoch: 7\tLoss: 0.8763588666915894\n",
      "Epoch: 8\tLoss: 1.0174925327301025\n",
      "Epoch: 9\tLoss: 1.4019241333007812\n",
      "Epoch: 10\tLoss: 1.395279884338379\n",
      "Epoch: 11\tLoss: 1.0960237979888916\n",
      "Epoch: 12\tLoss: 0.8347333669662476\n",
      "Epoch: 13\tLoss: 0.8828728199005127\n",
      "Epoch: 14\tLoss: 1.0718517303466797\n",
      "Epoch: 15\tLoss: 1.1417597532272339\n",
      "Epoch: 16\tLoss: 1.0444450378417969\n",
      "Epoch: 17\tLoss: 0.8860328197479248\n",
      "Epoch: 18\tLoss: 0.8144442439079285\n",
      "Epoch: 19\tLoss: 0.8773074746131897\n",
      "Epoch: 20\tLoss: 0.967329204082489\n",
      "Epoch: 21\tLoss: 0.9819587469100952\n",
      "Epoch: 22\tLoss: 0.9154919981956482\n",
      "Epoch: 23\tLoss: 0.8364591598510742\n",
      "Epoch: 24\tLoss: 0.8156362175941467\n",
      "Epoch: 25\tLoss: 0.8583726286888123\n",
      "Epoch: 26\tLoss: 0.9025368690490723\n",
      "Epoch: 27\tLoss: 0.900412380695343\n",
      "Epoch: 28\tLoss: 0.8579831719398499\n",
      "Epoch: 29\tLoss: 0.8186146020889282\n",
      "Epoch: 30\tLoss: 0.8182404637336731\n",
      "Epoch: 31\tLoss: 0.8448351621627808\n",
      "Epoch: 32\tLoss: 0.864382803440094\n",
      "Epoch: 33\tLoss: 0.8562692999839783\n",
      "Epoch: 34\tLoss: 0.8307905197143555\n",
      "Epoch: 35\tLoss: 0.8133692145347595\n",
      "Epoch: 36\tLoss: 0.817943811416626\n",
      "Epoch: 37\tLoss: 0.8345591425895691\n",
      "Epoch: 38\tLoss: 0.8411335945129395\n",
      "Epoch: 39\tLoss: 0.8306208848953247\n",
      "Epoch: 40\tLoss: 0.8158861994743347\n",
      "Epoch: 41\tLoss: 0.8121271133422852\n",
      "Epoch: 42\tLoss: 0.8199731111526489\n",
      "Epoch: 43\tLoss: 0.8272162675857544\n",
      "Epoch: 44\tLoss: 0.8262650370597839\n",
      "Epoch: 45\tLoss: 0.81861811876297\n",
      "Epoch: 46\tLoss: 0.811813235282898\n",
      "Epoch: 47\tLoss: 0.8128425478935242\n",
      "Epoch: 48\tLoss: 0.8188040852546692\n",
      "Epoch: 49\tLoss: 0.8207616806030273\n",
      "Epoch: 50\tLoss: 0.8162829279899597\n",
      "Epoch: 51\tLoss: 0.811458170413971\n",
      "Epoch: 52\tLoss: 0.8117170333862305\n",
      "Epoch: 53\tLoss: 0.8151312470436096\n",
      "Epoch: 54\tLoss: 0.8166494965553284\n",
      "Epoch: 55\tLoss: 0.8144382834434509\n",
      "Epoch: 56\tLoss: 0.8113448023796082\n",
      "Epoch: 57\tLoss: 0.8110259771347046\n",
      "Epoch: 58\tLoss: 0.8130675554275513\n",
      "Epoch: 59\tLoss: 0.8143467307090759\n",
      "Epoch: 60\tLoss: 0.8127744197845459\n",
      "Epoch: 61\tLoss: 0.8109182715415955\n",
      "Epoch: 62\tLoss: 0.8108034729957581\n",
      "Epoch: 63\tLoss: 0.8120890855789185\n",
      "Epoch: 64\tLoss: 0.8126931190490723\n",
      "Epoch: 65\tLoss: 0.8118413686752319\n",
      "Epoch: 66\tLoss: 0.8106530904769897\n",
      "Epoch: 67\tLoss: 0.8106622099876404\n",
      "Epoch: 68\tLoss: 0.8114287257194519\n",
      "Epoch: 69\tLoss: 0.8117535710334778\n",
      "Epoch: 70\tLoss: 0.8111963272094727\n",
      "Epoch: 71\tLoss: 0.8104968070983887\n",
      "Epoch: 72\tLoss: 0.8105242252349854\n",
      "Epoch: 73\tLoss: 0.8111021518707275\n",
      "Epoch: 74\tLoss: 0.8112185597419739\n",
      "Epoch: 75\tLoss: 0.8106867074966431\n",
      "Epoch: 76\tLoss: 0.8103280663490295\n",
      "Epoch: 77\tLoss: 0.8105188012123108\n",
      "Epoch: 78\tLoss: 0.8108370900154114\n",
      "Epoch: 79\tLoss: 0.8107454776763916\n",
      "Epoch: 80\tLoss: 0.8103786706924438\n",
      "Epoch: 81\tLoss: 0.8102630376815796\n",
      "Epoch: 82\tLoss: 0.8104410171508789\n",
      "Epoch: 83\tLoss: 0.8105806708335876\n",
      "Epoch: 84\tLoss: 0.8104081153869629\n",
      "Epoch: 85\tLoss: 0.8102192878723145\n",
      "Epoch: 86\tLoss: 0.8102363348007202\n",
      "Epoch: 87\tLoss: 0.8103788495063782\n",
      "Epoch: 88\tLoss: 0.810364842414856\n",
      "Epoch: 89\tLoss: 0.8102074861526489\n",
      "Epoch: 90\tLoss: 0.8101324439048767\n",
      "Epoch: 91\tLoss: 0.8102140426635742\n",
      "Epoch: 92\tLoss: 0.810257613658905\n",
      "Epoch: 93\tLoss: 0.810189425945282\n",
      "Epoch: 94\tLoss: 0.8100905418395996\n",
      "Epoch: 95\tLoss: 0.8101122975349426\n",
      "Epoch: 96\tLoss: 0.8101722598075867\n",
      "Epoch: 97\tLoss: 0.8101457953453064\n",
      "Epoch: 98\tLoss: 0.8100671172142029\n",
      "Epoch: 99\tLoss: 0.8100478053092957\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    output = decoder(encoder(sample_input), out_shape=sample_input.shape[2:])\n",
    "    loss = torch.nn.functional.l1_loss(output, sample_input)\n",
    "    print(f\"Epoch: {epoch}\\tLoss: {loss.item()}\")\n",
    "    optimizer.zero_grad(True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
