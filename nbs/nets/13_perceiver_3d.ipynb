{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/perceiver_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.layers.attention import Attention1DWithMLP, Attention1DWithMLPConfig\n",
    "from vision_architectures.layers.embeddings import (\n",
    "    AbsolutePositionEmbeddings1D,\n",
    "    AbsolutePositionEmbeddings1DConfig,\n",
    "    AbsolutePositionEmbeddings3D,\n",
    ")\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, model_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DChannelMappingConfig(CustomBaseModel):\n",
    "    in_channels: int | set[int]\n",
    "    out_channels: int\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderEncodeConfig(Attention1DWithMLPConfig):\n",
    "    dim: int\n",
    "    num_latent_tokens: int\n",
    "    num_layers: int\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderProcessConfig(Attention1DWithMLPConfig):\n",
    "    dim: int\n",
    "    num_layers: int\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderConfig(CustomBaseModel):\n",
    "    encode: Perceiver3DEncoderEncodeConfig\n",
    "    process: Perceiver3DEncoderProcessConfig\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.encode.dim\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.encode.dim == self.process.dim, \"encode and process dims must be equal\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class Perceiver3DDecoderConfig(Attention1DWithMLPConfig):\n",
    "    dim: int\n",
    "    num_layers: int\n",
    "    out_channels: int\n",
    "\n",
    "\n",
    "class Perceiver3DConfig(Perceiver3DEncoderConfig):\n",
    "    decode: Perceiver3DDecoderConfig\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.encode.dim == self.decode.dim, \"encode and decode dims must be equal\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m384\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_config = Perceiver3DConfig.model_validate(\n",
    "    {\n",
    "        \"encode\": {\n",
    "            \"dim\": 384,\n",
    "            \"num_latent_tokens\": 1024,\n",
    "            \"num_layers\": 1,\n",
    "            \"num_heads\": 16,\n",
    "        },\n",
    "        \"process\": {\n",
    "            \"dim\": 384,\n",
    "            \"num_layers\": 3,\n",
    "            \"num_heads\": 16,\n",
    "        },\n",
    "        \"decode\": {\n",
    "            \"dim\": 384,\n",
    "            \"out_channels\": 1,\n",
    "            \"num_layers\": 3,\n",
    "            \"num_heads\": 16,\n",
    "        },\n",
    "    }\n",
    ")\n",
    "test_config.dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def unfold_with_rollover_1d(x: torch.Tensor, window_size: int | None, stride: int | None):\n",
    "    # x: (b, T, dim)\n",
    "    if window_size is None or stride is None:\n",
    "        return x.unsqueeze(0)\n",
    "    total_len = x.shape[1]\n",
    "    num_windows = (total_len + stride - 1) // stride  # Number of windows needed\n",
    "    pad_len = max(0, window_size + (num_windows - 1) * stride - total_len)  # How many elements are missing\n",
    "    if pad_len > 0:\n",
    "        x = torch.cat([x, x[:, :pad_len]], dim=1)  # Rollover padding\n",
    "    x = x.unfold(1, window_size, stride)\n",
    "    x = rearrange(x, \"b num_windows dim window_size -> num_windows b window_size dim\")\n",
    "    # (num_windows, b, window_size, dim)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "          \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "          \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m,\n",
       "          \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m,\n",
       "          \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4).reshape(1, 4, 1)\n",
    "unfolded = unfold_with_rollover_1d(x, 3, 2)\n",
    "unfolded.shape, unfolded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def unfold_with_rollover_3d_with_mask(\n",
    "    x: torch.Tensor, window_size: tuple[int, int, int] | None, stride: tuple[int, int, int] | None\n",
    "):\n",
    "    if window_size is None or stride is None:\n",
    "        return x.unsqueeze(0)\n",
    "\n",
    "    b, c, d, h, w = x.shape\n",
    "    w_d, w_h, w_w = window_size\n",
    "    s_d, s_h, s_w = stride\n",
    "\n",
    "    # Calculate number of windows and required padding\n",
    "    n_d = (d + s_d - 1) // s_d\n",
    "    n_h = (h + s_h - 1) // s_h\n",
    "    n_w = (w + s_w - 1) // s_w\n",
    "\n",
    "    # Calculate padding with rollover\n",
    "    pad_d = max(0, w_d + (n_d - 1) * s_d - d)\n",
    "    pad_h = max(0, w_h + (n_h - 1) * s_h - h)\n",
    "    pad_w = max(0, w_w + (n_w - 1) * s_w - w)\n",
    "\n",
    "    # Apply rollover padding efficiently\n",
    "    if pad_d > 0:\n",
    "        x = torch.cat([x, x[:, :, :pad_d]], dim=2)\n",
    "    if pad_h > 0:\n",
    "        x = torch.cat([x, x[:, :, :, :pad_h]], dim=3)\n",
    "    if pad_w > 0:\n",
    "        x = torch.cat([x, x[:, :, :, :, :pad_w]], dim=4)\n",
    "\n",
    "    # Extract patches using stride tricks\n",
    "    # This approach avoids creating intermediate tensors\n",
    "    patches = x.unfold(2, w_d, s_d).unfold(3, w_h, s_h).unfold(4, w_w, s_w)\n",
    "\n",
    "    # Rearrange dimensions using einops for clarity and efficiency\n",
    "    patches = rearrange(\n",
    "        patches, \"b c d_windows h_windows w_windows w_d w_h w_w -> (d_windows h_windows w_windows) b c w_d w_h w_w\"\n",
    "    )\n",
    "\n",
    "    # Padded shape\n",
    "    padded_d, padded_h, padded_w = x.shape[2], x.shape[3], x.shape[4]\n",
    "\n",
    "    # Generate positional information for each fold\n",
    "    # Create meshgrid for all window positions\n",
    "    d_indices = torch.arange(n_d, device=x.device)\n",
    "    h_indices = torch.arange(n_h, device=x.device)\n",
    "    w_indices = torch.arange(n_w, device=x.device)\n",
    "\n",
    "    # Calculate starting indices for each window\n",
    "    d_starts = d_indices * s_d\n",
    "    h_starts = h_indices * s_h\n",
    "    w_starts = w_indices * s_w\n",
    "\n",
    "    # Generate all possible window starting positions\n",
    "    d_pos, h_pos, w_pos = torch.meshgrid(d_starts, h_starts, w_starts, indexing=\"ij\")\n",
    "    positions = torch.stack([d_pos, h_pos, w_pos], dim=-1).reshape(-1, 3)\n",
    "\n",
    "    # Generate usage count mask\n",
    "    usage_mask = torch.zeros(padded_d, padded_h, padded_w, device=x.device)\n",
    "\n",
    "    # For each position, increment the count for all pixels in that window\n",
    "    for pos in positions:\n",
    "        pos_d, pos_h, pos_w = pos\n",
    "        # Handle potential out-of-bounds for the last window\n",
    "        end_d = min(pos_d + w_d, padded_d)\n",
    "        end_h = min(pos_h + w_h, padded_h)\n",
    "        end_w = min(pos_w + w_w, padded_w)\n",
    "\n",
    "        usage_mask[pos_d:end_d, pos_h:end_h, pos_w:end_w] += 1\n",
    "\n",
    "    # Crop usage_mask back to original dimensions (without padding)\n",
    "    usage_mask = usage_mask[:d, :h, :w]\n",
    "\n",
    "    # Repeat usage mask for batch and channel dimensions\n",
    "    usage_mask = usage_mask.expand(b, c, d, h, w)\n",
    "\n",
    "    return patches, positions, usage_mask\n",
    "\n",
    "\n",
    "def fold_back_3d(\n",
    "    patches: torch.Tensor,\n",
    "    positions: torch.Tensor,\n",
    "    usage_mask: torch.Tensor,\n",
    "    output_shape: tuple,\n",
    "    window_size: tuple[int, int, int],\n",
    "    aggregation_mode: str = \"mean\",\n",
    "):\n",
    "    b, c, d, h, w = output_shape\n",
    "    w_d, w_h, w_w = window_size\n",
    "\n",
    "    # Initialize output tensor\n",
    "    output = torch.zeros(output_shape, dtype=patches.dtype, device=patches.device)\n",
    "\n",
    "    # Handle case where we want sum instead of mean\n",
    "    if aggregation_mode == \"sum\":\n",
    "        usage_mask = torch.ones_like(usage_mask)\n",
    "\n",
    "    # Fold each patch back to its position\n",
    "    for i, pos in enumerate(positions):\n",
    "        pos_d, pos_h, pos_w = pos\n",
    "        # Get the patch\n",
    "        patch = patches[i]\n",
    "\n",
    "        # Handle window dimensions (might be smaller at edges)\n",
    "        valid_w_d = min(w_d, d - pos_d) if pos_d < d else 0\n",
    "        valid_w_h = min(w_h, h - pos_h) if pos_h < h else 0\n",
    "        valid_w_w = min(w_w, w - pos_w) if pos_w < w else 0\n",
    "\n",
    "        if valid_w_d <= 0 or valid_w_h <= 0 or valid_w_w <= 0:\n",
    "            continue\n",
    "\n",
    "        # Only add the valid portion of the patch\n",
    "        output[:, :, pos_d : pos_d + valid_w_d, pos_h : pos_h + valid_w_h, pos_w : pos_w + valid_w_w] += patch[\n",
    "            :, :, :valid_w_d, :valid_w_h, :valid_w_w\n",
    "        ]\n",
    "\n",
    "    # Average according to usage count\n",
    "    if aggregation_mode == \"mean\":\n",
    "        # Avoid division by zero\n",
    "        mask = (usage_mask > 0).float()\n",
    "        output = output / (usage_mask + (1 - mask))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DChannelMapping(nn.Module):\n",
    "    def __init__(self, config: Perceiver3DChannelMappingConfig = {}, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Perceiver3DChannelMappingConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.in_channels = self.config.in_channels\n",
    "        self.out_channels = self.config.out_channels\n",
    "\n",
    "        if isinstance(self.in_channels, int):\n",
    "            self.in_channels = {self.in_channels}\n",
    "\n",
    "        self.mappers = nn.ModuleDict()\n",
    "        for in_channels in self.in_channels:\n",
    "            self.mappers[str(in_channels)] = nn.Conv3d(in_channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (b, in_channels, z, y, x)\n",
    "\n",
    "        in_channels = x.shape[1]\n",
    "        if in_channels not in self.in_channels:\n",
    "            raise ValueError(f\"Input channels {in_channels} not in {self.in_channels}\")\n",
    "\n",
    "        mapper = self.mappers[str(in_channels)]\n",
    "        x = mapper(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 4, 4, 4])\n",
      "torch.Size([1, 24, 4, 4, 4])\n",
      "torch.Size([1, 24, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "test = Perceiver3DChannelMapping(in_channels={12, 24, 48}, out_channels=24)\n",
    "for in_channels in [12, 24, 48]:\n",
    "    x = torch.randn(1, in_channels, 4, 4, 4)\n",
    "    print(test(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderEncode(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Perceiver3DEncoderEncodeConfig | Perceiver3DEncoderConfig = {},\n",
    "        channel_mapping: Perceiver3DChannelMapping | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(config, Perceiver3DEncoderConfig):\n",
    "            config = config.encode\n",
    "        if \"encode\" in config:\n",
    "            config = config[\"encode\"]\n",
    "\n",
    "        self.config = Perceiver3DEncoderEncodeConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        num_latent_tokens = self.config.num_latent_tokens\n",
    "        num_layers = self.config.num_layers\n",
    "\n",
    "        self.latent_tokens = nn.Parameter(torch.empty(num_latent_tokens, dim), requires_grad=True)\n",
    "        nn.init.xavier_uniform_(self.latent_tokens)\n",
    "\n",
    "        self.position_embeddings_config = AbsolutePositionEmbeddings1DConfig(\n",
    "            dim=dim, length=num_latent_tokens, learnable=False\n",
    "        )\n",
    "        self.position_embeddings = AbsolutePositionEmbeddings1D(self.position_embeddings_config)\n",
    "\n",
    "        self.channel_mapping = channel_mapping\n",
    "\n",
    "        self.cross_attention = nn.ModuleList(\n",
    "            [\n",
    "                Attention1DWithMLP(self.config.model_dump(), checkpointing_level=checkpointing_level)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        x: torch.Tensor | list[torch.Tensor],\n",
    "        sliding_window: int | None = None,\n",
    "        sliding_stride: int | None = None,\n",
    "        return_all: bool = False,\n",
    "    ) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "        # x: [(b, in_channels, z, y, x), ...]\n",
    "\n",
    "        # Prepare keys and values\n",
    "        def prepare_keys_values(x: torch.Tensor | list[torch.Tensor]):\n",
    "            if not isinstance(x, list):\n",
    "                x = [x]\n",
    "            # x is now a list of tensors\n",
    "            if self.channel_mapping is None:\n",
    "                kv = x\n",
    "            else:\n",
    "                kv = []\n",
    "                for i in range(len(x)):\n",
    "                    mapped = self.channel_mapping(x[i])  # modifying in-place leads to errors when checkpointing\n",
    "                    mapped = rearrange(mapped, \"b d z y x -> b (z y x) d\")\n",
    "                    kv.append(mapped)\n",
    "            kv = torch.cat(kv, dim=1)\n",
    "            return kv\n",
    "\n",
    "        kv = self.checkpointing_level1(prepare_keys_values, x)\n",
    "        # (b, num_kv_tokens, dim)\n",
    "\n",
    "        # Prepare queries\n",
    "        b = kv.shape[0]\n",
    "        q = repeat(self.latent_tokens, \"t d -> b t d\", b=b)\n",
    "        q = q + self.position_embeddings(batch_size=b)\n",
    "        # (b, num_latent_tokens, dim)\n",
    "\n",
    "        # Prepare sliding window\n",
    "        kv_windows = unfold_with_rollover_1d(kv, sliding_window, sliding_stride)\n",
    "        # (num_windows, b, window_size, dim)\n",
    "\n",
    "        # Perform attention\n",
    "        embeddings = []\n",
    "        for cross_attention_layer in self.cross_attention:\n",
    "            embedding = torch.zeros_like(q)\n",
    "            for kv_window in kv_windows:\n",
    "                embedding_window = cross_attention_layer(q, kv_window, kv_window)\n",
    "                embedding = embedding + embedding_window\n",
    "            q = embedding  # To pass to the next layer\n",
    "            embeddings.append(embedding)\n",
    "        # (b, num_latent_tokens, dim)\n",
    "\n",
    "        return_value = embeddings[-1]\n",
    "        if return_all:\n",
    "            return_value = {\n",
    "                \"embeddings\": return_value,\n",
    "                \"all_embeddings\": embeddings,\n",
    "            }\n",
    "        return return_value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor | list[torch.Tensor],\n",
    "        sliding_window: int | None = None,  # Sliding window may be beneficial during inference time\n",
    "        sliding_stride: int | None = None,\n",
    "        return_all: bool = False,\n",
    "    ):\n",
    "        return self.checkpointing_level4(self._forward, x, sliding_window, sliding_stride, return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPerceiver3DEncoderEncode\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mchannel_mapping\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DChannelMapping\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mmappers\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcross_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m384\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 963 ms, total: 2.03 s\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test = Perceiver3DEncoderEncode(test_config, Perceiver3DChannelMapping(in_channels=512, out_channels=384))\n",
    "display(test)\n",
    "o = test(torch.randn(2, 512, 4, 4, 4))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m384\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.69 s, sys: 268 ms, total: 3.96 s\n",
      "Wall time: 64.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test.eval()\n",
    "with torch.no_grad():\n",
    "    o = test(torch.randn(2, 512, 4, 4, 4), sliding_window=8, sliding_stride=8)\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DEncoderProcess(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Perceiver3DEncoderProcessConfig | Perceiver3DEncoderConfig = {},\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(config, Perceiver3DEncoderConfig):\n",
    "            config = config.process\n",
    "        if \"process\" in config:\n",
    "            config = config[\"process\"]\n",
    "\n",
    "        self.config = Perceiver3DEncoderProcessConfig.model_validate(config | kwargs)\n",
    "\n",
    "        num_layers = self.config.num_layers\n",
    "\n",
    "        self.self_attention = nn.ModuleList(\n",
    "            [\n",
    "                Attention1DWithMLP(self.config.model_dump(), checkpointing_level=checkpointing_level)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    def _forward(self, qkv, return_all: bool = False) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "        # qkv: (b, num_tokens, dim)\n",
    "\n",
    "        embeddings = []\n",
    "        embedding = qkv\n",
    "        for self_attention_layer in self.self_attention:\n",
    "            embedding = self_attention_layer(embedding, embedding, embedding)\n",
    "            embeddings.append(embedding)\n",
    "        # (b, num_tokens, dim)\n",
    "\n",
    "        return_value = embeddings[-1]\n",
    "        if return_all:\n",
    "            return_value = {\n",
    "                \"embeddings\": return_value,\n",
    "                \"all_embeddings\": embeddings,\n",
    "            }\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    def forward(self, q: torch.Tensor, return_all: bool = False):\n",
    "        return self.checkpointing_level4(self._forward, q, return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPerceiver3DEncoderProcess\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mself_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m2\u001b[0m x \u001b[1;35mAttention1DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4096\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4096\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m1024\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Perceiver3DEncoderProcess(dim=1024, num_heads=16, num_layers=2)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 512, 1024))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DEncoder(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Perceiver3DEncoderConfig = {},\n",
    "        channel_mapping: Perceiver3DChannelMapping | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Perceiver3DEncoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.encode = Perceiver3DEncoderEncode(config.encode, channel_mapping, checkpointing_level)\n",
    "        self.process = Perceiver3DEncoderProcess(config.process, checkpointing_level)\n",
    "\n",
    "        self.checkpointing_level5 = ActivationCheckpointing(5, checkpointing_level)\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        x,\n",
    "        sliding_window: int | None = None,\n",
    "        sliding_stride: int | None = None,\n",
    "        return_all: bool = False,\n",
    "    ) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "        # x: (b, in_channels, z, y, x)\n",
    "\n",
    "        return_value = {}\n",
    "\n",
    "        encode_embeddings = self.encode(x, sliding_window, sliding_stride, return_all=True)[\"all_embeddings\"]\n",
    "        return_value[\"encode_embeddings\"] = encode_embeddings\n",
    "        embeddings = encode_embeddings[-1]\n",
    "        # (b, num_tokens, dim)\n",
    "\n",
    "        process_embeddings = self.process(embeddings, return_all=True)[\"all_embeddings\"]\n",
    "        return_value[\"process_embeddings\"] = process_embeddings\n",
    "        embeddings = process_embeddings[-1]\n",
    "\n",
    "        return_value[\"embeddings\"] = embeddings\n",
    "\n",
    "        if not return_all:\n",
    "            return_value = embeddings\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        sliding_window: int | None = None,\n",
    "        sliding_stride: int | None = None,\n",
    "        return_all: bool = False,\n",
    "    ):\n",
    "        return self.checkpointing_level5(self._forward, x, sliding_window, sliding_stride, return_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPerceiver3DEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mencode\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DEncoderEncode\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mchannel_mapping\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DChannelMapping\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmappers\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m512\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m256\u001b[0m, \u001b[1;36m384\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcross_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mprocess\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DEncoderProcess\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mself_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m x \u001b[1;35mAttention1DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m384\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Perceiver3DEncoder(test_config, Perceiver3DChannelMapping(in_channels={512, 256}, out_channels=384))\n",
    "display(test)\n",
    "o = test(torch.randn(2, 256, 4, 4, 4))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Perceiver3DDecoder(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Perceiver3DDecoderConfig | Perceiver3DConfig = {},\n",
    "        position_embeddings: AbsolutePositionEmbeddings3D = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(config, Perceiver3DConfig):\n",
    "            config = config.decode\n",
    "        if \"decode\" in config:\n",
    "            config = config[\"decode\"]\n",
    "\n",
    "        self.config = Perceiver3DDecoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        num_layers = self.config.num_layers\n",
    "\n",
    "        self.empty_token = nn.Parameter(torch.randn(dim, 1) * 0.02, requires_grad=True)\n",
    "        # Initialized with gaussian for robust training stability\n",
    "\n",
    "        self.position_embeddings = position_embeddings\n",
    "\n",
    "        self.cross_attention = nn.ModuleList(\n",
    "            [\n",
    "                Attention1DWithMLP(config.model_dump(), checkpointing_level=checkpointing_level)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.channel_mapping = Perceiver3DChannelMapping(in_channels=dim, out_channels=self.config.out_channels)\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        kv,\n",
    "        out_shape: tuple[int, int, int],\n",
    "        sliding_window: tuple[int, int, int] | None = None,\n",
    "        sliding_stride: tuple[int, int, int] | None = None,\n",
    "        crop_offsets: torch.Tensor = None,\n",
    "        return_all: bool = False,\n",
    "    ) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "        # kv: (b, num_tokens, dim)\n",
    "\n",
    "        b = kv.shape[0]\n",
    "\n",
    "        q = repeat(\n",
    "            self.empty_token,\n",
    "            \"d 1 -> b d z y x\",\n",
    "            b=b,\n",
    "            z=out_shape[0],\n",
    "            y=out_shape[1],\n",
    "            x=out_shape[2],\n",
    "        )\n",
    "        # (b, dim, z, y, x)\n",
    "\n",
    "        if self.position_embeddings is not None:\n",
    "            q = q + self.position_embeddings(\n",
    "                batch_size=b, dim=q.shape[1], grid_size=out_shape, device=q.device, crop_offsets=crop_offsets\n",
    "            )\n",
    "\n",
    "        # Prepare sliding windows  # TODO\n",
    "        # q_windows, _, _ = unfold_with_rollover_3d_with_mask(q, sliding_window, sliding_stride)\n",
    "        # (num_windows, b, dim, window_size_z, window_size_y, window_size_x)\n",
    "\n",
    "        # Perform attention\n",
    "        # outputs = []\n",
    "        # for cross_attention_layer in self.cross_attention:\n",
    "        #     output = torch.zeros_like(q)\n",
    "        #     for q_window in q_windows:\n",
    "        #         output_window = cross_attention_layer(q_window, kv, kv)\n",
    "        #         ...\n",
    "        q = rearrange(q, \"b d z y x -> b (z y x) d\")\n",
    "        # (b, num_output_tokens, dim)\n",
    "        outputs = [q]\n",
    "        for cross_attention_layer in self.cross_attention:\n",
    "            q = outputs[-1]\n",
    "            outputs.append(cross_attention_layer(q, kv, kv))\n",
    "        # (b, num_output_tokens, dim)\n",
    "\n",
    "        output = outputs[-1]\n",
    "        output = rearrange(\n",
    "            output,\n",
    "            \"b (z y x) d -> b d z y x\",\n",
    "            z=out_shape[0],\n",
    "            y=out_shape[1],\n",
    "            x=out_shape[2],\n",
    "        )\n",
    "        # (b, dim, z, y, x)\n",
    "\n",
    "        output = self.channel_mapping(output)\n",
    "        # (b, out_channels, z, y, x)\n",
    "\n",
    "        return_value = output\n",
    "        if return_all:\n",
    "            return_value = {\n",
    "                \"output\": output,\n",
    "                \"all_outputs\": outputs,\n",
    "            }\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    # def _forward(\n",
    "    #     self,\n",
    "    #     kv,\n",
    "    #     out_shape: tuple[int, int, int],\n",
    "    #     sliding_window: tuple[int, int, int] | None = None,\n",
    "    #     sliding_stride: tuple[int, int, int] | None = None,\n",
    "    #     crop_offset: torch.Tensor = None,\n",
    "    #     return_all: bool = False,\n",
    "    # ) -> torch.Tensor | dict[str, torch.Tensor]:\n",
    "    #     # kv: (b, num_tokens, dim)\n",
    "\n",
    "    #     b = kv.shape[0]\n",
    "\n",
    "    #     q = repeat(\n",
    "    #         self.empty_token,\n",
    "    #         \"d 1 -> b d z y x\",\n",
    "    #         b=b,\n",
    "    #         z=out_shape[0],\n",
    "    #         y=out_shape[1],\n",
    "    #         x=out_shape[2],\n",
    "    #     )\n",
    "    #     # (b, dim, z, y, x)\n",
    "\n",
    "    #     if self.position_embeddings is not None:\n",
    "    #         q = q + self.position_embeddings(\n",
    "    #             batch_size=b, grid_size=out_shape, device=q.device, crop_offset=crop_offset\n",
    "    #         )\n",
    "\n",
    "    #     # Conditional execution based on sliding window parameters\n",
    "    #     if sliding_window is not None and sliding_stride is not None:\n",
    "    #         # Apply sliding window processing\n",
    "    #         q_windows, positions, usage_mask = unfold_with_rollover_3d_with_mask(q, sliding_window, sliding_stride)\n",
    "    #         # q_windows: (num_windows, b, dim, window_size_z, window_size_y, window_size_x)\n",
    "\n",
    "    #         # Process each window with attention\n",
    "    #         num_windows = q_windows.shape[0]\n",
    "    #         window_shape = q_windows.shape[-3:]  # (window_size_z, window_size_y, window_size_x)\n",
    "\n",
    "    #         # Initialize output tensor for each layer's output\n",
    "    #         outputs = []\n",
    "    #         current_windows = q_windows\n",
    "\n",
    "    #         # Apply cross-attention to each window sequentially through all layers\n",
    "    #         for layer_idx, cross_attention_layer in enumerate(self.cross_attention):\n",
    "    #             processed_windows = torch.zeros_like(current_windows)\n",
    "\n",
    "    #             for window_idx in range(num_windows):\n",
    "    #                 # Extract current window\n",
    "    #                 window = current_windows[window_idx]  # (b, dim, w_z, w_y, w_x)\n",
    "\n",
    "    #                 # Reshape for attention operation\n",
    "    #                 flat_window = rearrange(window, \"b d z y x -> b (z y x) d\")\n",
    "\n",
    "    #                 # Apply cross-attention\n",
    "    #                 attended_window = cross_attention_layer(flat_window, kv, kv)\n",
    "\n",
    "    #                 # Reshape back to 3D\n",
    "    #                 processed_window = rearrange(\n",
    "    #                     attended_window,\n",
    "    #                     \"b (z y x) d -> b d z y x\",\n",
    "    #                     z=window_shape[0],\n",
    "    #                     y=window_shape[1],\n",
    "    #                     x=window_shape[2],\n",
    "    #                 )\n",
    "\n",
    "    #                 # Store processed window\n",
    "    #                 processed_windows[window_idx] = processed_window\n",
    "\n",
    "    #             # Update current windows for next layer\n",
    "    #             current_windows = processed_windows\n",
    "\n",
    "    #             # Fold windows back to full volume for this layer's output\n",
    "    #             folded_output = fold_back_3d(\n",
    "    #                 processed_windows, positions, usage_mask, (b, q.shape[1], out_shape[0], out_shape[1], out_shape[2])\n",
    "    #             )\n",
    "\n",
    "    #             # Store layer output for return_all\n",
    "    #             flat_output = rearrange(\n",
    "    #                 folded_output, \"b d z y x -> b (z y x) d\", z=out_shape[0], y=out_shape[1], x=out_shape[2]\n",
    "    #             )\n",
    "    #             outputs.append(flat_output)\n",
    "\n",
    "    #         # Final output is from the last layer\n",
    "    #         output = folded_output\n",
    "\n",
    "    #     else:\n",
    "    #         # Original processing without sliding windows\n",
    "    #         q = rearrange(q, \"b d z y x -> b (z y x) d\")\n",
    "    #         # (b, num_output_tokens, dim)\n",
    "    #         outputs = [q]\n",
    "    #         for cross_attention_layer in self.cross_attention:\n",
    "    #             q = outputs[-1]\n",
    "    #             outputs.append(cross_attention_layer(q, kv, kv))\n",
    "    #         # (b, num_output_tokens, dim)\n",
    "\n",
    "    #         output = outputs[-1]\n",
    "    #         output = rearrange(\n",
    "    #             output,\n",
    "    #             \"b (z y x) d -> b d z y x\",\n",
    "    #             z=out_shape[0],\n",
    "    #             y=out_shape[1],\n",
    "    #             x=out_shape[2],\n",
    "    #         )\n",
    "    #         # (b, dim, z, y, x)\n",
    "\n",
    "    #     # Apply channel mapping to get final output\n",
    "    #     output = self.channel_mapping(output)\n",
    "    #     # (b, out_channels, z, y, x)\n",
    "\n",
    "    #     return_value = output\n",
    "    #     if return_all:\n",
    "    #         return_value = {\n",
    "    #             \"output\": output,\n",
    "    #             \"all_outputs\": outputs,\n",
    "    #         }\n",
    "\n",
    "    #     return return_value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        kv: torch.Tensor,\n",
    "        out_shape: tuple[int, int, int],\n",
    "        sliding_window: tuple[int, int, int] | None = None,\n",
    "        sliding_stride: tuple[int, int, int] | None = None,\n",
    "        crop_offsets: torch.Tensor = None,\n",
    "        return_all: bool = False,\n",
    "    ):\n",
    "        return self.checkpointing_level4(\n",
    "            self._forward, kv, out_shape, sliding_window, sliding_stride, crop_offsets, return_all\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPerceiver3DDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcross_attention\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m x \u001b[1;35mAttention1DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mchannel_mapping\u001b[1m)\u001b[0m: \u001b[1;35mPerceiver3DChannelMapping\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mmappers\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Perceiver3DDecoder(test_config, AbsolutePositionEmbeddings3D(dim=test_config.dim))\n",
    "display(test)\n",
    "o = test(torch.randn(2, 1024, 384), (20, 20, 20))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
