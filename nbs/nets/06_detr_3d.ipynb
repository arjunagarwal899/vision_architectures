{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/detr_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from vision_architectures.blocks.transformer import Attention1DWithMLPConfig, TransformerDecoderBlock1D\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.layers.embeddings import AbsolutePositionEmbeddings3D, AbsolutePositionEmbeddings3DConfig\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field\n",
    "from vision_architectures.utils.rearrange import rearrange_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRDecoderConfig(Attention1DWithMLPConfig):\n",
    "    num_layers: int = Field(..., description=\"Number of transformer decoder layers.\")\n",
    "\n",
    "\n",
    "class DETRBBoxMLPConfig(CustomBaseModel):\n",
    "    dim: int = Field(..., description=\"Dimension of the input features.\")\n",
    "    num_classes: int = Field(..., description=\"Number of classes for the bounding box predictions.\")\n",
    "\n",
    "\n",
    "class DETR3DConfig(DETRDecoderConfig, DETRBBoxMLPConfig, AbsolutePositionEmbeddings3DConfig):\n",
    "    num_objects: int = Field(..., description=\"Maximum number of objects to detect.\")\n",
    "    drop_prob: float = Field(0.0, description=\"Dropout probability for input embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRDecoder(nn.Module, PyTorchModelHubMixin):\n",
    "    \"\"\"DETR Transformer decoder.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETRDecoderConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the DETRDecoder. Activation checkpointing level 4.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETRDecoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerDecoderBlock1D(config, checkpointing_level) for _ in range(self.config.num_layers)]\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self, object_queries: torch.Tensor, embeddings: torch.Tensor, return_intermediates: bool = False\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, list[torch.Tensor]]:\n",
    "        \"\"\"Forward pass of the DETR3D decoder.\n",
    "\n",
    "        Args:\n",
    "            object_queries: Tokens that represent object queries. {INPUT_1D_DOC}\n",
    "            embeddings: Actual embeddings of the input. {INPUT_1D_DOC}\n",
    "            return_intermediates: If True, also returns the outputs of all layers. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            If return_intermediates is True, returns the final object embeddings and a list of outputs from all layers.\n",
    "            Otherwise, returns only the final object embeddings.\n",
    "        \"\"\"\n",
    "        # object_queries: (b, num_possible_objects, dim)\n",
    "        # embeddings: (b, num_embed_tokens, dim)\n",
    "\n",
    "        object_embeddings = object_queries\n",
    "\n",
    "        layer_outputs = []\n",
    "        for layer in self.layers:\n",
    "            object_embeddings = layer(object_embeddings, embeddings)\n",
    "            layer_outputs.append(object_embeddings)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return object_embeddings, layer_outputs\n",
    "        return object_embeddings\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level4(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETRDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"attn_drop_prob\": 0.2,\n",
    "    \"dim\": 54,\n",
    "    \"drop_prob\": 0.2,\n",
    "    \"embed_spacing_info\": False,\n",
    "    \"in_channels\": 1,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"learnable_absolute_position_embeddings\": False,\n",
    "    \"mlp_drop_prob\": 0.2,\n",
    "    \"num_heads\": 6,\n",
    "    \"patch_size\": (8, 16, 16),\n",
    "    \"proj_drop_prob\": 0.2,\n",
    "    \"num_layers\": 4,\n",
    "}\n",
    "\n",
    "test = DETRDecoder(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 10, 54),\n",
    "    torch.randn(2, 64, 54),\n",
    "    True,\n",
    ")\n",
    "display((o[0].shape, [x.shape for x in o[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRBBoxMLP(nn.Module):\n",
    "    \"\"\"DETR Bounding Box MLP. This module predicts bounding boxes and class scores from object query embeddings.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETRBBoxMLPConfig = {}, **kwargs):\n",
    "        \"\"\"Initialize the DETRBBoxMLP.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETRBBoxMLPConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.linear = nn.Linear(self.config.dim, 6 + 1 + self.config.num_classes)\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        object_embeddings: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the DETRBBoxMLP.\n",
    "\n",
    "        Args:\n",
    "            object_embeddings: Object embeddings from the DETR decoder. {INPUT_1D_DOC}\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (b, num_possible_objects, 1 objectness class + 6 bounding box parameters + num_classes)\n",
    "            containing the predicted bounding boxes and class scores.\n",
    "        \"\"\"\n",
    "        # object_embeddings: (b, num_possible_objects, dim)\n",
    "\n",
    "        bboxes = self.linear(object_embeddings)\n",
    "        # (b, num_possible_objects, 6 + 1 + num_classes)\n",
    "\n",
    "        # Sigmoid the bounding box parameters\n",
    "        bboxes[:, :, :6] = bboxes[:, :, :6].sigmoid()\n",
    "\n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETRBBoxMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlinear\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m17\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m10\u001b[0m, \u001b[1;36m17\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.6007\u001b[0m,  \u001b[1;36m0.5349\u001b[0m,  \u001b[1;36m0.3729\u001b[0m,  \u001b[1;36m0.6909\u001b[0m,  \u001b[1;36m0.5232\u001b[0m,  \u001b[1;36m0.5380\u001b[0m, \u001b[1;36m-0.2862\u001b[0m,  \u001b[1;36m0.1478\u001b[0m,\n",
       "        \u001b[1;36m-0.9619\u001b[0m,  \u001b[1;36m0.6182\u001b[0m,  \u001b[1;36m0.1466\u001b[0m,  \u001b[1;36m0.9278\u001b[0m,  \u001b[1;36m0.2595\u001b[0m, \u001b[1;36m-0.1345\u001b[0m, \u001b[1;36m-0.6035\u001b[0m, \u001b[1;36m-0.0612\u001b[0m,\n",
       "        \u001b[1;36m-0.0189\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSelectBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"dim\": 54,\n",
    "    \"num_classes\": 10,\n",
    "}\n",
    "\n",
    "test = DETRBBoxMLP(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 10, 54),\n",
    ")\n",
    "display((o[0].shape), o[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETR3D(nn.Module, PyTorchModelHubMixin):\n",
    "    \"\"\"DETR 3D model. Also implements bipartite matching loss which is essential for DETR training.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETR3DConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the DETR3D. Activation checkpointing level 4.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETR3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.embeddings = AbsolutePositionEmbeddings3D(config)\n",
    "        self.pos_drop = nn.Dropout(self.config.drop_prob)\n",
    "        self.num_possible_objects = self.config.num_objects\n",
    "        self.object_queries = nn.Parameter(torch.randn(1, self.num_possible_objects, self.config.dim))\n",
    "        self.decoder = DETRDecoder(config, checkpointing_level)\n",
    "        self.bbox_mlp = DETRBBoxMLP(config)\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self,\n",
    "        embeddings: torch.Tensor,\n",
    "        spacings: torch.Tensor | None = None,\n",
    "        channels_first: bool = True,\n",
    "        return_intermediates: bool = False,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor, list[torch.Tensor]]:\n",
    "        \"\"\"Forward pass of the DETR3D.\n",
    "\n",
    "        Args:\n",
    "            embeddings: Encoded input features. {INPUT_3D_DOC}\n",
    "            spacings: {SPACINGS_DOC}\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "            return_intermediates: If True, also returns the outputs of all layers. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing bounding boxes, object embeddings, and layer outputs if return_intermediates is True.\n",
    "            Else, returns only the bounding boxes.\n",
    "        \"\"\"\n",
    "        # embeddings: (b, [dim], num_tokens_z, num_tokens_y, num_tokens_x, [dim])\n",
    "        # spacings: (b, 3)\n",
    "\n",
    "        embeddings = rearrange_channels(embeddings, channels_first, True)\n",
    "        # (b, dim, num_tokens_z, num_tokens_y, num_tokens_x)\n",
    "\n",
    "        embeddings = self.embeddings(embeddings, spacings=spacings)\n",
    "        embeddings = self.pos_drop(embeddings)\n",
    "        # (b, dim, num_tokens_z, num_tokens_y, num_tokens_x)\n",
    "\n",
    "        embeddings = rearrange(embeddings, \"b d z y x -> b (z y x) d\")\n",
    "        # (b, num_embed_tokens, dim)\n",
    "\n",
    "        object_queries = repeat(self.object_queries, \"1 n d -> b n d\", b=embeddings.shape[0])\n",
    "        # (b, num_possible_objects, dim)\n",
    "\n",
    "        object_embeddings, layer_outputs = self.decoder(object_queries, embeddings, return_intermediates=True)\n",
    "        # object_embeddings: (b, num_possible_objects, dim)\n",
    "        # layer_outputs: list of (b, num_possible_objects, dim)\n",
    "\n",
    "        bboxes = self.bbox_mlp(object_embeddings)\n",
    "        # (b, num_possible_objects, 6 + 1 + num_classes)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return bboxes, object_embeddings, layer_outputs\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level4(self._forward, *args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def bipartite_matching_loss(\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor | list[torch.Tensor],\n",
    "        classification_cost_weight: float = 1.0,\n",
    "        bbox_l1_cost_weight: float = 1.0,\n",
    "        bbox_iou_cost_weight: float = 1.0,\n",
    "        reduction: str = \"mean\",\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Bipartite matching loss for DETR. The classes are expected to optimize for a multi-class classification\n",
    "        problem. Expects raw logits in class predictions, not probabilities. Use ``logits_to_scores_fn=None`` in the\n",
    "        ``forward`` function to avoid applying any transformation.\n",
    "\n",
    "        Args:\n",
    "            pred: Predicted bounding boxes and class scores. It should be of shape\n",
    "                `(B, num_objects, 6 + 1 + num_classes)`. Number of objects and number of classes will be inferred from\n",
    "                here.\n",
    "            target: Target bounding boxes and class scores. If provided as a list, each element should be a tensor for\n",
    "                the corresponding batch element in ``pred`` and therefore should have a length of `B`. Each tensor\n",
    "                should have less than or equal to the number of objects in `pred`. The number of classes can either be\n",
    "                the exact same as in `pred`, or it should be 1 argmax (one-cold) decoding.\n",
    "            classification_cost_weight: Weight for the classification cost in hungarian matching.\n",
    "            bbox_l1_cost_weight: Weight for the bounding box L1 loss cost in hungarian matching.\n",
    "            bbox_iou_cost_weight: Weight for the bounding box IoU cost in hungarian matching.\n",
    "            reduction: Specifies the reduction to apply to the output.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the bipartite matching loss with the shape depending on the `reduction` argument.\n",
    "        \"\"\"\n",
    "        B = pred.shape[0]\n",
    "\n",
    "        # Convert target to a list of tensors if not already\n",
    "        if isinstance(target, torch.Tensor):\n",
    "            target = list(target)\n",
    "\n",
    "        # argmax encode the class labels if they are not already\n",
    "        for i in range(len(target)):\n",
    "            if target[i].shape[-1] > 7:  # 6 bbox + 1 class\n",
    "                target[i] = torch.cat([target[i][:, :6], target[i][:, 6:].argmax(-1, keepdims=True)], dim=-1)\n",
    "\n",
    "        # Perform hungarian matching\n",
    "        matched_indices = DETR3D.hungarian_matching(\n",
    "            pred, target, classification_cost_weight, bbox_l1_cost_weight, bbox_iou_cost_weight\n",
    "        )\n",
    "\n",
    "        losses = []\n",
    "        for i in range(B):\n",
    "            pred_indices, target_indices = matched_indices[i]\n",
    "\n",
    "            matched_pred = pred[i][pred_indices]\n",
    "            matched_target = target[i][target_indices]\n",
    "\n",
    "            pred_bboxes = matched_pred[:, :6]\n",
    "            target_bboxes = matched_target[:, :6]\n",
    "\n",
    "            pred_classes = matched_pred[:, 6:]\n",
    "            target_class_labels = matched_target[:, 6].long()\n",
    "\n",
    "            # Compute losses for matched pairs\n",
    "            # BBox L1 loss\n",
    "            bbox_l1_loss = F.l1_loss(pred_bboxes, target_bboxes)\n",
    "\n",
    "            # BBox IOU loss\n",
    "            bbox_iou_loss = 1 - DETR3D._generalized_bbox_iou(pred_bboxes, target_bboxes)\n",
    "\n",
    "            # Classification loss\n",
    "            class_loss = F.cross_entropy(pred_classes, target_class_labels)\n",
    "\n",
    "            # Total loss for this batch element\n",
    "            total_loss = (\n",
    "                classification_cost_weight * class_loss\n",
    "                + bbox_l1_cost_weight * bbox_l1_loss\n",
    "                + bbox_iou_cost_weight * bbox_iou_loss\n",
    "            )\n",
    "            losses.append(total_loss)\n",
    "\n",
    "        # Stack batch losses and apply reduction\n",
    "        loss = torch.stack(losses)\n",
    "\n",
    "        if reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        elif reduction == \"none\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reduction mode: {reduction}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @staticmethod\n",
    "    def hungarian_matching(\n",
    "        pred: torch.Tensor,\n",
    "        target: list[torch.Tensor],\n",
    "        classification_cost_weight: float = 1.0,\n",
    "        bbox_l1_cost_weight: float = 1.0,\n",
    "        bbox_iou_cost_weight: float = 1.0,\n",
    "    ) -> list[tuple[list[int], list[int]]]:\n",
    "        \"\"\"Hungarian matching between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            pred: Predicted bounding boxes and class scores. It should be of shape\n",
    "                `(B, num_objects, 6 + 1 + num_classes)`. Number of objects and number of classes will be inferred from\n",
    "                here.\n",
    "            target: Target bounding boxes and class scores. This is in argmax encoding.\n",
    "            classification_cost_weight: Weight for the classification cost.\n",
    "            bbox_l1_cost_weight: Weight for the bounding box L1 loss cost.\n",
    "            bbox_iou_cost_weight: Weight for the bounding box IoU cost.\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples containing matched indices for predictions and targets. Each tuple is of the form\n",
    "            `(pred_indices, target_indices)`, where `pred_indices` and `target_indices` are lists of indices for the\n",
    "            matched predictions and targets, respectively.\n",
    "        \"\"\"\n",
    "        B = pred.shape[0]\n",
    "\n",
    "        matched_indices = []\n",
    "        for i in range(B):\n",
    "            pred_bboxes = pred[i, :, :6]  # (num_objects, 6)\n",
    "            target_bboxes = target[i][:, :6]  # (<=num_objects, 6)\n",
    "\n",
    "            pred_class_logits = pred[i, :, 6:]  # (num_objects, num_classes)\n",
    "            target_class_labels = target[i][:, 6].long()  # (<=num_objects,) this is in argmax encoding\n",
    "\n",
    "            # ----- Cost matrix calculation -----\n",
    "\n",
    "            # Classification cost\n",
    "            pred_class_probabilities = F.softmax(pred_class_logits, dim=-1)\n",
    "            # (num_objects, num_classes)\n",
    "\n",
    "            classification_cost = -pred_class_probabilities[:, target_class_labels]\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # L1 loss for bounding boxes\n",
    "            bbox_l1_cost = torch.cdist(pred_bboxes, target_bboxes, p=1)\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # IOU cost for bounding boxes\n",
    "            bbox_iou_cost = 1 - DETR3D._generalized_pairwise_bbox_iou(pred_bboxes, target_bboxes)\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # Total cost matrix\n",
    "            cost_matrix = (\n",
    "                classification_cost_weight * classification_cost\n",
    "                + bbox_l1_cost_weight * bbox_l1_cost\n",
    "                + bbox_iou_cost_weight * bbox_iou_cost\n",
    "            )\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # Hungarian matching\n",
    "            pred_indices_element, target_indices_element = linear_sum_assignment(cost_matrix.detach().cpu().numpy())\n",
    "\n",
    "            matched_indices.append((list(pred_indices_element), list(target_indices_element)))\n",
    "\n",
    "        return matched_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def _generalized_bbox_iou(\n",
    "        pred_bboxes: torch.Tensor,\n",
    "        target_bboxes: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the IoU loss between two matched sets of bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            pred_bbox: Predicted bounding box of shape `(num_boxes, 6)`.\n",
    "            target_bbox: Target bounding box of shape `(num_boxes, 6)`.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the IoU loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert bboxes from center format (z, y, x, d, h, w) to corner format\n",
    "        def center_to_corners(bboxes):\n",
    "            centers = bboxes[:, :3]\n",
    "            sizes = bboxes[:, 3:] / 2\n",
    "            min_coords = centers - sizes\n",
    "            max_coords = centers + sizes\n",
    "            return torch.cat([min_coords, max_coords], dim=1)  # shape (N, 6)\n",
    "\n",
    "        pred_corners = center_to_corners(pred_bboxes)\n",
    "        target_corners = center_to_corners(target_bboxes)\n",
    "\n",
    "        # Intersection corners\n",
    "        max_min = torch.max(pred_corners[:, :3], target_corners[:, :3])\n",
    "        min_max = torch.min(pred_corners[:, 3:], target_corners[:, 3:])\n",
    "        inter_dims = (min_max - max_min).clamp(min=0)\n",
    "        inter_vol = inter_dims.prod(dim=1)\n",
    "\n",
    "        # Volumes\n",
    "        pred_dims = pred_corners[:, 3:] - pred_corners[:, :3]\n",
    "        target_dims = target_corners[:, 3:] - target_corners[:, :3]\n",
    "        pred_vol = pred_dims.prod(dim=1)\n",
    "        target_vol = target_dims.prod(dim=1)\n",
    "        union_vol = pred_vol + target_vol - inter_vol\n",
    "\n",
    "        # Enclosing box corners\n",
    "        enc_min = torch.min(pred_corners[:, :3], target_corners[:, :3])\n",
    "        enc_max = torch.max(pred_corners[:, 3:], target_corners[:, 3:])\n",
    "        enc_dims = (enc_max - enc_min).clamp(min=0)\n",
    "        enc_vol = enc_dims.prod(dim=1)\n",
    "\n",
    "        iou = inter_vol / union_vol.clamp(min=1e-7)\n",
    "        giou = iou - (enc_vol - union_vol) / enc_vol.clamp(min=1e-7)\n",
    "\n",
    "        return giou.mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def _generalized_pairwise_bbox_iou(\n",
    "        pred_bboxes: torch.Tensor,\n",
    "        target_bboxes: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the IoU loss between all combinations of predicted and target bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            pred_bboxes: Predicted bounding boxes of shape `(num_objects, 6)`.\n",
    "            target_bboxes: Target bounding boxes of shape `(<=num_objects, 6)`.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the IoU losses of all combinations.\n",
    "        \"\"\"\n",
    "        # Compute pairwise IoU\n",
    "        gious = []\n",
    "        for i in range(pred_bboxes.shape[0]):\n",
    "            row_ious = []\n",
    "            for j in range(target_bboxes.shape[0]):\n",
    "                giou = DETR3D._generalized_bbox_iou(pred_bboxes[i : i + 1], target_bboxes[j : j + 1])\n",
    "                row_ious.append(giou)\n",
    "\n",
    "            gious.append(torch.stack(row_ious))\n",
    "\n",
    "        return torch.stack(gious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETR3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0membeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpos_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdecoder\u001b[1m)\u001b[0m: \u001b[1;35mDETRDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mbbox_mlp\u001b[1m)\u001b[0m: \u001b[1;35mDETRBBoxMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlinear\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m12\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m6.0385\u001b[0m, \u001b[1;36m5.0299\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m5.8381\u001b[0m, \u001b[1;36m3.9212\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"patch_size\": (8, 16, 16),\n",
    "    \"in_channels\": 1,\n",
    "    \"dim\": 54,\n",
    "    \"num_heads\": 6,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"attn_drop_prob\": 0.2,\n",
    "    \"proj_drop_prob\": 0.2,\n",
    "    \"mlp_drop_prob\": 0.2,\n",
    "    \"learnable_absolute_position_embeddings\": True,\n",
    "    \"embed_spacing_info\": False,\n",
    "    \"image_size\": (32, 512, 512),\n",
    "    \"num_objects\": 10,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_layers\": 4,\n",
    "}\n",
    "\n",
    "test = DETR3D(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 1, 4, 32, 32),\n",
    "    torch.randn(2, 3),\n",
    "    return_intermediates=True,\n",
    ")\n",
    "display((o[0].shape, o[1].shape, [x.shape for x in o[2]]))\n",
    "\n",
    "for gt_bboxes in [\n",
    "    [\n",
    "        torch.cat([torch.rand(10, 7), torch.randint(0, 5, (10, 1))], dim=-1),\n",
    "        torch.cat([torch.rand(10, 7), torch.randint(0, 5, (10, 1))], dim=-1),\n",
    "    ],  # Regular testing\n",
    "    [torch.rand(10, 12), torch.rand(2, 12)],  # Requiring argmax encoding\n",
    "]:\n",
    "    display(DETR3D.bipartite_matching_loss(o[0], gt_bboxes, reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
