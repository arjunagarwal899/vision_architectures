{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/detr_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from vision_architectures.blocks.transformer import Attention1DWithMLPConfig, TransformerDecoderBlock1D\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.layers.embeddings import AbsolutePositionEmbeddings3D, AbsolutePositionEmbeddings3DConfig\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, model_validator\n",
    "from vision_architectures.utils.rearrange import rearrange_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRDecoderConfig(Attention1DWithMLPConfig):\n",
    "    num_layers: int = Field(..., description=\"Number of transformer decoder layers.\")\n",
    "\n",
    "\n",
    "class DETRBBoxMLPConfig(CustomBaseModel):\n",
    "    dim: int = Field(..., description=\"Dimension of the input features.\")\n",
    "    num_classes: int = Field(..., description=\"Number of classes for the bounding box predictions.\")\n",
    "    bbox_size_activation: Literal[\"sigmoid\", \"softplus\"] = \"sigmoid\"\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.num_classes > 0, \"Number of classes must be greater than zero. If not predicting classes, set to 1.\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class DETR3DConfig(DETRDecoderConfig, DETRBBoxMLPConfig, AbsolutePositionEmbeddings3DConfig):\n",
    "    num_objects: int = Field(..., description=\"Maximum number of objects to detect.\")\n",
    "    drop_prob: float = Field(0.0, description=\"Dropout probability for input embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRDecoder(nn.Module, PyTorchModelHubMixin):\n",
    "    \"\"\"DETR Transformer decoder.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETRDecoderConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the DETRDecoder. Activation checkpointing level 4.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETRDecoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerDecoderBlock1D(config, checkpointing_level) for _ in range(self.config.num_layers)]\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self, object_queries: torch.Tensor, embeddings: torch.Tensor, return_intermediates: bool = False\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, list[torch.Tensor]]:\n",
    "        \"\"\"Forward pass of the DETR3D decoder.\n",
    "\n",
    "        Args:\n",
    "            object_queries: Tokens that represent object queries. {INPUT_1D_DOC}\n",
    "            embeddings: Actual embeddings of the input. {INPUT_1D_DOC}\n",
    "            return_intermediates: If True, also returns the outputs of all layers. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            If return_intermediates is True, returns the final object embeddings and a list of outputs from all layers.\n",
    "            Otherwise, returns only the final object embeddings.\n",
    "        \"\"\"\n",
    "        # object_queries: (b, num_possible_objects, dim)\n",
    "        # embeddings: (b, num_embed_tokens, dim)\n",
    "\n",
    "        object_embeddings = object_queries\n",
    "\n",
    "        layer_outputs = []\n",
    "        for layer in self.layers:\n",
    "            object_embeddings = layer(object_embeddings, embeddings)\n",
    "            layer_outputs.append(object_embeddings)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return object_embeddings, layer_outputs\n",
    "        return object_embeddings\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level4(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETRDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"attn_drop_prob\": 0.2,\n",
    "    \"dim\": 54,\n",
    "    \"drop_prob\": 0.2,\n",
    "    \"embed_spacing_info\": False,\n",
    "    \"in_channels\": 1,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"learnable_absolute_position_embeddings\": False,\n",
    "    \"mlp_drop_prob\": 0.2,\n",
    "    \"num_heads\": 6,\n",
    "    \"patch_size\": (8, 16, 16),\n",
    "    \"proj_drop_prob\": 0.2,\n",
    "    \"num_layers\": 4,\n",
    "}\n",
    "\n",
    "test = DETRDecoder(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 10, 54),\n",
    "    torch.randn(2, 64, 54),\n",
    "    True,\n",
    ")\n",
    "display((o[0].shape, [x.shape for x in o[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRBBoxMLP(nn.Module):\n",
    "    \"\"\"DETR Bounding Box MLP. This module predicts bounding boxes and class scores from object query embeddings.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETRBBoxMLPConfig = {}, **kwargs):\n",
    "        \"\"\"Initialize the DETRBBoxMLP.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETRBBoxMLPConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.linear = nn.Linear(self.config.dim, 6 + 1 + self.config.num_classes)\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        object_embeddings: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the DETRBBoxMLP.\n",
    "\n",
    "        Args:\n",
    "            object_embeddings: Object embeddings from the DETR decoder. {INPUT_1D_DOC}\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (b, num_possible_objects, 1 objectness class + 6 bounding box parameters + num_classes)\n",
    "            containing the predicted bounding boxes and class scores. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "        \"\"\"\n",
    "        # object_embeddings: (b, num_possible_objects, dim)\n",
    "\n",
    "        bboxes = self.linear(object_embeddings)\n",
    "        # (b, num_possible_objects, 6 + 1 + num_classes)\n",
    "\n",
    "        # Bound the bounding box parameters\n",
    "        centers, sizes, logits = bboxes[:, :, :3], bboxes[:, :, 3:6], bboxes[:, :, 6:]\n",
    "        centers = centers.sigmoid()  # center coordinates should be in the bbox\n",
    "        if self.config.bbox_size_activation == \"sigmoid\":\n",
    "            sizes = sizes.sigmoid()  # sizes should be between 0 and 1\n",
    "        elif self.config.bbox_size_activation == \"softplus\":\n",
    "            sizes = F.softplus(sizes)  # sizes should be positive but unbounded\n",
    "        bboxes = torch.cat([centers, sizes, logits], dim=-1)\n",
    "\n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETRBBoxMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlinear\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m17\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m10\u001b[0m, \u001b[1;36m17\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.5408\u001b[0m,  \u001b[1;36m0.4994\u001b[0m,  \u001b[1;36m0.4254\u001b[0m,  \u001b[1;36m0.5085\u001b[0m,  \u001b[1;36m0.3362\u001b[0m,  \u001b[1;36m0.4264\u001b[0m, \u001b[1;36m-1.2220\u001b[0m, \u001b[1;36m-0.0613\u001b[0m,\n",
       "         \u001b[1;36m1.0281\u001b[0m, \u001b[1;36m-0.3249\u001b[0m,  \u001b[1;36m0.3751\u001b[0m, \u001b[1;36m-0.9232\u001b[0m,  \u001b[1;36m1.4117\u001b[0m,  \u001b[1;36m0.1219\u001b[0m,  \u001b[1;36m1.3651\u001b[0m,  \u001b[1;36m0.2143\u001b[0m,\n",
       "         \u001b[1;36m0.3949\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSelectBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"dim\": 54,\n",
    "    \"num_classes\": 10,\n",
    "}\n",
    "\n",
    "test = DETRBBoxMLP(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 10, 54),\n",
    ")\n",
    "display((o[0].shape), o[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETR3D(nn.Module, PyTorchModelHubMixin):\n",
    "    \"\"\"DETR 3D model. Also implements bipartite matching loss which is essential for DETR training.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETR3DConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the DETR3D. Activation checkpointing level 4.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETR3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.position_embeddings = AbsolutePositionEmbeddings3D(config)\n",
    "        self.pos_drop = nn.Dropout(self.config.drop_prob)\n",
    "        self.num_possible_objects = self.config.num_objects\n",
    "        self.object_queries = nn.Parameter(torch.randn(1, self.num_possible_objects, self.config.dim) * 0.01)\n",
    "        self.decoder = DETRDecoder(config, checkpointing_level)\n",
    "        self.bbox_mlp = DETRBBoxMLP(config)\n",
    "\n",
    "        self.class_prevalences = [None] * (self.config.num_classes + 1)  # used to weight cross entropy loss\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self,\n",
    "        embeddings: torch.Tensor,\n",
    "        spacings: torch.Tensor | None = None,\n",
    "        channels_first: bool = True,\n",
    "        return_intermediates: bool = False,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor, list[torch.Tensor]]:\n",
    "        \"\"\"Forward pass of the DETR3D.\n",
    "\n",
    "        Args:\n",
    "            embeddings: Encoded input features. {INPUT_3D_DOC}\n",
    "            spacings: {SPACINGS_DOC}\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "            return_intermediates: If True, also returns the outputs of all layers. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing bounding boxes, object embeddings, and layer outputs if return_intermediates is True.\n",
    "            Else, returns only the bounding boxes. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "        \"\"\"\n",
    "        # embeddings: (b, [dim], num_tokens_z, num_tokens_y, num_tokens_x, [dim])\n",
    "        # spacings: (b, 3)\n",
    "\n",
    "        embeddings = rearrange_channels(embeddings, channels_first, True)\n",
    "        # (b, dim, num_tokens_z, num_tokens_y, num_tokens_x)\n",
    "\n",
    "        embeddings = self.position_embeddings(embeddings, spacings=spacings)\n",
    "        embeddings = self.pos_drop(embeddings)\n",
    "        # (b, dim, num_tokens_z, num_tokens_y, num_tokens_x)\n",
    "\n",
    "        embeddings = rearrange(embeddings, \"b d z y x -> b (z y x) d\")\n",
    "        # (b, num_embed_tokens, dim)\n",
    "\n",
    "        object_queries = repeat(self.object_queries, \"1 n d -> b n d\", b=embeddings.shape[0])\n",
    "        # (b, num_possible_objects, dim)\n",
    "\n",
    "        object_embeddings, layer_outputs = self.decoder(object_queries, embeddings, return_intermediates=True)\n",
    "        # object_embeddings: (b, num_possible_objects, dim)\n",
    "        # layer_outputs: list of (b, num_possible_objects, dim)\n",
    "\n",
    "        bboxes = self.bbox_mlp(object_embeddings)\n",
    "        # (b, num_possible_objects, 6 + 1 + num_classes)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return bboxes, object_embeddings, layer_outputs\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level4(self._forward, *args, **kwargs)\n",
    "\n",
    "    def bipartite_matching_loss(\n",
    "        self,\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor | list[torch.Tensor],\n",
    "        classification_cost_weight: float = 1.0,\n",
    "        bbox_l1_cost_weight: float = 1.0,\n",
    "        bbox_iou_cost_weight: float = 1.0,\n",
    "        weight_loss_based_on_prevalence: bool = True,\n",
    "        reduction: str = \"mean\",\n",
    "        return_matching: bool = False,\n",
    "        return_loss_components: bool = False,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, list] | tuple[torch.Tensor, dict] | tuple[torch.Tensor, list, dict]:\n",
    "        \"\"\"Bipartite matching loss for DETR. The classes are expected to optimize for a multi-class classification\n",
    "        problem. Expects raw logits in class predictions, not probabilities. Use ``logits_to_scores_fn=None`` in the\n",
    "        ``forward`` function to avoid applying any transformation.\n",
    "\n",
    "        Args:\n",
    "            pred: Predicted bounding boxes and class scores. It should be of shape\n",
    "                `(B, num_objects, 6 + 1 + num_classes)`. Number of objects and number of classes will be inferred from\n",
    "                here.\n",
    "            target: Target bounding boxes and class scores. If provided as a list, each element should be a tensor for\n",
    "                the corresponding batch element in ``pred`` and therefore should have a length of `B`. Each tensor\n",
    "                should have less than or equal to the number of objects in `pred`. The number of classes can either be\n",
    "                the exact same as in `pred`, or it should be 1 argmax (one-cold) decoding.\n",
    "            classification_cost_weight: Weight for the classification cost in hungarian matching.\n",
    "            bbox_l1_cost_weight: Weight for the bounding box L1 loss cost in hungarian matching.\n",
    "            bbox_iou_cost_weight: Weight for the bounding box IoU cost in hungarian matching.\n",
    "            weight_loss_based_on_prevalence: Whether to weight the classification loss based on the prevalence of\n",
    "                each class in the dataset. If True, the loss will be weighted by the inverse prevalence of each class.\n",
    "            reduction: Specifies the reduction to apply to the output.\n",
    "            return_matching: Whether or not to return the matched indices from the bipartite matching.\n",
    "            return_loss_components: Whether or not to return the individual loss components.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the bipartite matching loss with the shape depending on the `reduction` argument. If\n",
    "            `return_matching` is True, also returns a list of tuples containing matched indices for predictions and\n",
    "            targets. Each tuple is of the form `(pred_indices, target_indices)`, where `pred_indices` and\n",
    "            `target_indices` are lists of indices for the matched predictions and targets, respectively. If\n",
    "            `return_loss_components` is True, also returns a dict of each loss component reduced based on `reduction`.\n",
    "        \"\"\"\n",
    "        B = pred.shape[0]\n",
    "\n",
    "        # Convert target to a list of tensors if not already\n",
    "        if isinstance(target, torch.Tensor):\n",
    "            target = list(target)\n",
    "\n",
    "        # argmax encode the class labels if they are not already\n",
    "        for i in range(len(target)):\n",
    "            if target[i].shape[-1] > 7:  # 6 bbox + 1 class\n",
    "                target[i] = torch.cat([target[i][:, :6], target[i][:, 6:].argmax(-1, keepdims=True)], dim=-1)\n",
    "\n",
    "        # Perform hungarian matching\n",
    "        matched_indices = DETR3D.hungarian_matching(\n",
    "            pred, target, classification_cost_weight, bbox_l1_cost_weight, bbox_iou_cost_weight\n",
    "        )\n",
    "\n",
    "        # Update class prevalences and calculate cross entropy loss weights if applicable\n",
    "        self._update_class_prevalences(target)\n",
    "        if weight_loss_based_on_prevalence:\n",
    "            classification_loss_weights = self._get_cross_entropy_loss_weights(pred.device)\n",
    "        else:\n",
    "            classification_loss_weights = torch.ones(\n",
    "                (self.config.num_classes + 1,), dtype=torch.float32, device=pred.device\n",
    "            )\n",
    "\n",
    "        losses = {\n",
    "            \"classification_loss\": [],\n",
    "            \"bbox_l1_loss\": [],\n",
    "            \"bbox_iou_loss\": [],\n",
    "            \"total_loss\": [],\n",
    "        }\n",
    "        for i in range(B):\n",
    "            batch_losses = {}\n",
    "\n",
    "            pred_indices, target_indices = matched_indices[i]\n",
    "\n",
    "            # Classification loss for ALL predictions\n",
    "            all_class_labels = torch.zeros_like(pred[i][:, 6], dtype=torch.long)\n",
    "            all_class_labels[pred_indices] = target[i][target_indices, 6].long()\n",
    "            all_pred_classes = pred[i][..., 6:]\n",
    "            batch_losses[\"classification_loss\"] = F.cross_entropy(\n",
    "                all_pred_classes, all_class_labels, weight=classification_loss_weights\n",
    "            )\n",
    "\n",
    "            # Calculate all other losses only if prediction and target have objects\n",
    "            if pred_indices != [] and target_indices != []:\n",
    "                matched_pred = pred[i][pred_indices]\n",
    "                matched_target = target[i][target_indices]\n",
    "\n",
    "                pred_bboxes = matched_pred[:, :6].clone()\n",
    "                target_bboxes = matched_target[:, :6].clone()\n",
    "\n",
    "                # Compute losses for matched pairs\n",
    "                # BBox L1 loss\n",
    "                batch_losses[\"bbox_l1_loss\"] = F.l1_loss(pred_bboxes, target_bboxes)\n",
    "\n",
    "                # For IOU calculation, it does not matter if bboxes are in actual pixel lengths or normalized based on\n",
    "                # image size, metric value will be the same.\n",
    "\n",
    "                # BBox IOU loss\n",
    "                batch_losses[\"bbox_iou_loss\"] = 1 - DETR3D._generalized_bbox_iou(pred_bboxes, target_bboxes).mean()\n",
    "\n",
    "            # Total loss for this batch element\n",
    "            total_loss = (\n",
    "                classification_cost_weight * batch_losses.get(\"classification_loss\", 0.0)\n",
    "                + bbox_l1_cost_weight * batch_losses.get(\"bbox_l1_loss\", 0.0)\n",
    "                + bbox_iou_cost_weight * batch_losses.get(\"bbox_iou_loss\", 0.0)\n",
    "            )\n",
    "\n",
    "            # Add to losses\n",
    "            losses[\"classification_loss\"].append(\n",
    "                batch_losses.get(\"classification_loss\", torch.tensor(torch.nan, device=pred.device)),\n",
    "            )\n",
    "            losses[\"bbox_l1_loss\"].append(\n",
    "                batch_losses.get(\"bbox_l1_loss\", torch.tensor(torch.nan, device=pred.device)),\n",
    "            )\n",
    "            losses[\"bbox_iou_loss\"].append(\n",
    "                batch_losses.get(\"bbox_iou_loss\", torch.tensor(torch.nan, device=pred.device)),\n",
    "            )\n",
    "            losses[\"total_loss\"].append(total_loss)\n",
    "\n",
    "        # Stack batch losses and apply reduction\n",
    "        for key in losses:\n",
    "            loss = torch.stack(losses[key])\n",
    "\n",
    "            if reduction == \"mean\":\n",
    "                loss = loss.nanmean()\n",
    "            elif reduction == \"sum\":\n",
    "                loss = loss.nansum()\n",
    "            elif reduction == \"none\" or reduction is None:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid reduction mode: {reduction}\")\n",
    "\n",
    "            losses[key] = loss\n",
    "\n",
    "        loss = losses[\"total_loss\"]\n",
    "        losses.pop(\"total_loss\", None)\n",
    "\n",
    "        match return_matching, return_loss_components:\n",
    "            case False, False:\n",
    "                return loss\n",
    "            case True, False:\n",
    "                return loss, matched_indices\n",
    "            case False, True:\n",
    "                return loss, losses\n",
    "            case True, True:\n",
    "                return loss, matched_indices, losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @staticmethod\n",
    "    def hungarian_matching(\n",
    "        pred: torch.Tensor,\n",
    "        target: list[torch.Tensor],\n",
    "        classification_cost_weight: float = 1.0,\n",
    "        bbox_l1_cost_weight: float = 1.0,\n",
    "        bbox_iou_cost_weight: float = 1.0,\n",
    "    ) -> list[tuple[list[int], list[int]]]:\n",
    "        \"\"\"Hungarian matching between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            pred: Predicted bounding boxes and class scores. It should be of shape\n",
    "                `(B, num_objects, 6 + 1 + num_classes)`. Number of objects and number of classes will be inferred from\n",
    "                here.\n",
    "            target: Target bounding boxes and class scores. This is expected in argmax encoding or one-hot encoding.\n",
    "            classification_cost_weight: Weight for the classification cost.\n",
    "            bbox_l1_cost_weight: Weight for the bounding box L1 loss cost.\n",
    "            bbox_iou_cost_weight: Weight for the bounding box IoU cost.\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples containing matched indices for predictions and targets. Each tuple is of the form\n",
    "            `(pred_indices, target_indices)`, where `pred_indices` and `target_indices` are lists of indices for the\n",
    "            matched predictions and targets, respectively.\n",
    "        \"\"\"\n",
    "        B = pred.shape[0]\n",
    "\n",
    "        matched_indices = []\n",
    "        for i in range(B):\n",
    "            pred_bboxes = pred[i, :, :6]  # (num_objects, 6)\n",
    "            target_bboxes = target[i][:, :6]  # (<=num_objects, 6)\n",
    "\n",
    "            # If either prediction or target has no objects, skip matching\n",
    "            if pred_bboxes.shape[0] == 0 or target_bboxes.shape[0] == 0:\n",
    "                matched_indices.append(([], []))\n",
    "                continue\n",
    "\n",
    "            pred_class_logits = pred[i, :, 6:]  # (num_objects, num_classes)\n",
    "            target_class_labels = target[i][:, 6].long()  # (<=num_objects,) this is in argmax encoding\n",
    "\n",
    "            # ----- Cost matrix calculation -----\n",
    "\n",
    "            # Classification cost\n",
    "            pred_class_probabilities = F.softmax(pred_class_logits, dim=-1)\n",
    "            # (num_objects, num_classes)\n",
    "\n",
    "            classification_cost = -pred_class_probabilities[:, target_class_labels]\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # L1 loss for bounding boxes\n",
    "            bbox_l1_cost = torch.cdist(pred_bboxes, target_bboxes, p=1)\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # IOU cost for bounding boxes\n",
    "            bbox_iou_cost = 1 - DETR3D._generalized_pairwise_bbox_iou(pred_bboxes, target_bboxes)\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # Total cost matrix\n",
    "            cost_matrix = (\n",
    "                classification_cost_weight * classification_cost\n",
    "                + bbox_l1_cost_weight * bbox_l1_cost\n",
    "                + bbox_iou_cost_weight * bbox_iou_cost\n",
    "            )\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # Hungarian matching\n",
    "            pred_indices_element, target_indices_element = linear_sum_assignment(cost_matrix.detach().cpu().numpy())\n",
    "\n",
    "            matched_indices.append((list(pred_indices_element), list(target_indices_element)))\n",
    "\n",
    "        return matched_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def _generalized_bbox_iou(\n",
    "        pred_bboxes: torch.Tensor,\n",
    "        target_bboxes: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the IoU loss between two matched sets of bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            pred_bboxes: Predicted bounding boxes of shape `(num_boxes, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "            target_bboxes: Target bounding boxes of shape `(num_boxes, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the IoU loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert bboxes from center format (z, y, x, d, h, w) to corner format\n",
    "        def center_to_corners(bboxes):\n",
    "            centers = bboxes[:, :3]\n",
    "            sizes = bboxes[:, 3:] / 2\n",
    "            min_coords = centers - sizes\n",
    "            max_coords = centers + sizes\n",
    "            return torch.cat([min_coords, max_coords], dim=1)  # shape (N, 6)\n",
    "\n",
    "        pred_corners = center_to_corners(pred_bboxes)\n",
    "        target_corners = center_to_corners(target_bboxes)\n",
    "\n",
    "        # Intersection corners\n",
    "        max_min = torch.max(pred_corners[:, :3], target_corners[:, :3])\n",
    "        min_max = torch.min(pred_corners[:, 3:], target_corners[:, 3:])\n",
    "        inter_dims = (min_max - max_min).clamp(min=0)\n",
    "        inter_vol = inter_dims.prod(dim=1)\n",
    "\n",
    "        # Volumes\n",
    "        pred_dims = pred_corners[:, 3:] - pred_corners[:, :3]\n",
    "        target_dims = target_corners[:, 3:] - target_corners[:, :3]\n",
    "        pred_vol = pred_dims.prod(dim=1)\n",
    "        target_vol = target_dims.prod(dim=1)\n",
    "        union_vol = pred_vol + target_vol - inter_vol\n",
    "\n",
    "        # Enclosing box corners\n",
    "        enc_min = torch.min(pred_corners[:, :3], target_corners[:, :3])\n",
    "        enc_max = torch.max(pred_corners[:, 3:], target_corners[:, 3:])\n",
    "        enc_dims = (enc_max - enc_min).clamp(min=0)\n",
    "        enc_vol = enc_dims.prod(dim=1)\n",
    "\n",
    "        iou = inter_vol / union_vol.clamp(min=1e-7)\n",
    "        gious = iou - (enc_vol - union_vol) / enc_vol.clamp(min=1e-7)\n",
    "\n",
    "        return gious\n",
    "\n",
    "    @staticmethod\n",
    "    def _generalized_pairwise_bbox_iou(\n",
    "        pred_bboxes: torch.Tensor,\n",
    "        target_bboxes: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the IoU loss between all combinations of predicted and target bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            pred_bboxes: Predicted bounding boxes of shape `(num_objects, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "            target_bboxes: Target bounding boxes of shape `(<=num_objects, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the IoU losses of all combinations.\n",
    "        \"\"\"\n",
    "        # Compute pairwise IoU\n",
    "        gious = []\n",
    "        for i in range(pred_bboxes.shape[0]):\n",
    "            row_ious = []\n",
    "            for j in range(target_bboxes.shape[0]):\n",
    "                row_ious.append(DETR3D._generalized_bbox_iou(pred_bboxes[i : i + 1], target_bboxes[j : j + 1]).mean())\n",
    "\n",
    "            gious.append(torch.stack(row_ious))\n",
    "\n",
    "        return torch.stack(gious)\n",
    "\n",
    "    def _update_class_prevalences(self, target: list[torch.Tensor]):\n",
    "        \"\"\"Update the class prevalences based on the classes present in the ground truth\n",
    "\n",
    "        Args:\n",
    "            target: A list of ground truth tensors, each of shape (num_objects, 7) where the last dimension contains the\n",
    "                bounding box class.\n",
    "        \"\"\"\n",
    "        # Store the number of times each class has been encountered in this batch\n",
    "        class_counts = [0] * (self.config.num_classes + 1)\n",
    "        B = len(target)\n",
    "        for b in range(B):\n",
    "            # If there are more than self.config.num_objects targets, only the first self.config.num_objects will be\n",
    "            # considered\n",
    "            for c in target[b][: self.config.num_objects, -1].int():\n",
    "                class_counts[c] += 1\n",
    "\n",
    "            # For background class, prevalence is the number of extra bboxes that were predicted\n",
    "            class_counts[0] += max(0, self.config.num_objects - len(target[b]))\n",
    "            # max in case there are more target bboxes than self.config.num_objects\n",
    "\n",
    "        # Calculate prevalence of each class\n",
    "        new_prevalences = [count / sum(class_counts) for count in class_counts]\n",
    "\n",
    "        # Update current prevalences using EMA to allow for distribution shift in data\n",
    "        for i in range(self.config.num_classes + 1):\n",
    "            if self.class_prevalences[i] is None:\n",
    "                if new_prevalences[i] > 0:\n",
    "                    self.class_prevalences[i] = new_prevalences[i]\n",
    "                # otherwise let it stay None\n",
    "            else:\n",
    "                # update to new value using EMA\n",
    "                self.class_prevalences[i] = self.class_prevalences[i] * 0.99 + new_prevalences[i] * 0.01\n",
    "\n",
    "    def _get_cross_entropy_loss_weights(self, device=torch.device(\"cpu\")) -> torch.Tensor:\n",
    "        \"\"\"Get the weights for the cross entropy loss based on class prevalences\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the weights for the cross entropy loss of shape (num_classes + 1,).\n",
    "        \"\"\"\n",
    "        # Get class prevalences and replace None with nan\n",
    "        class_prevalences = [\n",
    "            prevalence if prevalence is not None else torch.nan for prevalence in self.class_prevalences\n",
    "        ]\n",
    "\n",
    "        # Calculate weights as inverse of class prevalences\n",
    "        weights = 1 / torch.tensor(class_prevalences, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Substitute nan values with mean and clamp weights to a limit\n",
    "        # Assumption: all classes are visited at least once in the dataset\n",
    "        mu, std = weights.nanmean(), weights[~weights.isnan()].std()\n",
    "        weights = weights.nan_to_num(mu)\n",
    "        weights = weights.clamp(mu - 3 * std, mu + 3 * std)\n",
    "\n",
    "        # Normalize weights to sum to (self.config.num_classes + 1)\n",
    "        weights = (self.config.num_classes + 1) * weights / weights.sum()\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETR3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpos_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdecoder\u001b[1m)\u001b[0m: \u001b[1;35mDETRDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mbbox_mlp\u001b[1m)\u001b[0m: \u001b[1;35mDETRBBoxMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlinear\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m12\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4.3433\u001b[0m, \u001b[1;36m3.2983\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m2.5759\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.7373\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2747\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.1793\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_iou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.4927\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.3817\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3.7995\u001b[0m, \u001b[1;36m4.0535\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m2.0776\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m2.3716\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2339\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.2273\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_iou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.4880\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.4546\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.9979\u001b[0m, \u001b[1;36m1.1660\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.9979\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.1660\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'bbox_l1_loss'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'bbox_iou_loss'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"patch_size\": (8, 16, 16),\n",
    "    \"in_channels\": 1,\n",
    "    \"dim\": 54,\n",
    "    \"num_heads\": 6,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"attn_drop_prob\": 0.2,\n",
    "    \"proj_drop_prob\": 0.2,\n",
    "    \"mlp_drop_prob\": 0.2,\n",
    "    \"learnable_absolute_position_embeddings\": True,\n",
    "    \"embed_spacing_info\": False,\n",
    "    \"image_size\": (32, 512, 512),\n",
    "    \"num_objects\": 10,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_layers\": 4,\n",
    "}\n",
    "\n",
    "test = DETR3D(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 1, 4, 32, 32),\n",
    "    torch.randn(2, 3),\n",
    "    return_intermediates=True,\n",
    ")\n",
    "display((o[0].shape, o[1].shape, [x.shape for x in o[2]]))\n",
    "\n",
    "for gt_bboxes in [\n",
    "    [\n",
    "        torch.cat([torch.rand(10, 6), torch.randint(0, 6, (10, 1))], dim=-1),\n",
    "        torch.cat([torch.rand(2, 6), torch.randint(0, 6, (2, 1))], dim=-1),\n",
    "    ],  # Regular testing\n",
    "    [\n",
    "        torch.rand(10, 12),\n",
    "        torch.rand(2, 12),\n",
    "    ],  # Requiring argmax encoding\n",
    "    [\n",
    "        torch.tensor([]).reshape(0, 12),\n",
    "        torch.tensor([]).reshape(0, 7),\n",
    "    ],  # No ground truth boxes, classification loss should still be calculated\n",
    "]:\n",
    "    display(test.bipartite_matching_loss(o[0], gt_bboxes, reduction=\"none\", return_loss_components=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
