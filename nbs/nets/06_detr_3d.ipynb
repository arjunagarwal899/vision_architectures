{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/detr_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from vision_architectures.blocks.transformer import Attention1DWithMLPConfig, TransformerDecoderBlock1D\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.layers.embeddings import AbsolutePositionEmbeddings3D, AbsolutePositionEmbeddings3DConfig\n",
    "from vision_architectures.losses.class_balanced_cross_entropy_loss import (\n",
    "    ClassBalancedCrossEntropyLoss,\n",
    "    ClassBalancedCrossEntropyLossConfig,\n",
    ")\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, model_validator\n",
    "from vision_architectures.utils.rearrange import rearrange_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRDecoderConfig(Attention1DWithMLPConfig):\n",
    "    num_layers: int = Field(..., description=\"Number of transformer decoder layers.\")\n",
    "\n",
    "\n",
    "class DETRBBoxMLPConfig(CustomBaseModel):\n",
    "    dim: int = Field(..., description=\"Dimension of the input features.\")\n",
    "    num_classes: int = Field(..., description=\"Number of classes for the bounding box predictions.\")\n",
    "    bbox_size_activation: Literal[\"sigmoid\", \"softplus\"] = Field(\n",
    "        \"sigmoid\",\n",
    "        description=(\n",
    "            'Activation function for the bounding box size. \"sigmoid\" for normalized coordinates, \"softplus\" '\n",
    "            \"for absolute coordinates.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.num_classes > 0, \"Number of classes must be greater than zero. If not predicting classes, set to 1.\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class DETR3DConfig(DETRDecoderConfig, DETRBBoxMLPConfig, AbsolutePositionEmbeddings3DConfig):\n",
    "    num_objects: int = Field(..., description=\"Maximum number of objects to detect.\")\n",
    "    drop_prob: float = Field(0.0, description=\"Dropout probability for input embeddings.\")\n",
    "    classification_cost_fn: Literal[\"softmax\", \"log_softmax\"] = Field(\n",
    "        \"softmax\",\n",
    "        description=(\n",
    "            \"Method used to compute classification cost in Hungarian matching. While softmax is the default (as per \"\n",
    "            \"the paper), log_sofmtax is encouraged as the final bipartite matching loss calculation uses cross entropy \"\n",
    "            \"loss which performs log_softmax and NLLLoss internally making log_softmax more apt.\"\n",
    "        ),\n",
    "    )\n",
    "    classification_loss_fn: (\n",
    "        Literal[\"cross_entropy\", \"class_balanced_cross_entropy\"] | dict | ClassBalancedCrossEntropyLossConfig\n",
    "    ) = Field(\"cross_entropy\", description=\"Loss function for bbox classification.\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "\n",
    "        # Create an object of ClassBalancedCrossEntropyLossConfig if applicable\n",
    "        if self.classification_loss_fn == \"class_balanced_cross_entropy\":\n",
    "            self.classification_loss_fn = ClassBalancedCrossEntropyLossConfig(num_classes=self.num_classes + 1)\n",
    "        elif isinstance(self.classification_loss_fn, dict):\n",
    "            self.classification_loss_fn = ClassBalancedCrossEntropyLossConfig.model_validate(\n",
    "                self.classification_loss_fn\n",
    "            )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRDecoder(nn.Module, PyTorchModelHubMixin):\n",
    "    \"\"\"DETR Transformer decoder.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETRDecoderConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the DETRDecoder. Activation checkpointing level 4.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETRDecoderConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerDecoderBlock1D(config, checkpointing_level) for _ in range(self.config.num_layers)]\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self, object_queries: torch.Tensor, embeddings: torch.Tensor, return_intermediates: bool = False\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, list[torch.Tensor]]:\n",
    "        \"\"\"Forward pass of the DETR3D decoder.\n",
    "\n",
    "        Args:\n",
    "            object_queries: Tokens that represent object queries. {INPUT_1D_DOC}\n",
    "            embeddings: Actual embeddings of the input. {INPUT_1D_DOC}\n",
    "            return_intermediates: If True, also returns the outputs of all layers. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            If return_intermediates is True, returns the final object embeddings and a list of outputs from all layers.\n",
    "            Otherwise, returns only the final object embeddings.\n",
    "        \"\"\"\n",
    "        # object_queries: (b, num_possible_objects, dim)\n",
    "        # embeddings: (b, num_embed_tokens, dim)\n",
    "\n",
    "        object_embeddings = object_queries\n",
    "\n",
    "        layer_outputs = []\n",
    "        for layer in self.layers:\n",
    "            object_embeddings = layer(object_embeddings, embeddings)\n",
    "            layer_outputs.append(object_embeddings)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return object_embeddings, layer_outputs\n",
    "        return object_embeddings\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level4(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETRDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"attn_drop_prob\": 0.2,\n",
    "    \"dim\": 54,\n",
    "    \"drop_prob\": 0.2,\n",
    "    \"embed_spacing_info\": False,\n",
    "    \"in_channels\": 1,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"learnable_absolute_position_embeddings\": False,\n",
    "    \"mlp_drop_prob\": 0.2,\n",
    "    \"num_heads\": 6,\n",
    "    \"patch_size\": (8, 16, 16),\n",
    "    \"proj_drop_prob\": 0.2,\n",
    "    \"num_layers\": 4,\n",
    "}\n",
    "\n",
    "test = DETRDecoder(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 10, 54),\n",
    "    torch.randn(2, 64, 54),\n",
    "    True,\n",
    ")\n",
    "display((o[0].shape, [x.shape for x in o[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETRBBoxMLP(nn.Module):\n",
    "    \"\"\"DETR Bounding Box MLP. This module predicts bounding boxes and class scores from object query embeddings.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETRBBoxMLPConfig = {}, **kwargs):\n",
    "        \"\"\"Initialize the DETRBBoxMLP.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETRBBoxMLPConfig.model_validate(config | kwargs)\n",
    "\n",
    "        # Keeping bbox and classification heads separate so that bias-init/clipping/freezing can be done if needed\n",
    "        self.bbox_head = nn.Linear(self.config.dim, 6)\n",
    "        self.class_head = nn.Linear(self.config.dim, 1 + self.config.num_classes)\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        object_embeddings: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the DETRBBoxMLP.\n",
    "\n",
    "        Args:\n",
    "            object_embeddings: Object embeddings from the DETR decoder. {INPUT_1D_DOC}\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (b, num_possible_objects, 1 objectness class + 6 bounding box parameters + num_classes)\n",
    "            containing the predicted bounding boxes and class scores. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "        \"\"\"\n",
    "        # object_embeddings: (b, num_possible_objects, dim)\n",
    "\n",
    "        bbox_parameters: torch.Tensor = self.bbox_head(object_embeddings)\n",
    "        class_logits: torch.Tensor = self.class_head(object_embeddings)\n",
    "        # (b, num_possible_objects, 6 + 1 + num_classes)\n",
    "\n",
    "        # Bound the bounding box parameters\n",
    "        centers, sizes = bbox_parameters[:, :, :3], bbox_parameters[:, :, 3:6]\n",
    "        centers = centers.sigmoid()  # center coordinates should be in the bbox\n",
    "        if self.config.bbox_size_activation == \"sigmoid\":\n",
    "            sizes = sizes.sigmoid()  # sizes should be between 0 and 1\n",
    "        elif self.config.bbox_size_activation == \"softplus\":\n",
    "            sizes = F.softplus(sizes)  # sizes should be positive but unbounded\n",
    "        bboxes = torch.cat([centers, sizes, class_logits], dim=-1)\n",
    "\n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETRBBoxMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mbbox_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mclass_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m11\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m10\u001b[0m, \u001b[1;36m17\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.3917\u001b[0m,  \u001b[1;36m0.5846\u001b[0m,  \u001b[1;36m0.2497\u001b[0m,  \u001b[1;36m0.4194\u001b[0m,  \u001b[1;36m0.4995\u001b[0m,  \u001b[1;36m0.6928\u001b[0m,  \u001b[1;36m0.1971\u001b[0m, \u001b[1;36m-0.3480\u001b[0m,\n",
       "         \u001b[1;36m0.8096\u001b[0m,  \u001b[1;36m0.8929\u001b[0m, \u001b[1;36m-0.2808\u001b[0m,  \u001b[1;36m0.0617\u001b[0m, \u001b[1;36m-0.6956\u001b[0m, \u001b[1;36m-0.3701\u001b[0m, \u001b[1;36m-0.4366\u001b[0m,  \u001b[1;36m0.3426\u001b[0m,\n",
       "        \u001b[1;36m-0.0956\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSelectBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"dim\": 54,\n",
    "    \"num_classes\": 10,\n",
    "}\n",
    "\n",
    "test = DETRBBoxMLP(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 10, 54),\n",
    ")\n",
    "display((o[0].shape), o[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def bbox_zyxdhw_to_z1z2y1y2x1x2(bboxes: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Convert bboxes from center format (z, y, x, d, h, w) to corner format (z1, z2, y1, y2, x1, x2)\n",
    "\n",
    "    Args:\n",
    "        bboxes: Bounding boxes of shape (N, 6) in center format (z, y, x, d, h, w)\n",
    "\n",
    "    Returns:\n",
    "        Bounding boxes of shape (N, 6) in corner format (z1, z2, y1, y2, x1, x2)\n",
    "    \"\"\"\n",
    "    centers = bboxes[:, :3]\n",
    "    sizes = bboxes[:, 3:] / 2\n",
    "    min_coords = centers - sizes\n",
    "    max_coords = centers + sizes\n",
    "    return torch.cat([min_coords, max_coords], dim=1)  # shape (N, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DETR3D(nn.Module, PyTorchModelHubMixin):\n",
    "    \"\"\"DETR 3D model. Also implements bipartite matching loss which is essential for DETR training.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: DETR3DConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the DETR3D. Activation checkpointing level 4.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = DETR3DConfig.model_validate(config | kwargs)\n",
    "\n",
    "        self.position_embeddings = AbsolutePositionEmbeddings3D(config)\n",
    "        self.pos_drop = nn.Dropout(self.config.drop_prob)\n",
    "        self.num_possible_objects = self.config.num_objects\n",
    "        self.object_queries = nn.Parameter(torch.randn(1, self.num_possible_objects, self.config.dim) * 0.01)\n",
    "        self.decoder = DETRDecoder(config, checkpointing_level)\n",
    "        self.bbox_mlp = DETRBBoxMLP(config)\n",
    "\n",
    "        self.class_balanced_cross_entropy_loss = None\n",
    "        if isinstance(self.config.classification_loss_fn, ClassBalancedCrossEntropyLossConfig):\n",
    "            self.class_balanced_cross_entropy_loss = ClassBalancedCrossEntropyLoss(\n",
    "                config=self.config.classification_loss_fn\n",
    "            )\n",
    "\n",
    "        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self,\n",
    "        embeddings: torch.Tensor,\n",
    "        spacings: torch.Tensor | None = None,\n",
    "        channels_first: bool = True,\n",
    "        return_intermediates: bool = False,\n",
    "        process_intermediates: bool = True,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor, list[torch.Tensor]]:\n",
    "        \"\"\"Forward pass of the DETR3D.\n",
    "\n",
    "        Args:\n",
    "            embeddings: Encoded input features. {INPUT_3D_DOC}\n",
    "            spacings: {SPACINGS_DOC}\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "            return_intermediates: If True, also returns the outputs of all layers. Defaults to False.\n",
    "            process_intermediates: If True, passes the layer outputs through the bbox_mlp too. Requires\n",
    "                `return_intermediates` to be True too.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing bounding boxes, object embeddings, and layer outputs if return_intermediates is True.\n",
    "            Else, returns only the bounding boxes. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "        \"\"\"\n",
    "        # embeddings: (b, [dim], num_tokens_z, num_tokens_y, num_tokens_x, [dim])\n",
    "        # spacings: (b, 3)\n",
    "\n",
    "        embeddings = rearrange_channels(embeddings, channels_first, True)\n",
    "        # (b, dim, num_tokens_z, num_tokens_y, num_tokens_x)\n",
    "\n",
    "        embeddings = self.position_embeddings(embeddings, spacings=spacings)\n",
    "        embeddings = self.pos_drop(embeddings)\n",
    "        # (b, dim, num_tokens_z, num_tokens_y, num_tokens_x)\n",
    "\n",
    "        embeddings = rearrange(embeddings, \"b d z y x -> b (z y x) d\")\n",
    "        # (b, num_embed_tokens, dim)\n",
    "\n",
    "        object_queries = repeat(self.object_queries, \"1 n d -> b n d\", b=embeddings.shape[0])\n",
    "        # (b, num_possible_objects, dim)\n",
    "\n",
    "        object_embeddings, layer_outputs = self.decoder(object_queries, embeddings, return_intermediates=True)\n",
    "        # object_embeddings: (b, num_possible_objects, dim)\n",
    "        # layer_outputs: list of (b, num_possible_objects, dim)\n",
    "\n",
    "        bboxes = self.bbox_mlp(object_embeddings)\n",
    "        # (b, num_possible_objects, 6 + 1 + num_classes)\n",
    "\n",
    "        if return_intermediates:\n",
    "            if process_intermediates:\n",
    "                for i in range(len(layer_outputs)):\n",
    "                    layer_outputs[i] = self.bbox_mlp(layer_outputs[i])\n",
    "            return bboxes, object_embeddings, layer_outputs\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level4(self._forward, *args, **kwargs)\n",
    "\n",
    "    def bipartite_matching_loss(\n",
    "        self,\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor | list[torch.Tensor],\n",
    "        intermediate_preds: list[torch.Tensor] | None = None,\n",
    "        classification_cost_weight: float = 1.0,\n",
    "        bbox_l1_cost_weight: float = 1.0,\n",
    "        bbox_giou_cost_weight: float = 1.0,\n",
    "        matched_indices: tuple[list[int], list[int]] | None = None,\n",
    "        update_class_prevalences: bool = True,\n",
    "        reduction: str = \"mean\",\n",
    "        return_matching: bool = False,\n",
    "        return_loss_components: bool = False,\n",
    "    ) -> (\n",
    "        torch.Tensor  # just loss\n",
    "        | list[torch.Tensor]  # main loss plus auxiliary losses\n",
    "        | tuple[torch.Tensor, tuple]  # loss with matching\n",
    "        | tuple[list[torch.Tensor], tuple]  # main+auxiliary losses with matching\n",
    "        | tuple[torch.Tensor, dict[str, torch.Tensor]]  # loss with loss components\n",
    "        | tuple[list[torch.Tensor], list[dict[str, torch.Tensor]]]  # main+auxiliary losses with loss components\n",
    "        | tuple[torch.Tensor, tuple, dict[str, torch.Tensor]]  # loss, matching, loss components\n",
    "        | tuple[list[torch.Tensor], tuple, list[dict[str, torch.Tensor]]]  # all losses, matching, all loss components\n",
    "    ):\n",
    "        \"\"\"Bipartite matching loss for DETR. The classes are expected to optimize for a multi-class classification\n",
    "        problem. Expects raw logits in class predictions, not probabilities. Use ``logits_to_scores_fn=None`` in the\n",
    "        ``forward`` function to avoid applying any transformation.\n",
    "\n",
    "        Args:\n",
    "            pred: Predicted bounding boxes and class scores. It should be of shape\n",
    "                `(B, num_objects, 6 + 1 + num_classes)`. Number of objects and number of classes will be inferred from\n",
    "                here.\n",
    "            target: Target bounding boxes and class scores. If provided as a list, each element should be a tensor for\n",
    "                the corresponding batch element in ``pred`` and therefore should have a length of `B`. Each tensor\n",
    "                should have less than or equal to the number of objects in `pred`. The number of classes can either be\n",
    "                the exact same as in `pred`, or it should be 1 argmax (one-cold) decoding.\n",
    "            intermediate_preds: A list of any intermediate decoder outputs to use for auxiliary losses. If provided,\n",
    "                returned loss values become a list instead of single values. The matching used is the one used for the\n",
    "                final prediction i.e. `pred`.\n",
    "            classification_cost_weight: Weight for the classification cost in hungarian matching. Only used when\n",
    "                matched_indices is None.\n",
    "            bbox_l1_cost_weight: Weight for the bounding box L1 loss cost in hungarian matching. Only used when\n",
    "                matched_indices is None.\n",
    "            bbox_giou_cost_weight: Weight for the bounding box IoU cost in hungarian matching. Only used when\n",
    "                matched_indices is None.\n",
    "            matched_indices: Hungarian matching is done only if this is None, else this is used to match pred and target\n",
    "                boxes. Useful when calculating auxiliary losses with intermediate layers of DETR3D.\n",
    "            update_class_prevalences: Whether or not to update class prevalences in the class balanced cross entropy\n",
    "                loss. Useful when calculating auxiliary losses with intermediate layers of DETR3D.\n",
    "            reduction: Specifies the reduction to apply to the output.\n",
    "            return_matching: Whether or not to return the matched indices from the bipartite matching.\n",
    "            return_loss_components: Whether or not to return the individual loss components.\n",
    "\n",
    "        Returns:\n",
    "            The first element is always the main loss that has been calculated. If no `intermediate_preds` were\n",
    "            provided, a tensor containing the bipartite matching loss with the shape depending on the `reduction`\n",
    "            argument is present, else it is a list of the same for all preds provided with the first one being the main\n",
    "            pred.\n",
    "\n",
    "            If `return_matching` is True, also returns a list of tuples containing matched indices for predictions and\n",
    "            targets. Each tuple is of the form `(pred_indices, target_indices)`, where `pred_indices` and\n",
    "            `target_indices` are lists of indices for the matched predictions and targets, respectively.\n",
    "\n",
    "            If `return_loss_components` is True, also returns a dict of each loss component reduced based on\n",
    "            `reduction`. If `intermediate_preds` were also provided, it is a list of the same.\n",
    "        \"\"\"\n",
    "        B = pred.shape[0]\n",
    "\n",
    "        # Convert target to a list of tensors if not already\n",
    "        if isinstance(target, torch.Tensor):\n",
    "            target = list(target)\n",
    "\n",
    "        # argmax encode the class labels if they are not already\n",
    "        for i in range(len(target)):\n",
    "            if target[i].shape[-1] > 7:  # 6 bbox + 1 class\n",
    "                target[i] = torch.cat([target[i][:, :6], target[i][:, 6:].argmax(-1, keepdims=True)], dim=-1)\n",
    "\n",
    "        # Perform hungarian matching\n",
    "        if matched_indices is None:\n",
    "            matched_indices = self.hungarian_matching(\n",
    "                pred, target, classification_cost_weight, bbox_l1_cost_weight, bbox_giou_cost_weight\n",
    "            )\n",
    "\n",
    "        # Update class prevalences in class balanced cross entropy loss\n",
    "        if update_class_prevalences:\n",
    "            self._update_class_prevalences(target)\n",
    "\n",
    "        # Get a combined list of all preds\n",
    "        all_preds = [pred]\n",
    "        if intermediate_preds is not None:\n",
    "            all_preds += intermediate_preds\n",
    "\n",
    "        # Calculate losses for all preds\n",
    "        loss = []\n",
    "        loss_components = []\n",
    "        for pred in all_preds:  # pred is overwritten here\n",
    "            batch_loss_components = {\n",
    "                \"classification_loss\": [],\n",
    "                \"bbox_l1_loss\": [],\n",
    "                \"bbox_giou_loss\": [],\n",
    "                \"total_loss\": [],\n",
    "            }\n",
    "            for i in range(B):\n",
    "                batch_losses = {}\n",
    "\n",
    "                pred_indices, target_indices = matched_indices[i]\n",
    "\n",
    "                # Classification loss for ALL predictions\n",
    "                all_class_labels = torch.zeros_like(pred[i][:, 6], dtype=torch.long)\n",
    "                all_class_labels[pred_indices] = target[i][target_indices, 6].long()\n",
    "                all_pred_classes = pred[i][..., 6:]\n",
    "                if self.class_balanced_cross_entropy_loss is None:\n",
    "                    batch_losses[\"classification_loss\"] = F.cross_entropy(all_pred_classes, all_class_labels)\n",
    "                else:\n",
    "                    batch_losses[\"classification_loss\"] = self.class_balanced_cross_entropy_loss(\n",
    "                        all_pred_classes, all_class_labels, update_class_prevalences=False\n",
    "                    )\n",
    "\n",
    "                # Calculate all other losses only if prediction and target have objects\n",
    "                if pred_indices != [] and target_indices != []:\n",
    "                    matched_pred = pred[i][pred_indices]\n",
    "                    matched_target = target[i][target_indices]\n",
    "\n",
    "                    pred_bboxes = matched_pred[:, :6].clone()\n",
    "                    target_bboxes = matched_target[:, :6].clone()\n",
    "\n",
    "                    # Compute losses for matched pairs\n",
    "                    # BBox L1 loss\n",
    "                    batch_losses[\"bbox_l1_loss\"] = F.l1_loss(pred_bboxes, target_bboxes)\n",
    "\n",
    "                    # For IOU calculation, it does not matter if bboxes are in actual pixel lengths or normalized based on\n",
    "                    # image size, metric value will be the same.\n",
    "\n",
    "                    # BBox IOU loss\n",
    "                    batch_losses[\"bbox_giou_loss\"] = 1 - DETR3D.generalized_bbox_iou(pred_bboxes, target_bboxes).mean()\n",
    "\n",
    "                # Total loss for this batch element\n",
    "                total_loss = (\n",
    "                    classification_cost_weight * batch_losses.get(\"classification_loss\", 0.0)\n",
    "                    + bbox_l1_cost_weight * batch_losses.get(\"bbox_l1_loss\", 0.0)\n",
    "                    + bbox_giou_cost_weight * batch_losses.get(\"bbox_giou_loss\", 0.0)\n",
    "                )\n",
    "\n",
    "                # Add to losses\n",
    "                batch_loss_components[\"classification_loss\"].append(\n",
    "                    batch_losses.get(\"classification_loss\", torch.tensor(torch.nan, device=pred.device)),\n",
    "                )\n",
    "                batch_loss_components[\"bbox_l1_loss\"].append(\n",
    "                    batch_losses.get(\"bbox_l1_loss\", torch.tensor(torch.nan, device=pred.device)),\n",
    "                )\n",
    "                batch_loss_components[\"bbox_giou_loss\"].append(\n",
    "                    batch_losses.get(\"bbox_giou_loss\", torch.tensor(torch.nan, device=pred.device)),\n",
    "                )\n",
    "                batch_loss_components[\"total_loss\"].append(total_loss)\n",
    "\n",
    "            # Stack batch losses and apply reduction\n",
    "            for key in batch_loss_components:\n",
    "                batch_loss_component = torch.stack(batch_loss_components[key])\n",
    "\n",
    "                if reduction == \"mean\":\n",
    "                    batch_loss_component = batch_loss_component.nanmean()\n",
    "                elif reduction == \"sum\":\n",
    "                    batch_loss_component = batch_loss_component.nansum()\n",
    "                elif reduction == \"none\" or reduction is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid reduction mode: {reduction}\")\n",
    "\n",
    "                batch_loss_components[key] = batch_loss_component\n",
    "\n",
    "            loss.append(batch_loss_components.pop(\"total_loss\"))\n",
    "            loss_components.append(batch_loss_components)\n",
    "\n",
    "        # Finalize return value\n",
    "        if len(loss) == 1:\n",
    "            loss = loss[0]\n",
    "            loss_components = loss_components[0]\n",
    "\n",
    "        return_value = [loss]\n",
    "        if return_matching:\n",
    "            return_value.append(matched_indices)\n",
    "        if return_loss_components:\n",
    "            return_value.append(loss_components)\n",
    "\n",
    "        if len(return_value) == 1:\n",
    "            return_value = return_value[0]\n",
    "        else:\n",
    "            return_value = tuple(return_value)\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def hungarian_matching(\n",
    "        self,\n",
    "        pred: torch.Tensor,\n",
    "        target: list[torch.Tensor],\n",
    "        classification_cost_weight: float = 1.0,\n",
    "        bbox_l1_cost_weight: float = 1.0,\n",
    "        bbox_giou_cost_weight: float = 1.0,\n",
    "    ) -> list[tuple[list[int], list[int]]]:\n",
    "        \"\"\"Hungarian matching between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            pred: Predicted bounding boxes and class scores. It should be of shape\n",
    "                `(B, num_objects, 6 + 1 + num_classes)`. Number of objects and number of classes will be inferred from\n",
    "                here.\n",
    "            target: Target bounding boxes and class scores. This is expected in argmax encoding or one-hot encoding.\n",
    "            classification_cost_weight: Weight for the classification cost.\n",
    "            bbox_l1_cost_weight: Weight for the bounding box L1 loss cost.\n",
    "            bbox_giou_cost_weight: Weight for the bounding box IoU cost.\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples containing matched indices for predictions and targets. Each tuple is of the form\n",
    "            `(pred_indices, target_indices)`, where `pred_indices` and `target_indices` are lists of indices for the\n",
    "            matched predictions and targets, respectively.\n",
    "        \"\"\"\n",
    "        B = pred.shape[0]\n",
    "\n",
    "        matched_indices = []\n",
    "        for i in range(B):\n",
    "            pred_bboxes = pred[i, :, :6]  # (num_objects, 6)\n",
    "            target_bboxes = target[i][:, :6]  # (<=num_objects, 6)\n",
    "\n",
    "            # If either prediction or target has no objects, skip matching\n",
    "            if pred_bboxes.shape[0] == 0 or target_bboxes.shape[0] == 0:\n",
    "                matched_indices.append(([], []))\n",
    "                continue\n",
    "\n",
    "            pred_class_logits = pred[i, :, 6:]  # (num_objects, num_classes)\n",
    "            target_class_labels = target[i][:, 6].long()  # (<=num_objects,) this is in argmax encoding\n",
    "\n",
    "            # ----- Cost matrix calculation -----\n",
    "\n",
    "            # Classification cost\n",
    "            if self.config.classification_cost_fn == \"softmax\":\n",
    "                pred_class_probabilities = F.softmax(pred_class_logits, dim=-1)\n",
    "            elif self.config.classification_cost_fn == \"log_softmax\":\n",
    "                pred_class_probabilities = F.log_softmax(pred_class_logits, dim=-1)\n",
    "            # (num_objects, num_classes)\n",
    "\n",
    "            classification_cost = -pred_class_probabilities[:, target_class_labels]\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # L1 loss for bounding boxes\n",
    "            bbox_l1_cost = torch.cdist(pred_bboxes, target_bboxes, p=1)\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # IOU cost for bounding boxes\n",
    "            bbox_giou_cost = 1 - DETR3D.generalized_pairwise_bbox_iou(pred_bboxes, target_bboxes)\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # Total cost matrix\n",
    "            cost_matrix = (\n",
    "                classification_cost_weight * classification_cost\n",
    "                + bbox_l1_cost_weight * bbox_l1_cost\n",
    "                + bbox_giou_cost_weight * bbox_giou_cost\n",
    "            )\n",
    "            # (num_objects, <=num_objects)\n",
    "\n",
    "            # Hungarian matching\n",
    "            pred_indices_element, target_indices_element = linear_sum_assignment(cost_matrix.detach().cpu().numpy())\n",
    "\n",
    "            matched_indices.append((list(pred_indices_element), list(target_indices_element)))\n",
    "\n",
    "        return matched_indices\n",
    "\n",
    "    @staticmethod\n",
    "    @populate_docstring\n",
    "    def bbox_iou(\n",
    "        pred_bboxes: torch.Tensor,\n",
    "        target_bboxes: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the IoU between two matched sets of bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            pred_bboxes: Predicted bounding boxes of shape `(num_boxes, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "            target_bboxes: Target bounding boxes of shape `(num_boxes, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the IoU.\n",
    "        \"\"\"\n",
    "        pred_corners = bbox_zyxdhw_to_z1z2y1y2x1x2(pred_bboxes)\n",
    "        target_corners = bbox_zyxdhw_to_z1z2y1y2x1x2(target_bboxes)\n",
    "\n",
    "        # Intersection corners\n",
    "        max_min = torch.max(pred_corners[:, :3], target_corners[:, :3])\n",
    "        min_max = torch.min(pred_corners[:, 3:], target_corners[:, 3:])\n",
    "        inter_dims = (min_max - max_min).clamp(min=0)\n",
    "        inter_vol = inter_dims.prod(dim=1)\n",
    "\n",
    "        # Volumes\n",
    "        pred_dims = pred_corners[:, 3:] - pred_corners[:, :3]\n",
    "        target_dims = target_corners[:, 3:] - target_corners[:, :3]\n",
    "        pred_vol = pred_dims.prod(dim=1)\n",
    "        target_vol = target_dims.prod(dim=1)\n",
    "        union_vol = pred_vol + target_vol - inter_vol\n",
    "\n",
    "        # IOU\n",
    "        iou = inter_vol / union_vol.clamp(min=1e-7)\n",
    "\n",
    "        return iou\n",
    "\n",
    "    @staticmethod\n",
    "    @populate_docstring\n",
    "    def generalized_bbox_iou(\n",
    "        pred_bboxes: torch.Tensor,\n",
    "        target_bboxes: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the IoU loss between two matched sets of bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            pred_bboxes: Predicted bounding boxes of shape `(num_boxes, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "            target_bboxes: Target bounding boxes of shape `(num_boxes, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the IoU loss.\n",
    "        \"\"\"\n",
    "        pred_corners = bbox_zyxdhw_to_z1z2y1y2x1x2(pred_bboxes)\n",
    "        target_corners = bbox_zyxdhw_to_z1z2y1y2x1x2(target_bboxes)\n",
    "\n",
    "        # Intersection corners\n",
    "        max_min = torch.max(pred_corners[:, :3], target_corners[:, :3])\n",
    "        min_max = torch.min(pred_corners[:, 3:], target_corners[:, 3:])\n",
    "        inter_dims = (min_max - max_min).clamp(min=0)\n",
    "        inter_vol = inter_dims.prod(dim=1)\n",
    "\n",
    "        # Volumes\n",
    "        pred_dims = pred_corners[:, 3:] - pred_corners[:, :3]\n",
    "        target_dims = target_corners[:, 3:] - target_corners[:, :3]\n",
    "        pred_vol = pred_dims.prod(dim=1)\n",
    "        target_vol = target_dims.prod(dim=1)\n",
    "        union_vol = pred_vol + target_vol - inter_vol\n",
    "\n",
    "        # IOU\n",
    "        iou = inter_vol / union_vol.clamp(min=1e-7)\n",
    "        # Enclosing box corners\n",
    "        enc_min = torch.min(pred_corners[:, :3], target_corners[:, :3])\n",
    "        enc_max = torch.max(pred_corners[:, 3:], target_corners[:, 3:])\n",
    "        enc_dims = (enc_max - enc_min).clamp(min=0)\n",
    "        enc_vol = enc_dims.prod(dim=1)\n",
    "\n",
    "        gious = iou - (enc_vol - union_vol) / enc_vol.clamp(min=1e-7)\n",
    "\n",
    "        return gious\n",
    "\n",
    "    @staticmethod\n",
    "    @populate_docstring\n",
    "    def generalized_pairwise_bbox_iou(\n",
    "        pred_bboxes: torch.Tensor,\n",
    "        target_bboxes: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the IoU loss between all combinations of predicted and target bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            pred_bboxes: Predicted bounding boxes of shape `(num_objects, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "            target_bboxes: Target bounding boxes of shape `(<=num_objects, 6)`. {BOUNDING_BOXES_FORMAT_DOC}\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing the IoU losses of all combinations.\n",
    "        \"\"\"\n",
    "        # Compute pairwise IoU\n",
    "        gious = []\n",
    "        for i in range(pred_bboxes.shape[0]):\n",
    "            row_gious = []\n",
    "            for j in range(target_bboxes.shape[0]):\n",
    "                row_gious.append(DETR3D.generalized_bbox_iou(pred_bboxes[i : i + 1], target_bboxes[j : j + 1]).mean())\n",
    "\n",
    "            gious.append(torch.stack(row_gious))\n",
    "\n",
    "        return torch.stack(gious)\n",
    "\n",
    "    def _update_class_prevalences(self, target: list[torch.Tensor]):\n",
    "        \"\"\"Update the class prevalences based on the classes present in the ground truth\n",
    "\n",
    "        Args:\n",
    "            target: A list of ground truth tensors, each of shape (num_objects, 7) where the last dimension contains the\n",
    "                bounding box class.\n",
    "        \"\"\"\n",
    "        if self.class_balanced_cross_entropy_loss is not None:\n",
    "            # Note that target classes will not be in order here. But since counts are all that matters, this doesn't\n",
    "            # affect the results\n",
    "            target_classes = pad_sequence(target, batch_first=True)[..., -1].long()\n",
    "            if target_classes.shape[-1] > self.config.num_objects:\n",
    "                # If there are more than self.config.num_objects targets, only the first self.config.num_objects will be\n",
    "                # considered\n",
    "                target_classes = target_classes[..., : self.config.num_objects]\n",
    "            elif target_classes.shape[-1] < self.config.num_objects:\n",
    "                # If there are less than self.config.num_objects targets, pad with background class\n",
    "                target_classes = F.pad(target_classes, (0, self.config.num_objects - target_classes.shape[-1]), value=0)\n",
    "            self.class_balanced_cross_entropy_loss.update_class_prevalences(target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETR3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpos_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdecoder\u001b[1m)\u001b[0m: \u001b[1;35mDETRDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mbbox_mlp\u001b[1m)\u001b[0m: \u001b[1;35mDETRBBoxMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mbbox_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mclass_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4.6000\u001b[0m, \u001b[1;36m2.9489\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m2.7217\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.6751\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2963\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.2389\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_giou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.5820\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.0349\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3.9040\u001b[0m, \u001b[1;36m3.4109\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.9519\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.9546\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2921\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.2405\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_giou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.6601\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.2157\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2.1474\u001b[0m, \u001b[1;36m1.8935\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m2.1474\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.8935\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'bbox_l1_loss'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'bbox_giou_loss'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"patch_size\": (8, 16, 16),\n",
    "    \"in_channels\": 1,\n",
    "    \"dim\": 54,\n",
    "    \"num_heads\": 6,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"attn_drop_prob\": 0.2,\n",
    "    \"proj_drop_prob\": 0.2,\n",
    "    \"mlp_drop_prob\": 0.2,\n",
    "    \"learnable_absolute_position_embeddings\": True,\n",
    "    \"embed_spacing_info\": False,\n",
    "    \"image_size\": (32, 512, 512),\n",
    "    \"num_objects\": 10,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_layers\": 4,\n",
    "}\n",
    "\n",
    "test = DETR3D(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 1, 4, 32, 32),\n",
    "    torch.randn(2, 3),\n",
    "    return_intermediates=True,\n",
    ")\n",
    "display((o[0].shape, o[1].shape, [x.shape for x in o[2]]))\n",
    "\n",
    "for gt_bboxes in [\n",
    "    [\n",
    "        torch.cat([torch.rand(10, 6), torch.randint(0, 6, (10, 1))], dim=-1),\n",
    "        torch.cat([torch.rand(2, 6), torch.randint(0, 6, (2, 1))], dim=-1),\n",
    "    ],  # Regular testing\n",
    "    [\n",
    "        torch.rand(10, 12),\n",
    "        torch.rand(2, 12),\n",
    "    ],  # Requiring argmax encoding\n",
    "    [\n",
    "        torch.tensor([]).reshape(0, 12),\n",
    "        torch.tensor([]).reshape(0, 7),\n",
    "    ],  # No ground truth boxes, classification loss should still be calculated\n",
    "]:\n",
    "    display(test.bipartite_matching_loss(o[0], gt_bboxes, reduction=\"none\", return_loss_components=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETR3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpos_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdecoder\u001b[1m)\u001b[0m: \u001b[1;35mDETRDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mbbox_mlp\u001b[1m)\u001b[0m: \u001b[1;35mDETRBBoxMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mbbox_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mclass_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mclass_balanced_cross_entropy_loss\u001b[1m)\u001b[0m: \u001b[1;35mClassBalancedCrossEntropyLoss\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/projects/vision_architectures/vision_architectures/losses/class_balanced_cross_entropy_loss.py:152: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  mu, std = weights.nanmean(), weights[~weights.isnan()].std()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2.9890\u001b[0m, \u001b[1;36m3.4998\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m2.9890\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m3.4998\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'bbox_l1_loss'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'bbox_giou_loss'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4.4833\u001b[0m, \u001b[1;36m4.3253\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m2.8460\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m2.5242\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2587\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.3132\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_giou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.3786\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.4879\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.0954\u001b[0m, \u001b[1;36m1.8986\u001b[0m, \u001b[1;36m1.8986\u001b[0m, \u001b[1;36m0.4746\u001b[0m, \u001b[1;36m0.6329\u001b[0m, \u001b[1;36m1.0000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4.9778\u001b[0m, \u001b[1;36m5.5608\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m3.2387\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m3.8991\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2993\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.2937\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[32m'bbox_giou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.4398\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.3680\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.0968\u001b[0m, \u001b[1;36m1.8988\u001b[0m, \u001b[1;36m1.8802\u001b[0m, \u001b[1;36m0.4807\u001b[0m, \u001b[1;36m0.6436\u001b[0m, \u001b[1;36m1.0000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"patch_size\": (8, 16, 16),\n",
    "    \"in_channels\": 1,\n",
    "    \"dim\": 54,\n",
    "    \"num_heads\": 6,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"attn_drop_prob\": 0.2,\n",
    "    \"proj_drop_prob\": 0.2,\n",
    "    \"mlp_drop_prob\": 0.2,\n",
    "    \"learnable_absolute_position_embeddings\": True,\n",
    "    \"embed_spacing_info\": False,\n",
    "    \"image_size\": (32, 512, 512),\n",
    "    \"num_objects\": 10,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_layers\": 4,\n",
    "    \"classification_loss_fn\": \"class_balanced_cross_entropy\",\n",
    "}\n",
    "\n",
    "test = DETR3D(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 1, 4, 32, 32),\n",
    "    torch.randn(2, 3),\n",
    "    return_intermediates=True,\n",
    ")\n",
    "display((o[0].shape, o[1].shape, [x.shape for x in o[2]]))\n",
    "\n",
    "for gt_bboxes in [\n",
    "    [\n",
    "        torch.tensor([]).reshape(0, 12),\n",
    "        torch.tensor([]).reshape(0, 7),\n",
    "    ],  # No ground truth boxes, classification loss should still be calculated\n",
    "    [\n",
    "        torch.cat([torch.rand(10, 6), torch.randint(0, 6, (10, 1))], dim=-1),\n",
    "        torch.cat([torch.rand(2, 6), torch.randint(0, 6, (2, 1))], dim=-1),\n",
    "    ],  # Regular testing\n",
    "    [\n",
    "        torch.rand(10, 12),\n",
    "        torch.rand(2, 12),\n",
    "    ],  # Requiring argmax encoding\n",
    "]:\n",
    "    display(\n",
    "        test.bipartite_matching_loss(o[0], gt_bboxes, reduction=\"none\", return_loss_components=True),\n",
    "        test.class_balanced_cross_entropy_loss.get_class_weights(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDETR3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpos_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdecoder\u001b[1m)\u001b[0m: \u001b[1;35mDETRDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mbbox_mlp\u001b[1m)\u001b[0m: \u001b[1;35mDETRBBoxMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mbbox_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mclass_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mclass_balanced_cross_entropy_loss\u001b[1m)\u001b[0m: \u001b[1;35mClassBalancedCrossEntropyLoss\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level4\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m5.0699\u001b[0m, \u001b[1;36m3.5974\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mStackBackward0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m4.3932\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m3.4228\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m5.0699\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m3.5974\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m[\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m3.1020\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m2.0411\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2642\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.1645\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'bbox_giou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.7037\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.3918\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m2.5019\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.4951\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2550\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.2248\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'bbox_giou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.6363\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.7029\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'classification_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m3.1020\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m2.0411\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'bbox_l1_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0.2642\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m0.1645\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[32m'bbox_giou_loss'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1.7037\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m1.3918\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<StackBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"patch_size\": (8, 16, 16),\n",
    "    \"in_channels\": 1,\n",
    "    \"dim\": 54,\n",
    "    \"num_heads\": 6,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"attn_drop_prob\": 0.2,\n",
    "    \"proj_drop_prob\": 0.2,\n",
    "    \"mlp_drop_prob\": 0.2,\n",
    "    \"learnable_absolute_position_embeddings\": True,\n",
    "    \"embed_spacing_info\": False,\n",
    "    \"image_size\": (32, 512, 512),\n",
    "    \"num_objects\": 10,\n",
    "    \"num_classes\": 5,\n",
    "    \"num_layers\": 4,\n",
    "    \"classification_loss_fn\": \"class_balanced_cross_entropy\",\n",
    "}\n",
    "\n",
    "test = DETR3D(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 1, 4, 32, 32),\n",
    "    torch.randn(2, 3),\n",
    "    return_intermediates=True,\n",
    ")\n",
    "display((o[0].shape, o[1].shape, [x.shape for x in o[2]]))\n",
    "\n",
    "gt_bboxes = [\n",
    "    torch.cat([torch.rand(10, 6), torch.randint(0, 6, (10, 1))], dim=-1),\n",
    "    torch.cat([torch.rand(2, 6), torch.randint(0, 6, (2, 1))], dim=-1),\n",
    "]\n",
    "display(test.bipartite_matching_loss(o[0], gt_bboxes, o[2][-2:], reduction=\"none\", return_loss_components=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
