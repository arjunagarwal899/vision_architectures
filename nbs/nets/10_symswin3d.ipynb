{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/symswin_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture is only an idea. It builds on SwinV2, however, instead of applying windowed attention in a window of consecutive tokens, it applies attention on symmetrically opposite tokens (along the x-axis). This was inspired from medical radiology imaging, particularly HeadCTs, that rely heavily on symmetry in the left and right hemispheres to determine abnormality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's say this is the original input:\n",
    "\n",
    "a1 b1 c1 d1 d2 c2 b2 a2\n",
    "e1 f1 g1 h1 h2 g2 f2 e2\n",
    "i1 j1 k1 l1 l2 k2 j2 i2\n",
    "m1 n1 o1 p1 p2 o2 n2 m2\n",
    "q1 r1 s1 t1 t2 s2 r2 q2\n",
    "u1 v1 w1 x1 x2 w2 v2 u2\n",
    "y1 z1 A1 B1 B2 A2 z2 y2\n",
    "C1 D1 E1 F1 F2 E2 D2 C2\n",
    "\n",
    "with a window size of (4, 4)\n",
    "\n",
    "---\n",
    "\n",
    "There are two options:\n",
    "\n",
    "---\n",
    "\n",
    "Option A:\n",
    "\n",
    "Rearranging it like this:\n",
    "\n",
    "a1 b1 b2 a2 c1 d1 d2 c2\n",
    "e1 f1 f2 e2 g1 h1 h2 g2\n",
    "i1 j1 j2 i2 k1 l1 l2 k2\n",
    "m1 n1 n2 m2 o1 p1 p2 o2\n",
    "q1 r1 r2 q2 s1 t1 t2 s2\n",
    "u1 v1 v2 u2 w1 x1 x2 w2\n",
    "y1 z1 z2 y2 A1 B1 B2 A2\n",
    "C1 D1 D2 C2 E1 F1 F2 E2\n",
    "\n",
    "When using this, the window attention works as expected:\n",
    "a1 b1 b2 a2    c1 d1 d2 c2\n",
    "e1 f1 f2 e2    g1 h1 h2 g2\n",
    "i1 j1 j2 i2    k1 l1 l2 k2\n",
    "m1 n1 n2 m2    o1 p1 p2 o2\n",
    "\n",
    "q1 r1 r2 q2    s1 t1 t2 s2\n",
    "u1 v1 v2 u2    w1 x1 x2 w2\n",
    "y1 z1 z2 y2    A1 B1 B2 A2\n",
    "C1 D1 D2 C2    E1 F1 F2 E2\n",
    "\n",
    "But it doesn't work well with shifted window attention:\n",
    "a1 b1    b2 a2 c1 d1    d2 c2\n",
    "e1 f1    f2 e2 g1 h1    h2 g2\n",
    "\n",
    "i1 j1    j2 i2 k1 l1    l2 k2\n",
    "m1 n1    n2 m2 o1 p1    p2 o2\n",
    "q1 r1    r2 q2 s1 t1    t2 s2\n",
    "u1 v1    v2 u2 w1 x1    x2 w2\n",
    "\n",
    "y1 z1    z2 y2 A1 B1    B2 A2\n",
    "C1 D1    D2 C2 E1 F1    F2 E2\n",
    "\n",
    "This causes every \"column\" of patches to be grouped together based on window_size (here 2). It is better to have each\n",
    "column to be independent of it's neighbours and for it to attend with patches on one side in the windowed layer and\n",
    "the other side in the shifted windowed layer.\n",
    "\n",
    "---\n",
    "\n",
    "Option B:\n",
    "\n",
    "Rearranging it like this:\n",
    "\n",
    "a1 a2 b1 b2 c1 c2 d1 d2\n",
    "e1 e2 f1 f2 g1 g2 h1 h2\n",
    "i1 i2 j1 j2 k1 k2 l1 l2\n",
    "m1 m2 n1 n2 o1 o2 p1 p2\n",
    "q1 q2 r1 r2 s1 s2 t1 t2\n",
    "u1 u2 v1 v2 w1 w2 x1 x2\n",
    "y1 y2 z1 z2 A1 A2 B1 B2\n",
    "C1 C2 D1 D2 E1 E2 F1 F2\n",
    "\n",
    "When using this, the window attention still works with the same patches:\n",
    "a1 a2 b1 b2    c1 c2 d1 d2\n",
    "e1 e2 f1 f2    g1 g2 h1 h2\n",
    "i1 i2 j1 j2    k1 k2 l1 l2\n",
    "m1 m2 n1 n2    o1 o2 p1 p2\n",
    "\n",
    "q1 q2 r1 r2    s1 s2 t1 t2\n",
    "u1 u2 v1 v2    w1 w2 x1 x2\n",
    "y1 y2 z1 z2    A1 A2 B1 B2\n",
    "C1 C2 D1 D2    E1 E2 F1 F2\n",
    "\n",
    "And it also works well with shifted window attention:\n",
    "a1 a2    b1 b2 c1 c2    d1 d2\n",
    "e1 e2    f1 f2 g1 g2    h1 h2\n",
    "\n",
    "i1 i2    j1 j2 k1 k2    l1 l2\n",
    "m1 m2    n1 n2 o1 o2    p1 p2\n",
    "q1 q2    r1 r2 s1 s2    t1 t2\n",
    "u1 u2    v1 v2 w1 w2    x1 x2\n",
    "\n",
    "y1 y2    z1 z2 A1 A2    B1 B2\n",
    "C1 C2    D1 D2 E1 E2    F1 F2\n",
    "\n",
    "Now we can see that the shifted windows make much more sense. Every column first looks at the windows on one side, and\n",
    "then on the other side. For eg. patch j1 always attends to j2, attends to i1, i2 in windowed attention, and attends to\n",
    "k1, k2 in shifted windowed attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import torch\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is not used. We are using option B\n",
    "\n",
    "\n",
    "def symmetry_attention_rearrange_forward(hidden_states: torch.Tensor, window_size_x: int):\n",
    "    # hidden_states: (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "\n",
    "    # Flip the second half of the patches along the x axis based on window_size_x\n",
    "    first_half = hidden_states[..., : hidden_states.shape[-2] // 2, :]\n",
    "    second_half = hidden_states[..., hidden_states.shape[-2] // 2 :, :]\n",
    "    second_half_windowed = rearrange(\n",
    "        second_half,\n",
    "        \"b num_patches_z num_patches_y (num_windows_x window_size_x) dim -> b num_patches_z num_patches_y num_windows_x window_size_x dim\",\n",
    "        window_size_x=window_size_x,\n",
    "    )\n",
    "    second_half_windowed_flipped = second_half_windowed.flip(-3)\n",
    "    second_half_windowed_flipped_unwindowed = rearrange(\n",
    "        second_half_windowed_flipped,\n",
    "        \"b num_patches_z num_patches_y num_windows_x window_size_x dim -> b num_patches_z num_patches_y (num_windows_x window_size_x) dim\",\n",
    "    )\n",
    "    hidden_states = torch.cat([first_half, second_half_windowed_flipped_unwindowed], dim=-2)\n",
    "    # (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "\n",
    "    # Rearrange the patches to alternate between the two halves along the x axis\n",
    "    hidden_states = rearrange(\n",
    "        hidden_states,\n",
    "        \"b num_patches_z num_patches_y (two half_num_windowes_x window_size_x) dim -> \"\n",
    "        \"b num_patches_z num_patches_y (half_num_windowes_x two window_size_x) dim\",\n",
    "        two=2,\n",
    "        window_size_x=window_size_x,\n",
    "    )\n",
    "    # (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "def symmetry_attention_rearrange_backward(hidden_states: torch.Tensor, window_size_x: int):\n",
    "    # hidden_states: (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "\n",
    "    # Return the patches to their previous order along the x-axis\n",
    "    hidden_states = rearrange(\n",
    "        hidden_states,\n",
    "        \"b num_patches_z num_patches_y (half_num_windowes_x two window_size_x) dim -> \"\n",
    "        \"b num_patches_z num_patches_y (two half_num_windowes_x window_size_x) dim\",\n",
    "        two=2,\n",
    "        window_size_x=window_size_x,\n",
    "    )\n",
    "    # (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "\n",
    "    # Flip the second half of the patches along the x axis to return to the original state\n",
    "    first_half = hidden_states[..., : hidden_states.shape[-2] // 2, :]\n",
    "    second_half_windowed_flipped_unwindowed = hidden_states[..., hidden_states.shape[-2] // 2 :, :]\n",
    "    second_half_windowed_flipped = rearrange(\n",
    "        second_half_windowed_flipped_unwindowed,\n",
    "        \"b num_patches_z num_patches_y (num_windows_x window_size_x) dim -> b num_patches_z num_patches_y num_windows_x window_size_x dim\",\n",
    "        window_size_x=window_size_x,\n",
    "    )\n",
    "    second_half_windowed = second_half_windowed_flipped.flip(-3)\n",
    "    second_half = rearrange(\n",
    "        second_half_windowed,\n",
    "        \"b num_patches_z num_patches_y num_windows_x window_size_x dim -> b num_patches_z num_patches_y (num_windows_x window_size_x) dim\",\n",
    "    )\n",
    "    hidden_states = torch.cat([first_half, second_half], dim=-2)\n",
    "    # (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def symmetry_attention_rearrange_forward(hidden_states: torch.Tensor):\n",
    "    # hidden_states: (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "\n",
    "    # Flip the second half of the patches along the x axis\n",
    "    last_dim_length = hidden_states.shape[-2]\n",
    "    hidden_states = torch.cat(\n",
    "        [\n",
    "            hidden_states[..., : last_dim_length // 2, :],\n",
    "            hidden_states[..., last_dim_length // 2 :, :].flip(-2),\n",
    "        ],\n",
    "        dim=-2,\n",
    "    )\n",
    "    # (b, num_patches_z, num_patches_y, num_patches_x_first_half + flipped_num_patches_second_half, dim)\n",
    "\n",
    "    # Rearrange the patches to alternate between the two halves along the x axis\n",
    "    hidden_states = rearrange(\n",
    "        hidden_states,\n",
    "        \"b num_patches_z num_patches_y (two half_num_patches_x) dim -> \"\n",
    "        \"b num_patches_z num_patches_y (half_num_patches_x two) dim\",\n",
    "        two=2,\n",
    "    ).contiguous()\n",
    "    # (b, num_patches_z, num_patches_y, rearranged_num_patches_x, dim)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "def symmetry_attention_rearrange_backward(hidden_states: torch.Tensor):\n",
    "    # hidden_states: (b, num_patches_z, num_patches_y, rearranged_num_patches_x, dim)\n",
    "\n",
    "    # Return the patches to their previous order along the x-axis\n",
    "    hidden_states = rearrange(\n",
    "        hidden_states,\n",
    "        \"b num_patches_z num_patches_y (half_num_patches_x two) dim -> \"\n",
    "        \"b num_patches_z num_patches_y (two half_num_patches_x) dim\",\n",
    "        two=2,\n",
    "    ).contiguous()\n",
    "    # (b, num_patches_z, num_patches_y, num_patches_x_first_half + flipped_num_patches_second_half, dim)\n",
    "\n",
    "    # Flip the second half of the patches along the x axis to return to the original state\n",
    "    last_dim_length = hidden_states.shape[-2]\n",
    "    hidden_states = torch.cat(\n",
    "        [\n",
    "            hidden_states[..., : last_dim_length // 2, :],\n",
    "            hidden_states[..., last_dim_length // 2 :, :].flip(-2),\n",
    "        ],\n",
    "        dim=-2,\n",
    "    )\n",
    "    # (b, num_patches_z, num_patches_y, num_patches_x, dim)\n",
    "\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m2\u001b[0m,  \u001b[1;36m3\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m6\u001b[0m,  \u001b[1;36m7\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m8\u001b[0m,  \u001b[1;36m9\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m13\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m16\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m18\u001b[0m, \u001b[1;36m19\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m22\u001b[0m, \u001b[1;36m23\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m25\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m27\u001b[0m, \u001b[1;36m28\u001b[0m, \u001b[1;36m29\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m31\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m33\u001b[0m, \u001b[1;36m34\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m36\u001b[0m, \u001b[1;36m37\u001b[0m, \u001b[1;36m38\u001b[0m, \u001b[1;36m39\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m40\u001b[0m, \u001b[1;36m41\u001b[0m, \u001b[1;36m42\u001b[0m, \u001b[1;36m43\u001b[0m, \u001b[1;36m44\u001b[0m, \u001b[1;36m45\u001b[0m, \u001b[1;36m46\u001b[0m, \u001b[1;36m47\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m48\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m50\u001b[0m, \u001b[1;36m51\u001b[0m, \u001b[1;36m52\u001b[0m, \u001b[1;36m53\u001b[0m, \u001b[1;36m54\u001b[0m, \u001b[1;36m55\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m56\u001b[0m, \u001b[1;36m57\u001b[0m, \u001b[1;36m58\u001b[0m, \u001b[1;36m59\u001b[0m, \u001b[1;36m60\u001b[0m, \u001b[1;36m61\u001b[0m, \u001b[1;36m62\u001b[0m, \u001b[1;36m63\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m,  \u001b[1;36m7\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m6\u001b[0m,  \u001b[1;36m2\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m3\u001b[0m,  \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m8\u001b[0m, \u001b[1;36m15\u001b[0m,  \u001b[1;36m9\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m13\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m16\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m22\u001b[0m, \u001b[1;36m18\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m19\u001b[0m, \u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m31\u001b[0m, \u001b[1;36m25\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m29\u001b[0m, \u001b[1;36m27\u001b[0m, \u001b[1;36m28\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m39\u001b[0m, \u001b[1;36m33\u001b[0m, \u001b[1;36m38\u001b[0m, \u001b[1;36m34\u001b[0m, \u001b[1;36m37\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m36\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m40\u001b[0m, \u001b[1;36m47\u001b[0m, \u001b[1;36m41\u001b[0m, \u001b[1;36m46\u001b[0m, \u001b[1;36m42\u001b[0m, \u001b[1;36m45\u001b[0m, \u001b[1;36m43\u001b[0m, \u001b[1;36m44\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m48\u001b[0m, \u001b[1;36m55\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m54\u001b[0m, \u001b[1;36m50\u001b[0m, \u001b[1;36m53\u001b[0m, \u001b[1;36m51\u001b[0m, \u001b[1;36m52\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m56\u001b[0m, \u001b[1;36m63\u001b[0m, \u001b[1;36m57\u001b[0m, \u001b[1;36m62\u001b[0m, \u001b[1;36m58\u001b[0m, \u001b[1;36m61\u001b[0m, \u001b[1;36m59\u001b[0m, \u001b[1;36m60\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m,  \u001b[1;36m1\u001b[0m,  \u001b[1;36m2\u001b[0m,  \u001b[1;36m3\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m6\u001b[0m,  \u001b[1;36m7\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m8\u001b[0m,  \u001b[1;36m9\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m13\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m16\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m18\u001b[0m, \u001b[1;36m19\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m21\u001b[0m, \u001b[1;36m22\u001b[0m, \u001b[1;36m23\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m24\u001b[0m, \u001b[1;36m25\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m27\u001b[0m, \u001b[1;36m28\u001b[0m, \u001b[1;36m29\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m31\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m33\u001b[0m, \u001b[1;36m34\u001b[0m, \u001b[1;36m35\u001b[0m, \u001b[1;36m36\u001b[0m, \u001b[1;36m37\u001b[0m, \u001b[1;36m38\u001b[0m, \u001b[1;36m39\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m40\u001b[0m, \u001b[1;36m41\u001b[0m, \u001b[1;36m42\u001b[0m, \u001b[1;36m43\u001b[0m, \u001b[1;36m44\u001b[0m, \u001b[1;36m45\u001b[0m, \u001b[1;36m46\u001b[0m, \u001b[1;36m47\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m48\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m50\u001b[0m, \u001b[1;36m51\u001b[0m, \u001b[1;36m52\u001b[0m, \u001b[1;36m53\u001b[0m, \u001b[1;36m54\u001b[0m, \u001b[1;36m55\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m56\u001b[0m, \u001b[1;36m57\u001b[0m, \u001b[1;36m58\u001b[0m, \u001b[1;36m59\u001b[0m, \u001b[1;36m60\u001b[0m, \u001b[1;36m61\u001b[0m, \u001b[1;36m62\u001b[0m, \u001b[1;36m63\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr = torch.arange(64).reshape(1, 1, 8, 8, 1)\n",
    "\n",
    "forward_test_arr = symmetry_attention_rearrange_forward(test_arr)\n",
    "backward_forward_test_arr = symmetry_attention_rearrange_backward(forward_test_arr)\n",
    "\n",
    "assert torch.allclose(test_arr, backward_forward_test_arr)\n",
    "\n",
    "test_arr.squeeze(0, 1, -1), forward_test_arr.squeeze(0, 1, -1), backward_forward_test_arr.squeeze(0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
