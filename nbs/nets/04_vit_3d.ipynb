{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp nets/vit_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.blocks.transformer import TransformerDecoderBlock1D, TransformerEncoderBlock1D\n",
    "from vision_architectures.layers.embeddings import AbsolutePositionEmbeddings3D, PatchEmbeddings3D\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ViT3DEncoderConfig(CustomBaseModel):\n",
    "    dim: int\n",
    "    num_heads: int\n",
    "    mlp_ratio: int\n",
    "    layer_norm_eps: float\n",
    "    attn_drop_prob: float = 0.0\n",
    "    proj_drop_prob: float = 0.0\n",
    "    mlp_drop_prob: float = 0.0\n",
    "    proj_drop_prob: float = 0.0\n",
    "    norm_location: Literal[\"pre\", \"post\"] = \"pre\"\n",
    "\n",
    "    encoder_depth: int\n",
    "\n",
    "\n",
    "class ViT3DConfig(ViT3DEncoderConfig):\n",
    "    patch_size: tuple[int, int, int]\n",
    "    in_channels: int\n",
    "    num_class_tokens: int\n",
    "\n",
    "    drop_prob: float = 0.0\n",
    "\n",
    "    # For MIM\n",
    "    image_size: tuple[int, int, int] | None = None\n",
    "    mask_ratio: float | None = None\n",
    "\n",
    "\n",
    "class ViT3DDecoderConfig(CustomBaseModel):\n",
    "    dim: int\n",
    "    num_heads: int\n",
    "    mlp_ratio: int\n",
    "    layer_norm_eps: float\n",
    "    attn_drop_prob: float = 0.0\n",
    "    proj_drop_prob: float = 0.0\n",
    "    mlp_drop_prob: float = 0.0\n",
    "    proj_drop_prob: float = 0.0\n",
    "    norm_location: Literal[\"pre\", \"post\"] = \"pre\"\n",
    "\n",
    "    decoder_depth: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mViT3DConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mdim\u001b[0m=\u001b[1;36m64\u001b[0m,\n",
       "    \u001b[33mnum_heads\u001b[0m=\u001b[1;36m8\u001b[0m,\n",
       "    \u001b[33mmlp_ratio\u001b[0m=\u001b[1;36m4\u001b[0m,\n",
       "    \u001b[33mlayer_norm_eps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m,\n",
       "    \u001b[33mattn_drop_prob\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mproj_drop_prob\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mmlp_drop_prob\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mnorm_location\u001b[0m=\u001b[32m'pre'\u001b[0m,\n",
       "    \u001b[33mencoder_depth\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "    \u001b[33mpatch_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33min_channels\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "    \u001b[33mnum_class_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "    \u001b[33mdrop_prob\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mimage_size\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmask_ratio\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_config = ViT3DConfig.model_validate(\n",
    "    {\n",
    "        \"patch_size\": (2, 2, 2),\n",
    "        \"in_channels\": 3,\n",
    "        \"dim\": 64,\n",
    "        \"num_heads\": 8,\n",
    "        \"mlp_ratio\": 4,\n",
    "        \"layer_norm_eps\": 1e-6,\n",
    "        \"encoder_depth\": 3,\n",
    "        \"decoder_depth\": 3,\n",
    "        \"num_class_tokens\": 0,\n",
    "    }\n",
    ")\n",
    "test_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ViT3DEncoder(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, config: ViT3DEncoderConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerEncoderBlock1D(config.model_dump()) for _ in range(config.encoder_depth)]\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, return_intermediates: bool = False):\n",
    "        # hidden_states: (b, num_tokens, dim)\n",
    "\n",
    "        layer_outputs = []\n",
    "        for encoder_layer in self.layers:\n",
    "            embeddings = encoder_layer(embeddings)\n",
    "            # (b, num_tokens, dim)\n",
    "\n",
    "            layer_outputs.append(embeddings)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return embeddings, layer_outputs\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mViT3DEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m3\u001b[0m x \u001b[1;35mTransformerEncoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = ViT3DEncoderConfig.model_validate(\n",
    "    {\n",
    "        \"dim\": 54,\n",
    "        \"num_heads\": 6,\n",
    "        \"mlp_ratio\": 2,\n",
    "        \"layer_norm_eps\": 1e-6,\n",
    "        \"attn_drop_prob\": 0.0,\n",
    "        \"proj_drop_prob\": 0.0,\n",
    "        \"mlp_drop_prob\": 0.0,\n",
    "        \"encoder_depth\": 3,\n",
    "    }\n",
    ")\n",
    "\n",
    "test = ViT3DEncoder(test_config)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 64, 54), return_intermediates=True)\n",
    "display((o[0].shape, [x.shape for x in o[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ViT3DDecoder(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, config: ViT3DDecoderConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerDecoderBlock1D(config.model_dump()) for _ in range(config.decoder_depth)]\n",
    "        )\n",
    "\n",
    "    def forward(self, q: torch.Tensor, kv: torch.Tensor, return_intermediates: bool = False):\n",
    "        # q: (b, num_q_tokens, dim)\n",
    "        # kv: (b, num_kv_tokens, dim)\n",
    "\n",
    "        embeddings = q\n",
    "\n",
    "        layer_outputs = []\n",
    "        for decoder_layer in self.layers:\n",
    "            embeddings = decoder_layer(embeddings, kv)\n",
    "            # (b, num_q_tokens, dim)\n",
    "\n",
    "            layer_outputs.append(embeddings)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return embeddings, layer_outputs\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mViT3DDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m5\u001b[0m x \u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = ViT3DDecoderConfig.model_validate(\n",
    "    {\n",
    "        \"dim\": 54,\n",
    "        \"num_heads\": 6,\n",
    "        \"mlp_ratio\": 2,\n",
    "        \"layer_norm_eps\": 1e-6,\n",
    "        \"attn_drop_prob\": 0.0,\n",
    "        \"proj_drop_prob\": 0.0,\n",
    "        \"mlp_drop_prob\": 0.0,\n",
    "        \"decoder_depth\": 5,\n",
    "    }\n",
    ")\n",
    "\n",
    "test = ViT3DDecoder(test_config)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 64, 54), torch.randn(2, 128, 54), return_intermediates=True)\n",
    "display((o[0].shape, [x.shape for x in o[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ViT3D(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, config: ViT3DConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patchify = PatchEmbeddings3D(patch_size=config.patch_size, in_channels=config.in_channels, dim=config.dim)\n",
    "        self.absolute_position_embeddings = AbsolutePositionEmbeddings3D(dim=config.dim, learnable=False)\n",
    "        self.pos_drop = nn.Dropout(config.drop_prob)\n",
    "        self.num_class_tokens = config.num_class_tokens\n",
    "        if self.num_class_tokens > 0:\n",
    "            self.class_tokens = nn.Parameter(torch.randn(1, config.num_class_tokens, config.dim))\n",
    "        self.encoder = ViT3DEncoder(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "        spacings: torch.Tensor,\n",
    "        mask_patches: torch.Tensor = None,\n",
    "        mask_token: torch.Tensor = None,\n",
    "        return_intermediates: bool = False,\n",
    "    ):\n",
    "        # pixel_values: (b, c, z, y, x)\n",
    "        # spacings: (b, 3)\n",
    "        # mask_patches: (b, num_patches_z, num_patches_y, num_patches_x)\n",
    "        # mask_token: (1, dim, 1, 1, 1)\n",
    "\n",
    "        embeddings = self.patchify(pixel_values)\n",
    "        # (b, dim, num_patches_z, num_patches_y, num_patches_x)\n",
    "\n",
    "        if mask_patches is not None:\n",
    "            # mask_patches (binary mask): (b, num_patches_z, num_patches_y, num_patches_x)\n",
    "            # mask_token: (1, dim, 1, 1, 1)\n",
    "            mask_patches = repeat(mask_patches, \"b z y x -> b d z y x\", d=embeddings.shape[1])\n",
    "            embeddings = (embeddings * (1 - mask_patches)) + (mask_patches * mask_token)\n",
    "\n",
    "        # (b, dim, num_patches_z, num_patches_y, num_patches_x)\n",
    "        embeddings = self.absolute_position_embeddings(embeddings, spacings=spacings, device=pixel_values.device)\n",
    "        # (b, dim, num_patches_z, num_patches_y, num_patches_x)\n",
    "\n",
    "        embeddings = rearrange(embeddings, \"b e nz ny nx -> b (nz ny nx) e\").contiguous()\n",
    "        # (b, num_tokens, dim)\n",
    "\n",
    "        embeddings = self.pos_drop(embeddings)\n",
    "        # (b, num_tokens, dim)\n",
    "\n",
    "        class_tokens = None\n",
    "        if self.num_class_tokens > 0:\n",
    "            class_tokens = repeat(self.class_tokens, \"1 n d -> b n d\", b=embeddings.shape[0])\n",
    "            embeddings = torch.cat([class_tokens, embeddings], dim=1)\n",
    "            # (b, num_tokens + num_class_tokens, dim)\n",
    "\n",
    "        encoded, layer_outputs = self.encoder(embeddings, return_intermediates=True)\n",
    "        # encoded: (b, num_tokens (+ num_class_tokens), dim)\n",
    "        # layer_outputs: list of (b, num_tokens (+ 1), dim)\n",
    "\n",
    "        if self.num_class_tokens > 0:\n",
    "            class_tokens = encoded[:, : self.num_class_tokens]\n",
    "            encoded = encoded[:, self.num_class_tokens :]\n",
    "\n",
    "        if return_intermediates:\n",
    "            return encoded, class_tokens, layer_outputs\n",
    "        return encoded, class_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mViT3DModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpatchify\u001b[1m)\u001b[0m: \u001b[1;35mPatchEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mpatch_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnormalization\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mabsolute_position_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mpos_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mencoder\u001b[1m)\u001b[0m: \u001b[1;35mViT3DEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerEncoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4096\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4098\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4098\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4098\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4098\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = ViT3DConfig.model_validate(\n",
    "    {\n",
    "        \"num_class_tokens\": 2,\n",
    "        \"attn_drop_prob\": 0.2,\n",
    "        \"dim\": 768,\n",
    "        \"drop_prob\": 0.2,\n",
    "        \"embed_spacing_info\": False,\n",
    "        \"encoder_depth\": 4,\n",
    "        \"image_size\": (32, 512, 512),\n",
    "        \"in_channels\": 1,\n",
    "        \"mlp_ratio\": 2,\n",
    "        \"layer_norm_eps\": 1e-6,\n",
    "        \"mlp_drop_prob\": 0.2,\n",
    "        \"num_heads\": 4,\n",
    "        \"patch_size\": (8, 16, 16),\n",
    "        \"proj_drop_prob\": 0.2,\n",
    "    }\n",
    ")\n",
    "\n",
    "test = ViT3D(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 1, 32, 512, 512),\n",
    "    torch.randn(2, 3),\n",
    "    return_intermediates=True,\n",
    ")\n",
    "display((o[0].shape, o[1].shape, [x.shape for x in o[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Image Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ViT3DMIMDecoder(nn.Module):\n",
    "    def __init__(self, dim, image_size, in_channels, patch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        out_dim = np.prod(self.patch_size) * self.in_channels\n",
    "\n",
    "        self.decoder = nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, encodings: torch.Tensor):\n",
    "        # encodings: (b, num_tokens, dim)\n",
    "\n",
    "        decoded = self.decoder(encodings)\n",
    "        # (b, num_tokens, new_dim)\n",
    "\n",
    "        decoded = rearrange(\n",
    "            decoded,\n",
    "            \"b (nz ny nx) (c pz py px) -> b c (nz pz) (ny py) (nx px)\",\n",
    "            c=self.in_channels,\n",
    "            pz=self.patch_size[0],\n",
    "            py=self.patch_size[1],\n",
    "            px=self.patch_size[2],\n",
    "            nz=self.image_size[0] // self.patch_size[0],\n",
    "            ny=self.image_size[1] // self.patch_size[1],\n",
    "            nx=self.image_size[2] // self.patch_size[2],\n",
    "        ).contiguous()\n",
    "        # (b, c, z, y, x)\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mViT3DMIMDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdecoder\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = ViT3DMIMDecoder(768, (32, 512, 512), 1, (8, 16, 16))\n",
    "display(test)\n",
    "display(test(torch.randn(2, 4096, 768)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ViT3DMIM(nn.Module):\n",
    "    def __init__(self, config: ViT3DConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.num_class_tokens == 0, \"MIM does not support class tokens\"\n",
    "\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.in_channels = config.in_channels\n",
    "        self.mask_ratio = config.mask_ratio\n",
    "\n",
    "        self.vit = ViT3D(config)\n",
    "        self.decoder = ViT3DMIMDecoder(config.dim, config.image_size, config.in_channels, config.patch_size)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.randn(1, config.dim, 1, 1, 1))\n",
    "\n",
    "    def mask_image(self, pixel_values: torch.Tensor):\n",
    "        b = pixel_values.shape[0]\n",
    "\n",
    "        mask_ratio = self.mask_ratio\n",
    "        grid_size = tuple([size // patch for size, patch in zip(self.image_size, self.patch_size)])\n",
    "        num_tokens = np.prod(grid_size)\n",
    "        mask_patches = []\n",
    "        for _ in range(b):\n",
    "            _mask_patches = torch.zeros(num_tokens, dtype=torch.int8, device=pixel_values.device)\n",
    "            _mask_patches[: int(mask_ratio * num_tokens)] = 1\n",
    "            _mask_patches = _mask_patches[torch.randperm(num_tokens)]\n",
    "            _mask_patches = rearrange(\n",
    "                _mask_patches,\n",
    "                \"(z y x) -> z y x\",\n",
    "                z=grid_size[0],\n",
    "                y=grid_size[1],\n",
    "                x=grid_size[2],\n",
    "            ).contiguous()\n",
    "            mask_patches.append(_mask_patches)\n",
    "        mask_patches: torch.Tensor = torch.stack(mask_patches, dim=0)\n",
    "\n",
    "        return mask_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ViT3DSimMIM(ViT3DMIM, PyTorchModelHubMixin):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_fn(pred: torch.Tensor, target: torch.Tensor, reduction=\"mean\"):\n",
    "        return nn.functional.l1_loss(pred, target, reduction=reduction)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, spacings: torch.Tensor):\n",
    "        mask_patches = self.mask_image(pixel_values)\n",
    "\n",
    "        encodings, _ = self.vit(pixel_values, spacings, mask_patches, self.mask_token)\n",
    "        decoded = self.decoder(encodings)\n",
    "\n",
    "        loss = self.loss_fn(decoded, pixel_values, reduction=\"none\")\n",
    "        mask = repeat(\n",
    "            mask_patches,\n",
    "            \"b z y x -> b (z pz) (y py) (x px)\",\n",
    "            pz=self.patch_size[0],\n",
    "            py=self.patch_size[1],\n",
    "            px=self.patch_size[2],\n",
    "        )\n",
    "        loss = (loss * mask).sum() / ((mask.sum() + 1e-5) * self.in_channels)\n",
    "\n",
    "        return decoded, loss, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mViT3DSimMIM\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mvit\u001b[1m)\u001b[0m: \u001b[1;35mViT3DModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mpatchify\u001b[1m)\u001b[0m: \u001b[1;35mPatchEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mpatch_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnormalization\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mabsolute_position_embeddings\u001b[1m)\u001b[0m: \u001b[1;35mAbsolutePositionEmbeddings3D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mpos_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mencoder\u001b[1m)\u001b[0m: \u001b[1;35mViT3DEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mTransformerEncoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1536\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mcheckpointing_level\u001b[0m=\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdecoder\u001b[1m)\u001b[0m: \u001b[1;35mViT3DMIMDecoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdecoder\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2.1494\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDivBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_config = ViT3DConfig.model_validate(\n",
    "    {\n",
    "        \"num_class_tokens\": 0,\n",
    "        \"attn_drop_prob\": 0.2,\n",
    "        \"dim\": 768,\n",
    "        \"drop_prob\": 0.2,\n",
    "        \"encoder_depth\": 4,\n",
    "        \"image_size\": (32, 128, 128),\n",
    "        \"in_channels\": 1,\n",
    "        \"mlp_ratio\": 2,\n",
    "        \"layer_norm_eps\": 1e-6,\n",
    "        \"mlp_drop_prob\": 0.2,\n",
    "        \"num_heads\": 4,\n",
    "        \"patch_size\": (8, 16, 16),\n",
    "        \"proj_drop_prob\": 0.2,\n",
    "        \"mask_ratio\": 0.8,\n",
    "    }\n",
    ")\n",
    "\n",
    "test = ViT3DSimMIM(test_config)\n",
    "display(test)\n",
    "o = test(\n",
    "    torch.randn(2, 1, 32, 128, 128),\n",
    "    torch.randn(2, 3),\n",
    ")\n",
    "display((o[0].shape, o[1], o[2].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m5523076\u001b[0m, \u001b[1;36m788480\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "sample_spacings = torch.tensor([[1, 0.1, 0.1], [2, 0.2, 0.2], [3, 0.3, 0.3]])\n",
    "sample_batch = torch.rand(3, 1, 16, 128, 128)\n",
    "sample_config = ViT3DConfig.model_validate(\n",
    "    {\n",
    "        \"num_class_tokens\": 0,\n",
    "        \"attn_drop_prob\": 0.2,\n",
    "        \"dim\": 384,\n",
    "        \"drop_prob\": 0.2,\n",
    "        \"embed_spacing_info\": False,\n",
    "        \"encoder_depth\": 4,\n",
    "        \"image_size\": (16, 128, 128),\n",
    "        \"in_channels\": 1,\n",
    "        \"mlp_ratio\": 2,\n",
    "        \"layer_norm_eps\": 1e-6,\n",
    "        \"mlp_drop_prob\": 0.2,\n",
    "        \"num_heads\": 4,\n",
    "        \"patch_size\": (8, 16, 16),\n",
    "        \"proj_drop_prob\": 0.2,\n",
    "        \"mask_ratio\": 0.8,\n",
    "    }\n",
    ")\n",
    "\n",
    "model = ViT3DSimMIM(sample_config)\n",
    "\n",
    "sum(x.numel() for x in model.vit.parameters()), sum(x.numel() for x in model.decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1 sample_batch = sample_batch.cuda()                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>sample_spacings = sample_spacings.cuda()                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>model = model.cuda()                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA error: out of memory\n",
       "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing <span style=\"color: #808000; text-decoration-color: #808000\">CUDA_LAUNCH_BLOCKING</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1 sample_batch = sample_batch.cuda()                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2 \u001b[0msample_spacings = sample_spacings.cuda()                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3 \u001b[0mmodel = model.cuda()                                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA error: out of memory\n",
       "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing \u001b[33mCUDA_LAUNCH_BLOCKING\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "sample_spacings = sample_spacings.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5240ab8f6d40f989644e069741125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.595103\tLR: 0.500000\n",
      "Loss: 2.010396\tLR: 0.500000\n",
      "Loss: 1.866450\tLR: 0.500000\n",
      "Loss: 1.758274\tLR: 0.500000\n",
      "Loss: 1.708617\tLR: 0.500000\n",
      "Loss: 1.616564\tLR: 0.450000\n",
      "Loss: 1.497164\tLR: 0.450000\n",
      "Loss: 1.458940\tLR: 0.450000\n",
      "Loss: 1.453419\tLR: 0.450000\n",
      "Loss: 1.416709\tLR: 0.450000\n",
      "Loss: 1.373173\tLR: 0.405000\n",
      "Loss: 1.327370\tLR: 0.405000\n",
      "Loss: 1.344934\tLR: 0.405000\n",
      "Loss: 1.332798\tLR: 0.405000\n",
      "Loss: 1.291747\tLR: 0.405000\n",
      "Loss: 1.290107\tLR: 0.364500\n",
      "Loss: 1.272306\tLR: 0.364500\n",
      "Loss: 1.237117\tLR: 0.364500\n",
      "Loss: 1.230260\tLR: 0.364500\n",
      "Loss: 1.220406\tLR: 0.364500\n",
      "Loss: 1.229550\tLR: 0.328050\n",
      "Loss: 1.217688\tLR: 0.328050\n",
      "Loss: 1.195621\tLR: 0.328050\n",
      "Loss: 1.182000\tLR: 0.328050\n",
      "Loss: 1.179293\tLR: 0.328050\n",
      "Loss: 1.177949\tLR: 0.295245\n",
      "Loss: 1.169717\tLR: 0.295245\n",
      "Loss: 1.165436\tLR: 0.295245\n",
      "Loss: 1.167471\tLR: 0.295245\n",
      "Loss: 1.160034\tLR: 0.295245\n",
      "Loss: 1.156658\tLR: 0.265721\n",
      "Loss: 1.145080\tLR: 0.265721\n",
      "Loss: 1.137138\tLR: 0.265721\n",
      "Loss: 1.134528\tLR: 0.265721\n",
      "Loss: 1.136742\tLR: 0.265721\n",
      "Loss: 1.133928\tLR: 0.239148\n",
      "Loss: 1.126968\tLR: 0.239148\n",
      "Loss: 1.123042\tLR: 0.239148\n",
      "Loss: 1.122602\tLR: 0.239148\n",
      "Loss: 1.117722\tLR: 0.239148\n",
      "Loss: 1.119784\tLR: 0.215234\n",
      "Loss: 1.114570\tLR: 0.215234\n",
      "Loss: 1.119632\tLR: 0.215234\n",
      "Loss: 1.116348\tLR: 0.215234\n",
      "Loss: 1.109936\tLR: 0.215234\n",
      "Loss: 1.110045\tLR: 0.193710\n",
      "Loss: 1.110198\tLR: 0.193710\n",
      "Loss: 1.106017\tLR: 0.193710\n",
      "Loss: 1.106632\tLR: 0.193710\n",
      "Loss: 1.111208\tLR: 0.193710\n",
      "Loss: 1.105564\tLR: 0.174339\n",
      "Loss: 1.106364\tLR: 0.174339\n",
      "Loss: 1.104630\tLR: 0.174339\n",
      "Loss: 1.102323\tLR: 0.174339\n",
      "Loss: 1.100629\tLR: 0.174339\n",
      "Loss: 1.099713\tLR: 0.156905\n",
      "Loss: 1.097166\tLR: 0.156905\n",
      "Loss: 1.099792\tLR: 0.156905\n",
      "Loss: 1.096397\tLR: 0.156905\n",
      "Loss: 1.098405\tLR: 0.156905\n",
      "Loss: 1.096333\tLR: 0.141215\n",
      "Loss: 1.094562\tLR: 0.141215\n",
      "Loss: 1.090382\tLR: 0.141215\n",
      "Loss: 1.098526\tLR: 0.141215\n",
      "Loss: 1.094915\tLR: 0.141215\n",
      "Loss: 1.092466\tLR: 0.127093\n",
      "Loss: 1.094188\tLR: 0.127093\n",
      "Loss: 1.093145\tLR: 0.127093\n",
      "Loss: 1.089633\tLR: 0.127093\n",
      "Loss: 1.090304\tLR: 0.127093\n",
      "Loss: 1.089429\tLR: 0.114384\n",
      "Loss: 1.091721\tLR: 0.114384\n",
      "Loss: 1.086944\tLR: 0.114384\n",
      "Loss: 1.089756\tLR: 0.114384\n",
      "Loss: 1.086575\tLR: 0.114384\n",
      "Loss: 1.088614\tLR: 0.102946\n",
      "Loss: 1.083953\tLR: 0.102946\n",
      "Loss: 1.085082\tLR: 0.102946\n",
      "Loss: 1.084660\tLR: 0.102946\n",
      "Loss: 1.082853\tLR: 0.102946\n",
      "Loss: 1.082728\tLR: 0.092651\n",
      "Loss: 1.086022\tLR: 0.092651\n",
      "Loss: 1.079702\tLR: 0.092651\n",
      "Loss: 1.085997\tLR: 0.092651\n",
      "Loss: 1.086200\tLR: 0.092651\n",
      "Loss: 1.082152\tLR: 0.083386\n",
      "Loss: 1.077381\tLR: 0.083386\n",
      "Loss: 1.080715\tLR: 0.083386\n",
      "Loss: 1.079507\tLR: 0.083386\n",
      "Loss: 1.077594\tLR: 0.083386\n",
      "Loss: 1.074214\tLR: 0.075047\n",
      "Loss: 1.079193\tLR: 0.075047\n",
      "Loss: 1.075211\tLR: 0.075047\n",
      "Loss: 1.079519\tLR: 0.075047\n",
      "Loss: 1.077114\tLR: 0.075047\n",
      "Loss: 1.082788\tLR: 0.067543\n",
      "Loss: 1.078726\tLR: 0.067543\n",
      "Loss: 1.077565\tLR: 0.067543\n",
      "Loss: 1.077984\tLR: 0.067543\n",
      "Loss: 1.080126\tLR: 0.067543\n",
      "Loss: 1.075677\tLR: 0.060788\n",
      "Loss: 1.073882\tLR: 0.060788\n",
      "Loss: 1.071355\tLR: 0.060788\n",
      "Loss: 1.078521\tLR: 0.060788\n",
      "Loss: 1.076218\tLR: 0.060788\n",
      "Loss: 1.070274\tLR: 0.054709\n",
      "Loss: 1.074371\tLR: 0.054709\n",
      "Loss: 1.074756\tLR: 0.054709\n",
      "Loss: 1.070593\tLR: 0.054709\n",
      "Loss: 1.072317\tLR: 0.054709\n",
      "Loss: 1.077191\tLR: 0.049239\n",
      "Loss: 1.072055\tLR: 0.049239\n",
      "Loss: 1.071670\tLR: 0.049239\n",
      "Loss: 1.072700\tLR: 0.049239\n",
      "Loss: 1.072190\tLR: 0.049239\n",
      "Loss: 1.072236\tLR: 0.044315\n",
      "Loss: 1.069212\tLR: 0.044315\n",
      "Loss: 1.071005\tLR: 0.044315\n",
      "Loss: 1.071706\tLR: 0.044315\n",
      "Loss: 1.072589\tLR: 0.044315\n",
      "Loss: 1.071698\tLR: 0.039883\n",
      "Loss: 1.077419\tLR: 0.039883\n",
      "Loss: 1.070816\tLR: 0.039883\n",
      "Loss: 1.073659\tLR: 0.039883\n",
      "Loss: 1.069051\tLR: 0.039883\n",
      "Loss: 1.072494\tLR: 0.035895\n",
      "Loss: 1.074913\tLR: 0.035895\n",
      "Loss: 1.073973\tLR: 0.035895\n",
      "Loss: 1.069229\tLR: 0.035895\n",
      "Loss: 1.070182\tLR: 0.035895\n",
      "Loss: 1.068817\tLR: 0.032305\n",
      "Loss: 1.070445\tLR: 0.032305\n",
      "Loss: 1.071325\tLR: 0.032305\n",
      "Loss: 1.068979\tLR: 0.032305\n",
      "Loss: 1.067827\tLR: 0.032305\n",
      "Loss: 1.070267\tLR: 0.029075\n",
      "Loss: 1.066822\tLR: 0.029075\n",
      "Loss: 1.068437\tLR: 0.029075\n",
      "Loss: 1.071212\tLR: 0.029075\n",
      "Loss: 1.068275\tLR: 0.029075\n",
      "Loss: 1.068257\tLR: 0.026167\n",
      "Loss: 1.070816\tLR: 0.026167\n",
      "Loss: 1.069474\tLR: 0.026167\n",
      "Loss: 1.068259\tLR: 0.026167\n",
      "Loss: 1.065555\tLR: 0.026167\n",
      "Loss: 1.063248\tLR: 0.023551\n",
      "Loss: 1.068873\tLR: 0.023551\n",
      "Loss: 1.070676\tLR: 0.023551\n",
      "Loss: 1.069987\tLR: 0.023551\n",
      "Loss: 1.067919\tLR: 0.023551\n",
      "Loss: 1.073363\tLR: 0.021196\n",
      "Loss: 1.069419\tLR: 0.021196\n",
      "Loss: 1.064237\tLR: 0.021196\n",
      "Loss: 1.070276\tLR: 0.021196\n",
      "Loss: 1.068959\tLR: 0.021196\n",
      "Loss: 1.067150\tLR: 0.019076\n",
      "Loss: 1.064881\tLR: 0.019076\n",
      "Loss: 1.069524\tLR: 0.019076\n",
      "Loss: 1.068152\tLR: 0.019076\n",
      "Loss: 1.065816\tLR: 0.019076\n",
      "Loss: 1.069598\tLR: 0.017168\n",
      "Loss: 1.068072\tLR: 0.017168\n",
      "Loss: 1.063531\tLR: 0.017168\n",
      "Loss: 1.065904\tLR: 0.017168\n",
      "Loss: 1.066719\tLR: 0.017168\n",
      "Loss: 1.069623\tLR: 0.015452\n",
      "Loss: 1.068607\tLR: 0.015452\n",
      "Loss: 1.065313\tLR: 0.015452\n",
      "Loss: 1.064792\tLR: 0.015452\n",
      "Loss: 1.065819\tLR: 0.015452\n",
      "Loss: 1.067447\tLR: 0.013906\n",
      "Loss: 1.068172\tLR: 0.013906\n",
      "Loss: 1.067693\tLR: 0.013906\n",
      "Loss: 1.069450\tLR: 0.013906\n",
      "Loss: 1.066187\tLR: 0.013906\n",
      "Loss: 1.068035\tLR: 0.012516\n",
      "Loss: 1.069540\tLR: 0.012516\n",
      "Loss: 1.068904\tLR: 0.012516\n",
      "Loss: 1.066811\tLR: 0.012516\n",
      "Loss: 1.064337\tLR: 0.012516\n",
      "Loss: 1.065546\tLR: 0.011264\n",
      "Loss: 1.071802\tLR: 0.011264\n",
      "Loss: 1.071429\tLR: 0.011264\n",
      "Loss: 1.066650\tLR: 0.011264\n",
      "Loss: 1.066979\tLR: 0.011264\n",
      "Loss: 1.063421\tLR: 0.010138\n",
      "Loss: 1.065728\tLR: 0.010138\n",
      "Loss: 1.063464\tLR: 0.010138\n",
      "Loss: 1.067122\tLR: 0.010138\n",
      "Loss: 1.067137\tLR: 0.010138\n",
      "Loss: 1.067517\tLR: 0.009124\n",
      "Loss: 1.066157\tLR: 0.009124\n",
      "Loss: 1.064606\tLR: 0.009124\n",
      "Loss: 1.070918\tLR: 0.009124\n",
      "Loss: 1.066870\tLR: 0.009124\n",
      "Loss: 1.065550\tLR: 0.008212\n",
      "Loss: 1.066066\tLR: 0.008212\n",
      "Loss: 1.064010\tLR: 0.008212\n",
      "Loss: 1.069673\tLR: 0.008212\n",
      "Loss: 1.063925\tLR: 0.008212\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch, sample_spacings)\n",
    "    print(f\"Loss: {output[1]:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    output[1].backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.encoder.layers.0.attn.logit_scale\n",
      "vit.encoder.layers.1.attn.logit_scale\n",
      "vit.encoder.layers.2.attn.logit_scale\n",
      "vit.encoder.layers.3.attn.logit_scale\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
