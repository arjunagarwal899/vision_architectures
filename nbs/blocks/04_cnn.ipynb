{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp blocks/cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from functools import cache\n",
    "from itertools import chain, permutations\n",
    "from typing import Any, Literal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.activations import get_act_layer\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, field_validator, model_validator\n",
    "from vision_architectures.utils.normalizations import get_norm_layer\n",
    "from vision_architectures.utils.rearrange import rearrange_channels\n",
    "from vision_architectures.utils.residuals import Residual\n",
    "from vision_architectures.utils.splitter_merger import Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "possible_sequences = [\"\".join(p) for p in chain.from_iterable(permutations(\"ACDN\", r) for r in range(5)) if \"C\" in p]\n",
    "\n",
    "\n",
    "class CNNBlockConfig(CustomBaseModel):\n",
    "    in_channels: int\n",
    "    out_channels: int\n",
    "    kernel_size: int | tuple[int, ...]\n",
    "    padding: int | tuple[int, ...] | str = \"same\"\n",
    "    stride: int = 1\n",
    "    conv_kwargs: dict[str, Any] = {}\n",
    "    transposed: bool = Field(False, description=\"Whether to perform ConvTranspose instead of Conv\")\n",
    "\n",
    "    normalization: str | None = \"batchnorm3d\"\n",
    "    normalization_pre_args: list = []\n",
    "    normalization_post_args: list = []\n",
    "    normalization_kwargs: dict = {}\n",
    "    activation: str | None = \"relu\"\n",
    "    activation_kwargs: dict = {}\n",
    "\n",
    "    sequence: Literal[tuple(possible_sequences)] = \"CNA\"\n",
    "\n",
    "    drop_prob: float = 0.0\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if self.normalization is None and \"N\" in self.sequence:\n",
    "            self.sequence = self.sequence.replace(\"N\", \"\")\n",
    "        if self.normalization is not None and \"N\" not in self.sequence:\n",
    "            raise ValueError(\"Add N to the sequence or set normalization=None.\")\n",
    "        if self.activation is None and \"A\" in self.sequence:\n",
    "            self.sequence = self.sequence.replace(\"A\", \"\")\n",
    "        if self.activation is not None and \"A\" not in self.sequence:\n",
    "            raise ValueError(\"Add A to the sequence or set activation=None.\")\n",
    "        if self.drop_prob == 0.0 and \"D\" in self.sequence:\n",
    "            self.sequence = self.sequence.replace(\"D\", \"\")\n",
    "        if self.drop_prob > 0.0 and \"D\" not in self.sequence:\n",
    "            raise ValueError(\"Add D to the sequence or set drop_prob=0.\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class MultiResCNNBlockConfig(CNNBlockConfig):\n",
    "    kernel_sizes: tuple[int | tuple[int, ...], ...] = (3, 5, 7)\n",
    "    filter_ratios: tuple[float, ...] = Field(\n",
    "        (1, 2, 3), description=\"Ratio of filters to out_channels for each conv layer. Will be scaled to sum to 1.\"\n",
    "    )\n",
    "    padding: Literal[\"same\"] = \"same\"\n",
    "\n",
    "    kernel_size: int = 3\n",
    "\n",
    "    @field_validator(\"filter_ratios\", mode=\"after\")\n",
    "    @classmethod\n",
    "    def scale_filter_ratios(cls, filter_ratios):\n",
    "        filter_ratios = tuple(ratio / sum(filter_ratios) for ratio in filter_ratios)\n",
    "        return filter_ratios\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.kernel_sizes == (3, 5, 7), \"Only kernel sizes of (3, 5, 7) are supported for MultiResCNNBlock\"\n",
    "        assert self.kernel_size == 3, \"only kernel_size = 3 is supported for MultiResCNNBlock\"\n",
    "        assert len(self.kernel_sizes) == len(\n",
    "            self.filter_ratios\n",
    "        ), \"kernel_sizes and filter_ratios must have the same length\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class _CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, spatial_dims: Literal[2, 3], config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = CNNBlockConfig.model_validate(config | kwargs)\n",
    "\n",
    "        normalization = self.config.normalization\n",
    "        activation = self.config.activation\n",
    "        drop_prob = self.config.drop_prob\n",
    "        sequence = self.config.sequence\n",
    "\n",
    "        bias = True\n",
    "        if normalization is not None and normalization.startswith(\"batchnorm\") and \"CN\" in sequence:\n",
    "            bias = False\n",
    "\n",
    "        match spatial_dims, self.config.transposed:\n",
    "            case 2, False:\n",
    "                conv_module = nn.Conv2d\n",
    "            case 2, True:\n",
    "                conv_module = nn.ConvTranspose2d\n",
    "            case 3, False:\n",
    "                conv_module = nn.Conv3d\n",
    "            case 3, True:\n",
    "                conv_module = nn.ConvTranspose3d\n",
    "            case _:\n",
    "                raise ValueError(f\"Unsupported spatial dimensions: {spatial_dims}\")\n",
    "\n",
    "        self.conv = conv_module(\n",
    "            in_channels=self.config.in_channels,\n",
    "            out_channels=self.config.out_channels,\n",
    "            kernel_size=self.config.kernel_size,\n",
    "            padding=self.config.padding,\n",
    "            stride=self.config.stride,\n",
    "            bias=bias,\n",
    "            **self.config.conv_kwargs,\n",
    "        )\n",
    "\n",
    "        self.norm = None\n",
    "        self.act = None\n",
    "        self.dropout = None\n",
    "\n",
    "        norm_channels = self.config.out_channels\n",
    "        if \"N\" in sequence.split(\"C\")[0]:\n",
    "            norm_channels = self.config.in_channels\n",
    "\n",
    "        if \"N\" in sequence:\n",
    "            self.norm = get_norm_layer(\n",
    "                normalization,\n",
    "                *self.config.normalization_pre_args,\n",
    "                norm_channels,\n",
    "                *self.config.normalization_post_args,\n",
    "                **self.config.normalization_kwargs,\n",
    "            )\n",
    "        if \"A\" in sequence:\n",
    "            self.act = get_act_layer(activation, **self.config.activation_kwargs)\n",
    "        if \"D\" in sequence:\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "\n",
    "    def _forward(self, x: torch.Tensor, channels_first: bool = True):\n",
    "        # x: (b, [in_channels], [z], y, x, [in_channels])\n",
    "\n",
    "        x = rearrange_channels(x, channels_first, True)\n",
    "        # Now x is (b, in_channels, [z], y, x)\n",
    "\n",
    "        for layer in self.config.sequence:\n",
    "            if layer == \"C\":\n",
    "                x = self.conv(x)\n",
    "            if layer == \"A\":\n",
    "                x = self.act(x)\n",
    "            elif layer == \"D\":\n",
    "                x = self.dropout(x)\n",
    "            elif layer == \"N\":\n",
    "                x = self.norm(x)\n",
    "        # (b, out_channels, [z], y, x)\n",
    "\n",
    "        x = rearrange_channels(x, True, channels_first)\n",
    "        # (b, [out_channels], [z], y, x, [out_channels])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level1(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CNNBlock3D(_CNNBlock):\n",
    "    def __init__(self, config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__(3, config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mgroups\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mGroupNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mSiLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CNNBlock3D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    kernel_size=3,\n",
    "    normalization=\"groupnorm\",\n",
    "    normalization_pre_args=[2],\n",
    "    activation=\"silu\",\n",
    "    drop_prob=0.5,\n",
    "    padding=1,\n",
    "    conv_kwargs={\"groups\": 2},\n",
    "    sequence=\"NCDA\",\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConvTranspose3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mgroups\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mPReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnum_parameters\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CNNBlock3D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    kernel_size=4,\n",
    "    normalization=\"batchnorm3d\",\n",
    "    activation=\"prelu\",\n",
    "    drop_prob=0.5,\n",
    "    padding=1,\n",
    "    stride=2,\n",
    "    conv_kwargs={\"groups\": 2},\n",
    "    sequence=\"NDAC\",\n",
    "    transposed=True,\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CNNBlock2D(_CNNBlock):\n",
    "    def __init__(self, config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__(2, config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mCNNBlock2D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mgroups\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mGroupNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mSiLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CNNBlock2D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    kernel_size=3,\n",
    "    normalization=\"groupnorm\",\n",
    "    normalization_pre_args=[2],\n",
    "    activation=\"silu\",\n",
    "    drop_prob=0.5,\n",
    "    padding=1,\n",
    "    conv_kwargs={\"groups\": 2},\n",
    "    sequence=\"NCDA\",\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mCNNBlock2D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConvTranspose2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mgroups\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mPReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnum_parameters\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CNNBlock2D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    kernel_size=4,\n",
    "    normalization=\"batchnorm2d\",\n",
    "    activation=\"prelu\",\n",
    "    drop_prob=0.5,\n",
    "    padding=1,\n",
    "    stride=2,\n",
    "    conv_kwargs={\"groups\": 2},\n",
    "    sequence=\"NDAC\",\n",
    "    transposed=True,\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class _MultiResCNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, spatial_dims: Literal[2, 3], config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = MultiResCNNBlockConfig.model_validate(config | kwargs)\n",
    "\n",
    "        assert self.config.kernel_sizes == (3, 5, 7), \"Only kernel sizes of (3, 5, 7) are supported for now\"\n",
    "\n",
    "        all_out_channels = [max(1, int(self.config.out_channels * ratio)) for ratio in self.config.filter_ratios[:-1]]\n",
    "        last_out_channels = self.config.out_channels - sum(all_out_channels)\n",
    "        all_out_channels.append(last_out_channels)\n",
    "        if last_out_channels <= 0:\n",
    "            raise ValueError(\n",
    "                f\"These filter values ({self.config.filter_ratios}) won't work with the given out_channels. Please \"\n",
    "                f\"adjust them. The out_channels of each conv layer is coming out to be {all_out_channels}.\"\n",
    "            )\n",
    "        all_in_channels = [self.config.in_channels] + all_out_channels[:-1]\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                _CNNBlock(\n",
    "                    spatial_dims,\n",
    "                    self.config.model_dump(),\n",
    "                    checkpointing_level,\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                )\n",
    "                for in_channels, out_channels in zip(all_in_channels, all_out_channels)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.residual_conv = _CNNBlock(\n",
    "            spatial_dims,\n",
    "            self.config.model_dump(),\n",
    "            checkpointing_level,\n",
    "            in_channels=self.config.in_channels,\n",
    "            out_channels=self.config.out_channels,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "        self.residual = Residual()\n",
    "\n",
    "        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)\n",
    "\n",
    "    def _forward(self, x: torch.Tensor, channels_first: bool = True):\n",
    "        # x: (b, [in_channels], [z], y, x, [in_channels])\n",
    "\n",
    "        x = rearrange_channels(x, channels_first, True)\n",
    "        # (b, in_channels, [z], y, x)\n",
    "\n",
    "        residual = self.residual_conv(x)\n",
    "        # (b, out_channels, [z], y, x)\n",
    "\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_input = conv_outputs[-1] if conv_outputs else x\n",
    "            conv_output = conv(conv_input)\n",
    "            conv_outputs.append(conv_output)\n",
    "            # (b, one_of_all_out_channels, [z], y, x)\n",
    "\n",
    "        x = torch.cat(conv_outputs, dim=1)\n",
    "        # (b, out_channels, [z], y, x)\n",
    "\n",
    "        x = self.residual(x, residual)\n",
    "        # (b, out_channels, [z], y, x)\n",
    "\n",
    "        x = rearrange_channels(x, True, channels_first)\n",
    "        # (b, [out_channels], [z], y, x, [out_channels])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level2(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MultiResCNNBlock3D(_MultiResCNNBlock):\n",
    "    def __init__(self, config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__(3, config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMultiResCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconvs\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual_conv\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MultiResCNNBlock3D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    filter_ratios=(3, 2, 1),\n",
    "    activation=\"gelu\",\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MultiResCNNBlock2D(_MultiResCNNBlock):\n",
    "    def __init__(self, config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        super().__init__(2, config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMultiResCNNBlock2D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconvs\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual_conv\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MultiResCNNBlock2D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    filter_ratios=(3, 2, 1),\n",
    "    activation=\"gelu\",\n",
    "    normalization=\"batchnorm2d\",\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor splitting inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class TensorSplittingConv(nn.Module):\n",
    "    \"\"\"Convolution layer that operates on splits of a tensor on desired device and concatenates the results to give a\n",
    "    lossless output. This is useful for large tensors that cannot fit in memory.\"\"\"\n",
    "\n",
    "    def __init__(self, conv: nn.Module, num_splits: int | tuple[int, ...]):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(conv, nn.Conv2d):\n",
    "            self.spatial_dims = 2\n",
    "        elif isinstance(conv, nn.Conv3d):\n",
    "            self.spatial_dims = 3\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported convolution type. Only Conv2d and Conv3d are supported.\")\n",
    "\n",
    "        assert conv.stride == (1,) * self.spatial_dims, \"Stride must be 1 for tensor splitting convolution.\"\n",
    "        assert conv.padding == \"same\", \"Padding must be 'same' for tensor splitting convolution.\"\n",
    "\n",
    "        if isinstance(num_splits, int):\n",
    "            num_splits = (num_splits,) * self.spatial_dims\n",
    "        assert len(num_splits) == self.spatial_dims, \"num_splits must be a tuple of length equal to spatial_dims\"\n",
    "\n",
    "        self.conv = conv\n",
    "        self.num_splits = num_splits\n",
    "\n",
    "    @cache\n",
    "    def get_receptive_field(self) -> tuple[int, ...]:\n",
    "        \"\"\"Calculate the receptive field of the convolution layer.\"\"\"\n",
    "        kernel_size = torch.tensor(self.conv.kernel_size)\n",
    "        dilation = torch.tensor(self.conv.dilation)\n",
    "        receptive_field = dilation * (kernel_size - 1) + 1\n",
    "        return tuple(receptive_field.tolist())\n",
    "\n",
    "    @cache\n",
    "    def get_edge_context(self):\n",
    "        \"\"\"Calculate the context size required to eliminate edge effects when merging the conv outputs into one.\"\"\"\n",
    "        receptive_field = self.get_receptive_field()\n",
    "        context = torch.tensor(receptive_field) // 2\n",
    "        return tuple(context.tolist())\n",
    "\n",
    "    def get_split_size(self, input_shape: tuple[int, ...] | torch.Tensor) -> tuple[int, ...]:\n",
    "        \"\"\"Calculate the split size for each dimension based on the input shape and number of splits.\n",
    "\n",
    "        Args:\n",
    "            input_shape: Shape of the input tensor. If a tensor is provided, its shape will be used.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of split sizes for each dimension.\n",
    "        \"\"\"\n",
    "        if isinstance(input_shape, torch.Tensor):\n",
    "            input_shape = input_shape.shape\n",
    "        input_shape = input_shape[-self.spatial_dims :]\n",
    "\n",
    "        context = self.get_edge_context()\n",
    "\n",
    "        split_size = []\n",
    "        for i in range(self.spatial_dims):\n",
    "            dim = input_shape[i]\n",
    "            num_splits = self.num_splits[i]\n",
    "            if dim % num_splits != 0:\n",
    "                raise ValueError(f\"Input dimension {dim} is not divisible by number of splits {num_splits}.\")\n",
    "            split_size.append(dim // num_splits + 2 * context[i])\n",
    "        split_size = tuple(split_size)\n",
    "        return split_size\n",
    "\n",
    "    def get_split_stride(self, input_shape: tuple[int, ...] | torch.Tensor) -> tuple[int, ...]:\n",
    "        \"\"\"Calculate the split stride for each dimension based on the input shape and context size.\"\"\"\n",
    "        context = self.get_edge_context()\n",
    "        split_size = self.get_split_size(input_shape)\n",
    "        split_stride = [split_size[i] - 2 * context[i] for i in range(self.spatial_dims)]\n",
    "        assert all(\n",
    "            split_stride[i] > 0 for i in range(self.spatial_dims)\n",
    "        ), \"Split stride must be greater than 0 for all dimensions.\"\n",
    "        return split_stride\n",
    "\n",
    "    def pad_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Pad the input with the context size for consistent merging.\"\"\"\n",
    "        context = self.get_edge_context()\n",
    "        padding = [0, 0] * (x.ndim - self.spatial_dims)\n",
    "        for i in range(self.spatial_dims):\n",
    "            padding.extend([context[i], context[i]])\n",
    "        x = nn.functional.pad(x, list(reversed(padding)))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the convolution layer with tensor splitting parallelism. Main convolution occurs on it's\n",
    "             device, but the output is built on the input tensor's device.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, in_channels, [z], y, x).\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, out_channels, [z], y, x).\n",
    "        \"\"\"\n",
    "        input_device = x.device\n",
    "        B, DIMS = x.shape[0], x.shape[2:]  # (batch_size, in_channels, [z], y, x)\n",
    "\n",
    "        # Calculate split size\n",
    "        split_size = self.get_split_size(x)\n",
    "\n",
    "        # Identify the stride required to split the input tensor such that overlapping regions can be counted only once\n",
    "        split_stride = self.get_split_stride(x)\n",
    "\n",
    "        # Pad the input\n",
    "        x = self.pad_input(x)\n",
    "\n",
    "        # Split the input tensor\n",
    "        splitter = Splitter(\n",
    "            split_dims=self.spatial_dims,\n",
    "            split_size=split_size,\n",
    "            stride=split_stride,\n",
    "        )\n",
    "        positions = splitter.get_positions(x)\n",
    "        x = splitter(x)\n",
    "        # (num_splits, batch_size, in_channels, [z1], y1, x1)\n",
    "\n",
    "        # Run the convolution on each split\n",
    "        outputs = []\n",
    "        for x_split in x:\n",
    "            x_split = x_split.to(self.conv.weight.device)\n",
    "            x_split = self.conv(x_split)\n",
    "            x_split = x_split.to(input_device)\n",
    "            outputs.append(x_split)\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        # (num_splits, batch_size, out_channels, [z1], y1, x1)\n",
    "\n",
    "        # Merge the outputs\n",
    "        context = self.get_edge_context()\n",
    "        merged = torch.zeros((B, outputs.shape[2], *DIMS), device=input_device)\n",
    "        for output, position in zip(outputs, positions):\n",
    "            output_slices = [slice(None), slice(None)]\n",
    "            for i in range(self.spatial_dims):\n",
    "                output_slices.append(slice(context[i], -context[i] if context[i] != 0 else None))\n",
    "            output = output[tuple(output_slices)]\n",
    "\n",
    "            merged_slices = [slice(None), slice(None)]\n",
    "            for i in range(self.spatial_dims):\n",
    "                merged_slices.append(slice(position[i], position[i] + split_stride[i]))\n",
    "            merged[tuple(merged_slices)] = output\n",
    "\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTensorSplittingConv\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m10\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m18\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv3d(in_channels=2, out_channels=4, kernel_size=3, padding=\"same\").eval()\n",
    "sample_input = torch.randn(2, 2, 16, 18, 16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output1 = conv(sample_input)\n",
    "\n",
    "test = TensorSplittingConv(conv, num_splits=(2, 3, 4))\n",
    "test.to(\"cuda:0\")\n",
    "display(test)\n",
    "\n",
    "display(test.get_receptive_field())\n",
    "display(test.get_edge_context())\n",
    "display(test.get_split_size(sample_input))\n",
    "display(test.get_split_stride(sample_input))\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output2 = test(sample_input)\n",
    "\n",
    "test_output2.shape, torch.allclose(test_output1, test_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
