{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp blocks/cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from functools import cache, wraps\n",
    "from itertools import chain, permutations\n",
    "from typing import Any, Literal\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.activations import get_act_layer\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field, field_validator, model_validator\n",
    "from vision_architectures.utils.normalizations import get_norm_layer\n",
    "from vision_architectures.utils.rearrange import rearrange_channels\n",
    "from vision_architectures.utils.residuals import Residual\n",
    "from vision_architectures.utils.splitter_merger import Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "possible_sequences = [\"\".join(p) for p in chain.from_iterable(permutations(\"ACDN\", r) for r in range(5)) if \"C\" in p]\n",
    "\n",
    "\n",
    "class CNNBlockConfig(CustomBaseModel):\n",
    "    in_channels: int = Field(..., description=\"Number of input channels\")\n",
    "    out_channels: int = Field(..., description=\"Number of output channels\")\n",
    "    kernel_size: int | tuple[int, ...] = Field(..., description=\"Kernel size for the convolution\")\n",
    "    padding: int | tuple[int, ...] | str = Field(\n",
    "        \"same\", description=\"Padding for the convolution. Can be 'same' or an integer/tuple of integers.\"\n",
    "    )\n",
    "    stride: int | tuple[int, ...] = Field(1, description=\"Stride for the convolution\")\n",
    "    conv_kwargs: dict[str, Any] = Field({}, description=\"Additional keyword arguments for the convolution layer\")\n",
    "    transposed: bool = Field(False, description=\"Whether to perform ConvTranspose instead of Conv\")\n",
    "\n",
    "    normalization: str | None = Field(\"batchnorm3d\", description=\"Normalization layer type.\")\n",
    "    normalization_pre_args: list = Field(\n",
    "        [],\n",
    "        description=(\n",
    "            \"Arguments for the normalization layer before providing the dimension. Useful when using \"\n",
    "            \"GroupNorm layers are being used to specify the number of groups.\"\n",
    "        ),\n",
    "    )\n",
    "    normalization_post_args: list = Field(\n",
    "        [], description=\"Arguments for the normalization layer after providing the dimension.\"\n",
    "    )\n",
    "    normalization_kwargs: dict = Field({}, description=\"Additional keyword arguments for the normalization layer\")\n",
    "    activation: str | None = Field(\"relu\", description=\"Activation function type.\")\n",
    "    activation_kwargs: dict = Field({}, description=\"Additional keyword arguments for the activation function.\")\n",
    "\n",
    "    sequence: Literal[tuple(possible_sequences)] = Field(\"CNA\", description=\"Sequence of operations in the block.\")\n",
    "\n",
    "    drop_prob: float = Field(0.0, description=\"Dropout probability.\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        if self.normalization is None and \"N\" in self.sequence:\n",
    "            self.sequence = self.sequence.replace(\"N\", \"\")\n",
    "        if self.normalization is not None and \"N\" not in self.sequence:\n",
    "            raise ValueError(\"Add N to the sequence or set normalization=None.\")\n",
    "        if self.activation is None and \"A\" in self.sequence:\n",
    "            self.sequence = self.sequence.replace(\"A\", \"\")\n",
    "        if self.activation is not None and \"A\" not in self.sequence:\n",
    "            raise ValueError(\"Add A to the sequence or set activation=None.\")\n",
    "        if self.drop_prob == 0.0 and \"D\" in self.sequence:\n",
    "            self.sequence = self.sequence.replace(\"D\", \"\")\n",
    "        if self.drop_prob > 0.0 and \"D\" not in self.sequence:\n",
    "            raise ValueError(\"Add D to the sequence or set drop_prob=0.\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class MultiResCNNBlockConfig(CNNBlockConfig):\n",
    "    kernel_sizes: tuple[int | tuple[int, ...], ...] = Field((3, 5, 7), description=\"Kernel sizes for each conv layer.\")\n",
    "    filter_ratios: tuple[float, ...] = Field(\n",
    "        (1, 2, 3), description=\"Ratio of filters to out_channels for each conv layer. Will be scaled to sum to 1.\"\n",
    "    )\n",
    "    padding: Literal[\"same\"] = Field(\n",
    "        \"same\", description=\"Padding for the convolution. Only 'same' is supported for MultiResCNNBlock.\"\n",
    "    )\n",
    "\n",
    "    kernel_size: int = Field(\n",
    "        3, description=\"Kernel size for the convolution. Only kernel_size=3 is supported for MultiResCNNBlock.\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"filter_ratios\", mode=\"after\")\n",
    "    @classmethod\n",
    "    def scale_filter_ratios(cls, filter_ratios):\n",
    "        filter_ratios = tuple(ratio / sum(filter_ratios) for ratio in filter_ratios)\n",
    "        return filter_ratios\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate(self):\n",
    "        super().validate()\n",
    "        assert self.kernel_sizes == (3, 5, 7), \"Only kernel sizes of (3, 5, 7) are supported for MultiResCNNBlock\"\n",
    "        assert self.kernel_size == 3, \"only kernel_size = 3 is supported for MultiResCNNBlock\"\n",
    "        assert len(self.kernel_sizes) == len(\n",
    "            self.filter_ratios\n",
    "        ), \"kernel_sizes and filter_ratios must have the same length\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class _CNNBlock(nn.Module):\n",
    "    \"\"\"A block to perform a sequence of convolution, activation, normalization, and dropout operations. Works for 2D as\n",
    "    well as 3D data.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(\n",
    "        self, spatial_dims: Literal[2, 3], config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize the CNNBlock block. Activation checkpointing level 1.\n",
    "\n",
    "        Args:\n",
    "            spatial_dims: Number of spatial dimensions (2 or 3). This is used to determine 2D vs 3D data\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = CNNBlockConfig.model_validate(config | kwargs)\n",
    "\n",
    "        normalization = self.config.normalization\n",
    "        activation = self.config.activation\n",
    "        drop_prob = self.config.drop_prob\n",
    "        sequence = self.config.sequence\n",
    "\n",
    "        bias = True\n",
    "        if normalization is not None and normalization.startswith(\"batchnorm\") and \"CN\" in sequence:\n",
    "            bias = False\n",
    "\n",
    "        match spatial_dims, self.config.transposed:\n",
    "            case 2, False:\n",
    "                conv_module = nn.Conv2d\n",
    "            case 2, True:\n",
    "                conv_module = nn.ConvTranspose2d\n",
    "            case 3, False:\n",
    "                conv_module = nn.Conv3d\n",
    "            case 3, True:\n",
    "                conv_module = nn.ConvTranspose3d\n",
    "            case _:\n",
    "                raise ValueError(f\"Unsupported spatial dimensions: {spatial_dims}\")\n",
    "\n",
    "        self.conv = conv_module(\n",
    "            in_channels=self.config.in_channels,\n",
    "            out_channels=self.config.out_channels,\n",
    "            kernel_size=self.config.kernel_size,\n",
    "            padding=self.config.padding,\n",
    "            stride=self.config.stride,\n",
    "            bias=bias,\n",
    "            **self.config.conv_kwargs,\n",
    "        )\n",
    "\n",
    "        self.norm = None\n",
    "        self.act = None\n",
    "        self.dropout = None\n",
    "\n",
    "        norm_channels = self.config.out_channels\n",
    "        if \"N\" in sequence.split(\"C\")[0]:\n",
    "            norm_channels = self.config.in_channels\n",
    "\n",
    "        if \"N\" in sequence:\n",
    "            self.norm = get_norm_layer(\n",
    "                normalization,\n",
    "                *self.config.normalization_pre_args,\n",
    "                norm_channels,\n",
    "                *self.config.normalization_post_args,\n",
    "                **self.config.normalization_kwargs,\n",
    "            )\n",
    "        if \"A\" in sequence:\n",
    "            self.act = get_act_layer(activation, **self.config.activation_kwargs)\n",
    "        if \"D\" in sequence:\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(self, x: torch.Tensor, channels_first: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the CNNBlock block.\n",
    "\n",
    "        Args:\n",
    "            x: {INPUT_3D_DOC}\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_3D_DOC}\n",
    "        \"\"\"\n",
    "        # x: (b, [in_channels], [z], y, x, [in_channels])\n",
    "\n",
    "        x = rearrange_channels(x, channels_first, True)\n",
    "        # Now x is (b, in_channels, [z], y, x)\n",
    "\n",
    "        for layer in self.config.sequence:\n",
    "            if layer == \"C\":\n",
    "                x = self.conv(x)\n",
    "            if layer == \"A\":\n",
    "                x = self.act(x)\n",
    "            elif layer == \"D\":\n",
    "                x = self.dropout(x)\n",
    "            elif layer == \"N\":\n",
    "                x = self.norm(x)\n",
    "        # (b, out_channels, [z], y, x)\n",
    "\n",
    "        x = rearrange_channels(x, True, channels_first)\n",
    "        # (b, [out_channels], [z], y, x, [out_channels])\n",
    "\n",
    "        return x\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level1(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class CNNBlock3D(_CNNBlock):\n",
    "    \"\"\"A block to perform a sequence of convolution, activation, normalization, and dropout operations.\n",
    "    {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the CNNBlock3D block. Activation checkpointing level 1.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__(3, config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mgroups\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mGroupNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mSiLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CNNBlock3D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    kernel_size=3,\n",
    "    normalization=\"groupnorm\",\n",
    "    normalization_pre_args=[2],\n",
    "    activation=\"silu\",\n",
    "    drop_prob=0.5,\n",
    "    padding=1,\n",
    "    conv_kwargs={\"groups\": 2},\n",
    "    sequence=\"NCDA\",\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConvTranspose3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mgroups\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mPReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnum_parameters\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CNNBlock3D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    kernel_size=4,\n",
    "    normalization=\"batchnorm3d\",\n",
    "    activation=\"prelu\",\n",
    "    drop_prob=0.5,\n",
    "    padding=1,\n",
    "    stride=2,\n",
    "    conv_kwargs={\"groups\": 2},\n",
    "    sequence=\"NDAC\",\n",
    "    transposed=True,\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class CNNBlock2D(_CNNBlock):\n",
    "    \"\"\"A block to perform a sequence of convolution, activation, normalization, and dropout operations.\n",
    "    {CLASS_DESCRIPTION_2D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the CNNBlock2D block. Activation checkpointing level 1.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__(2, config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mCNNBlock2D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mgroups\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mGroupNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mSiLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CNNBlock2D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    kernel_size=3,\n",
    "    normalization=\"groupnorm\",\n",
    "    normalization_pre_args=[2],\n",
    "    activation=\"silu\",\n",
    "    drop_prob=0.5,\n",
    "    padding=1,\n",
    "    conv_kwargs={\"groups\": 2},\n",
    "    sequence=\"NCDA\",\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mCNNBlock2D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConvTranspose2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mgroups\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mPReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnum_parameters\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = CNNBlock2D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    kernel_size=4,\n",
    "    normalization=\"batchnorm2d\",\n",
    "    activation=\"prelu\",\n",
    "    drop_prob=0.5,\n",
    "    padding=1,\n",
    "    stride=2,\n",
    "    conv_kwargs={\"groups\": 2},\n",
    "    sequence=\"NDAC\",\n",
    "    transposed=True,\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class _MultiResCNNBlock(nn.Module):\n",
    "    \"\"\"A block to perform a multi-resolution convolution operation using the cascading convolutions trick. It uses three\n",
    "    different kernel sizes (3, 5, and 7) to capture features at multiple resolutions. The output channels are split\n",
    "    according to the specified filter ratios. The block also includes a residual connection.\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(\n",
    "        self, spatial_dims: Literal[2, 3], config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize the MultiResCNNBlock block. Activation checkpointing level 2.\n",
    "\n",
    "        Args:\n",
    "            spatial_dims: Number of spatial dimensions (2 or 3). This is used to determine 2D vs 3D data\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = MultiResCNNBlockConfig.model_validate(config | kwargs)\n",
    "\n",
    "        assert self.config.kernel_sizes == (3, 5, 7), \"Only kernel sizes of (3, 5, 7) are supported for now\"\n",
    "\n",
    "        all_out_channels = [max(1, int(self.config.out_channels * ratio)) for ratio in self.config.filter_ratios[:-1]]\n",
    "        last_out_channels = self.config.out_channels - sum(all_out_channels)\n",
    "        all_out_channels.append(last_out_channels)\n",
    "        if last_out_channels <= 0:\n",
    "            raise ValueError(\n",
    "                f\"These filter values ({self.config.filter_ratios}) won't work with the given out_channels. Please \"\n",
    "                f\"adjust them. The out_channels of each conv layer is coming out to be {all_out_channels}.\"\n",
    "            )\n",
    "        all_in_channels = [self.config.in_channels] + all_out_channels[:-1]\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                _CNNBlock(\n",
    "                    spatial_dims,\n",
    "                    self.config.model_dump(),\n",
    "                    checkpointing_level,\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                )\n",
    "                for in_channels, out_channels in zip(all_in_channels, all_out_channels)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.residual_conv = _CNNBlock(\n",
    "            spatial_dims,\n",
    "            self.config.model_dump(),\n",
    "            checkpointing_level,\n",
    "            in_channels=self.config.in_channels,\n",
    "            out_channels=self.config.out_channels,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "        self.residual = Residual()\n",
    "\n",
    "        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(self, x: torch.Tensor, channels_first: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the MultiResCNNBlock block.\n",
    "\n",
    "        Args:\n",
    "            x: {INPUT_3D_DOC}\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_3D_DOC}\n",
    "        \"\"\"\n",
    "        # x: (b, [in_channels], [z], y, x, [in_channels])\n",
    "\n",
    "        x = rearrange_channels(x, channels_first, True)\n",
    "        # (b, in_channels, [z], y, x)\n",
    "\n",
    "        residual = self.residual_conv(x)\n",
    "        # (b, out_channels, [z], y, x)\n",
    "\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_input = conv_outputs[-1] if conv_outputs else x\n",
    "            conv_output = conv(conv_input)\n",
    "            conv_outputs.append(conv_output)\n",
    "            # (b, one_of_all_out_channels, [z], y, x)\n",
    "\n",
    "        x = torch.cat(conv_outputs, dim=1)\n",
    "        # (b, out_channels, [z], y, x)\n",
    "\n",
    "        x = self.residual(x, residual)\n",
    "        # (b, out_channels, [z], y, x)\n",
    "\n",
    "        x = rearrange_channels(x, True, channels_first)\n",
    "        # (b, [out_channels], [z], y, x, [out_channels])\n",
    "\n",
    "        return x\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level2(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MultiResCNNBlock3D(_MultiResCNNBlock):\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the MultiResCNNBlock3D block. Activation checkpointing level 2.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__(3, config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMultiResCNNBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconvs\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual_conv\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MultiResCNNBlock3D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    filter_ratios=(3, 2, 1),\n",
    "    activation=\"gelu\",\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MultiResCNNBlock2D(_MultiResCNNBlock):\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize the MultiResCNNBlock2D block. Activation checkpointing level 2.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__(2, config, checkpointing_level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMultiResCNNBlock2D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconvs\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual_conv\u001b[1m)\u001b[0m: \u001b[1;35m_CNNBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mBatchNorm2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33mmomentum\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33maffine\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mtrack_running_stats\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MultiResCNNBlock2D(\n",
    "    in_channels=4,\n",
    "    out_channels=8,\n",
    "    filter_ratios=(3, 2, 1),\n",
    "    activation=\"gelu\",\n",
    "    normalization=\"batchnorm2d\",\n",
    ")\n",
    "display(test)\n",
    "\n",
    "sample_input = torch.randn(2, 4, 16, 16)\n",
    "test(sample_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor splitting inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class TensorSplittingConv(nn.Module):\n",
    "    \"\"\"Convolution layer that operates on splits of a tensor on desired device and concatenates the results to give a\n",
    "    lossless output. This is useful for large input tensors that cause intermediate buffers in the conv layer that\n",
    "    don't fit in memory. Works for both 2D and 3D convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, conv: nn.Module, num_splits: int | tuple[int, ...], optimize_num_splits: bool = True):\n",
    "        \"\"\"Initialize the TensorSplittingConv layer.\n",
    "\n",
    "        Args:\n",
    "            conv: Convolution layer to be used for splitting. Must be either nn.Conv2d or nn.Conv3d.\n",
    "            num_splits: Number of splits for each spatial dimension. If an int is provided, it will be used for all\n",
    "                spatial dimensions. If a tuple is provided, it must have the same length as the number of spatial\n",
    "                dimensions.\n",
    "            optimize_num_splits: Whether to optimize the number of splits based on the input shape. An example of\n",
    "                optimization is provided below. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(conv, nn.Conv2d):\n",
    "            self.spatial_dims = 2\n",
    "        elif isinstance(conv, nn.Conv3d):\n",
    "            self.spatial_dims = 3\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported convolution type. Only Conv2d and Conv3d are supported.\")\n",
    "\n",
    "        assert conv.stride == (1,) * self.spatial_dims, \"Stride must be 1 for tensor splitting convolution.\"\n",
    "        assert conv.padding == \"same\" or torch.allclose(\n",
    "            torch.tensor(conv.padding), (torch.tensor(conv.kernel_size) - 1) // 2\n",
    "        ), \"Padding must be 'same' for tensor splitting convolution.\"\n",
    "\n",
    "        if isinstance(num_splits, int):\n",
    "            num_splits = (num_splits,) * self.spatial_dims\n",
    "        assert len(num_splits) == self.spatial_dims, \"num_splits must be a tuple of length equal to spatial_dims\"\n",
    "\n",
    "        self.conv = conv\n",
    "        self.num_splits = num_splits\n",
    "        self.optimize_num_splits = optimize_num_splits\n",
    "\n",
    "    @cache\n",
    "    def get_receptive_field(self) -> tuple[int, ...]:\n",
    "        \"\"\"Calculate the receptive field of the convolution layer.\"\"\"\n",
    "        kernel_size = torch.tensor(self.conv.kernel_size)\n",
    "        dilation = torch.tensor(self.conv.dilation)\n",
    "        receptive_field = dilation * (kernel_size - 1) + 1\n",
    "        return tuple(receptive_field.tolist())\n",
    "\n",
    "    @cache\n",
    "    def get_edge_context(self):\n",
    "        \"\"\"Calculate the context size required to eliminate edge effects when merging the conv outputs into one.\"\"\"\n",
    "        receptive_field = self.get_receptive_field()\n",
    "        context = torch.tensor(receptive_field) // 2\n",
    "        return tuple(context.tolist())\n",
    "\n",
    "    def get_input_shape(self, input_shape: tuple[int, ...] | torch.Size | torch.Tensor) -> tuple[int, ...]:\n",
    "        \"\"\"Get the input shape of the convolution layer. This function removes any unnecesary dimensions and ensures\n",
    "        that the input shape is of length equal to the number of spatial dimensions.\n",
    "\n",
    "        Args:\n",
    "            input_shape: Shape of the input tensor. If a tensor is provided, its shape will be used.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of the input shape for the convolution layer, with only the spatial dimensions.\n",
    "        \"\"\"\n",
    "        if isinstance(input_shape, torch.Tensor):\n",
    "            input_shape = input_shape.shape\n",
    "        if isinstance(input_shape, torch.Size):\n",
    "            input_shape = tuple(input_shape)\n",
    "        input_shape = input_shape[-self.spatial_dims :]\n",
    "        if len(input_shape) != self.spatial_dims:\n",
    "            raise ValueError(f\"Input shape must be of length {self.spatial_dims}. Got {len(input_shape)}.\")\n",
    "        return input_shape\n",
    "\n",
    "    def get_optimized_num_splits(self, input_shape: tuple[int, ...] | torch.Size | torch.Tensor) -> tuple[int, ...]:\n",
    "        \"\"\"Optimize the number of splits for each dimension based on the input shape and number of splits\n",
    "\n",
    "        Example:\n",
    "            Let's say input shape is (110, 110) and num_splits is (12, 12). The input will first be padded to (120, 120)\n",
    "            and then split into splits of size (10+overlap, 10+overlap) each. However, if you notice, the padding\n",
    "            that was also equal to 10, and therefore was completely unnecessary as the same result can be achieved by\n",
    "            using num_splits = (11, 11) and reducing 144-121=23 splits to be processed.\n",
    "\n",
    "        Args:\n",
    "            input_shape: Shape of the input tensor. If a tensor is provided, its shape will be used.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of optimized number of splits for each dimension.\n",
    "        \"\"\"\n",
    "        input_shape = self.get_input_shape(input_shape)\n",
    "\n",
    "        num_splits = list(self.num_splits)\n",
    "        for i in range(self.spatial_dims):\n",
    "            while True:\n",
    "                padding_required = (num_splits[i] - (input_shape[i] % num_splits[i])) % num_splits[i]\n",
    "                split_size = (input_shape[i] + padding_required) // num_splits[i]\n",
    "                if padding_required >= split_size:\n",
    "                    num_splits[i] -= 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return tuple(num_splits)\n",
    "\n",
    "    def pad_input_for_divisibility(self, x: torch.Tensor, num_splits: tuple[int, ...] = None) -> torch.Tensor:\n",
    "        \"\"\"Pad the input at the end of every spatial dimension such that it is perfectly divisible by the number of\n",
    "        splits.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, in_channels, [z], y, x).\n",
    "            num_splits: Number of splits for each spatial dimension. If None, the default num_splits will be used.\n",
    "\n",
    "        Returns:\n",
    "            Padded input tensor of shape (batch_size, in_channels, [z], y, x).\n",
    "        \"\"\"\n",
    "        if num_splits is None:\n",
    "            num_splits = self.num_splits\n",
    "        padding = [0, 0] * (x.ndim - self.spatial_dims)\n",
    "        for i in range(self.spatial_dims):\n",
    "            dim = i + 2\n",
    "            padding_required = (num_splits[i] - (x.shape[dim] % num_splits[i])) % num_splits[i]\n",
    "            padding.extend([padding_required, 0])\n",
    "        x = F.pad(x, list(reversed(padding)))\n",
    "        return x\n",
    "\n",
    "    def get_split_size(\n",
    "        self, input_shape: tuple[int, ...] | torch.Size | torch.Tensor, num_splits: tuple[int, ...] = None\n",
    "    ) -> tuple[int, ...]:\n",
    "        \"\"\"Calculate the split size for each dimension based on the input shape and number of splits.\n",
    "\n",
    "        Args:\n",
    "            input_shape: Shape of the input tensor. If a tensor is provided, its shape will be used.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of split sizes for each dimension.\n",
    "        \"\"\"\n",
    "        input_shape = self.get_input_shape(input_shape)\n",
    "        if num_splits is None:\n",
    "            num_splits = self.num_splits\n",
    "\n",
    "        context = self.get_edge_context()\n",
    "\n",
    "        split_size = []\n",
    "        for i in range(self.spatial_dims):\n",
    "            split_size.append(input_shape[i] // num_splits[i] + 2 * context[i])\n",
    "        split_size = tuple(split_size)\n",
    "        return split_size\n",
    "\n",
    "    def get_split_stride(\n",
    "        self, input_shape: tuple[int, ...] | torch.Size | torch.Tensor, num_splits: tuple[int, ...] = None\n",
    "    ) -> tuple[int, ...]:\n",
    "        \"\"\"Calculate the split stride for each dimension based on the input shape and context size.\n",
    "\n",
    "        Args:\n",
    "            input_shape: Shape of the input tensor. If a tensor is provided, its shape will be used.\n",
    "            num_splits: Number of splits for each spatial dimension. If None, the default num_splits will be used.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of split strides for each dimension.\n",
    "        \"\"\"\n",
    "        input_shape = self.get_input_shape(input_shape)\n",
    "        if num_splits is None:\n",
    "            num_splits = self.num_splits\n",
    "        context = self.get_edge_context()\n",
    "        split_size = self.get_split_size(input_shape, num_splits)\n",
    "        split_stride = [split_size[i] - 2 * context[i] for i in range(self.spatial_dims)]\n",
    "        assert all(\n",
    "            split_stride[i] > 0 for i in range(self.spatial_dims)\n",
    "        ), \"Split stride must be greater than 0 for all dimensions.\"\n",
    "        return split_stride\n",
    "\n",
    "    def pad_input_for_context(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Pad the input with the context size for consistent merging.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, in_channels, [z], y, x).\n",
    "\n",
    "        Returns:\n",
    "            Padded input tensor of shape (batch_size, in_channels, [z], y, x).\n",
    "        \"\"\"\n",
    "        context = self.get_edge_context()\n",
    "        padding = [0, 0] * (x.ndim - self.spatial_dims)\n",
    "        for i in range(self.spatial_dims):\n",
    "            padding.extend([context[i], context[i]])\n",
    "        x = F.pad(x, list(reversed(padding)))\n",
    "        return x\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(self, x: torch.Tensor, channels_first: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the convolution layer with tensor splitting parallelism. Main convolution occurs on it's\n",
    "             device, but the output is built on the input tensor's device.\n",
    "\n",
    "        Args:\n",
    "            x: {INPUT_3D_DOC}\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_3D_DOC}\n",
    "        \"\"\"\n",
    "        input_device = x.device\n",
    "\n",
    "        x = rearrange_channels(x, channels_first, True)\n",
    "        B, DIMS = x.shape[0], x.shape[2:]  # (batch_size, in_channels, [z], y, x)\n",
    "\n",
    "        # Optimize num_splits\n",
    "        num_splits = self.num_splits\n",
    "        if self.optimize_num_splits:\n",
    "            num_splits = self.get_optimized_num_splits(x)\n",
    "\n",
    "        # Pad input such that it is divisible by the number of splits\n",
    "        x = self.pad_input_for_divisibility(x, num_splits)\n",
    "\n",
    "        # Calculate split size\n",
    "        split_size = self.get_split_size(x, num_splits)\n",
    "\n",
    "        # Identify the stride required to split the input tensor such that overlapping regions can be counted only once\n",
    "        split_stride = self.get_split_stride(x, num_splits)\n",
    "\n",
    "        # Pad the input\n",
    "        x = self.pad_input_for_context(x)\n",
    "\n",
    "        # Split the input tensor\n",
    "        splitter = Splitter(\n",
    "            split_dims=self.spatial_dims,\n",
    "            split_size=split_size,\n",
    "            stride=split_stride,\n",
    "            extend_mode=None,  # Padding has been handled here for better control\n",
    "        )\n",
    "        x = splitter(x)\n",
    "        # (num_splits, batch_size, in_channels, [z1], y1, x1)\n",
    "\n",
    "        # Run the convolution on each split\n",
    "        outputs: list[torch.Tensor] = []\n",
    "        for x_split, position in x:\n",
    "            x_split = x_split.to(self.conv.weight.device)\n",
    "            x_split = self.conv(x_split)\n",
    "            x_split = x_split.to(input_device)\n",
    "            outputs.append((x_split, position))\n",
    "        # list of len num_splits, each of a tuple with the split of shape (batch_size, out_channels, [z1], y1, x1) and\n",
    "        # the position as a tensor\n",
    "\n",
    "        # Merge the outputs\n",
    "        context = self.get_edge_context()\n",
    "        merged = torch.empty((B, outputs[0][0].shape[1], *DIMS), device=input_device)\n",
    "        for output, position in outputs:\n",
    "            merged_slices = [slice(None), slice(None)]  # To track the coordinates where the output will be placed\n",
    "            output_slices = [slice(None), slice(None)]  # To track the actual output that should be placed\n",
    "            for i in range(self.spatial_dims):\n",
    "                dim = i + 2\n",
    "                merged_slice = slice(position[i], min(position[i] + split_stride[i], DIMS[i]))\n",
    "                output_slice = slice(context[i], -context[i] if context[i] != 0 else None)\n",
    "\n",
    "                merged_indices = merged_slice.indices(merged.shape[dim])\n",
    "                output_indices = output_slice.indices(output.shape[dim])\n",
    "                len_merged_slice = merged_indices[1] - merged_indices[0]\n",
    "                len_output_slice = output_indices[1] - output_indices[0]\n",
    "                if len_output_slice > len_merged_slice:\n",
    "                    output_slice = slice(output_slice.start, output_slice.start + len_merged_slice)\n",
    "\n",
    "                merged_slices.append(merged_slice)\n",
    "                output_slices.append(output_slice)\n",
    "\n",
    "            merged[tuple(merged_slices)] = output[tuple(output_slices)]\n",
    "\n",
    "        merged = rearrange_channels(merged, True, channels_first)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"num_splits={self.num_splits}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTensorSplittingConv\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[33mnum_splits\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m7\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m5\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m18\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "conv = nn.Conv3d(in_channels=2, out_channels=4, kernel_size=3, padding=\"same\").eval().to(device)\n",
    "sample_input = torch.randn(2, 2, 16, 18, 16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output1 = conv(sample_input.to(device)).cpu()\n",
    "\n",
    "test = TensorSplittingConv(conv, num_splits=(3, 4, 5))\n",
    "display(test)\n",
    "\n",
    "display(test.get_optimized_num_splits(sample_input))\n",
    "display(test.get_receptive_field())\n",
    "display(test.get_edge_context())\n",
    "display(test.get_split_size(sample_input))\n",
    "display(test.get_split_stride(sample_input))\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output2 = test(sample_input)\n",
    "\n",
    "test_output2.shape, torch.allclose(test_output1, test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTensorSplittingConv\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[33mnum_splits\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m12\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m11\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m11\u001b[0m, \u001b[1;36m13\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m9\u001b[0m, \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m110\u001b[0m, \u001b[1;36m110\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "conv = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=3, padding=\"same\").eval().to(device)\n",
    "sample_input = torch.randn(2, 2, 110, 110)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output1 = conv(sample_input.to(device)).cpu()\n",
    "\n",
    "test = TensorSplittingConv(conv, num_splits=(12, 10))\n",
    "display(test)\n",
    "\n",
    "display(test.get_optimized_num_splits(sample_input))\n",
    "display(test.get_receptive_field())\n",
    "display(test.get_edge_context())\n",
    "display(test.get_split_size(sample_input))\n",
    "display(test.get_split_stride(sample_input))\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_output2 = test(sample_input)\n",
    "\n",
    "test_output2.shape, torch.allclose(test_output1, test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def add_tsp_to_module(\n",
    "    module: nn.Module,\n",
    "    num_splits_2d: int | tuple[int, int] | None = None,\n",
    "    num_splits_3d: int | tuple[int, int, int] = None,\n",
    "    strict: bool = True,\n",
    ") -> nn.Module:\n",
    "    \"\"\"Recursively add TensorSplittingConv to the module for all Conv2d and Conv3d layers.\n",
    "\n",
    "    Args:\n",
    "        module: The module to modify.\n",
    "        num_splits_2d: Number of splits for 2D convolutions. If None, 2D convolutions will not be modified.\n",
    "        num_splits_3d: Number of splits for 3D convolutions. If None, 3D convolutions will not be modified.\n",
    "        strict: Whether to raise an error if a conversion fails. If False, it will log the error and continue.\n",
    "\n",
    "    Returns:\n",
    "        The modified module with TensorSplittingConv layers.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both num_splits_2d and num_splits_3d are None.\n",
    "        Exception: If a conversion fails and strict is True.\n",
    "    \"\"\"\n",
    "\n",
    "    if num_splits_2d is None and num_splits_3d is None:\n",
    "        raise ValueError(\"At least one of num_splits_2d or num_splits_3d must be provided.\")\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, TensorSplittingConv):\n",
    "            continue\n",
    "        if num_splits_2d is not None and isinstance(child, nn.Conv2d):\n",
    "            try:\n",
    "                setattr(module, name, TensorSplittingConv(child, num_splits_2d).to(child.weight.device))\n",
    "            except Exception as e:\n",
    "                if strict:\n",
    "                    raise e\n",
    "                else:\n",
    "                    logger.debug(f\"Could not convert {name} to TensorSplittingConv. Error: {e}\")\n",
    "\n",
    "        if num_splits_3d is not None and isinstance(child, nn.Conv3d):\n",
    "            try:\n",
    "                setattr(module, name, TensorSplittingConv(child, num_splits_3d).to(child.weight.device))\n",
    "            except Exception as e:\n",
    "                if strict:\n",
    "                    raise e\n",
    "                else:\n",
    "                    logger.debug(f\"Could not convert {name} to TensorSplittingConv. Error: {e}\")\n",
    "        else:\n",
    "            add_tsp_to_module(child, num_splits_2d, num_splits_3d, strict)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def remove_tsp_from_module(module: nn.Module) -> nn.Module:\n",
    "    \"\"\"Recursively remove TensorSplittingConv from the module and replace it with the original convolution layer.\n",
    "\n",
    "    Args:\n",
    "        module: The module to modify.\n",
    "\n",
    "    Returns:\n",
    "        The modified module with TensorSplittingConv layers replaced by the original convolution layers.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, TensorSplittingConv):\n",
    "            setattr(module, name, child.conv.to(child.conv.weight.device))\n",
    "        else:\n",
    "            remove_tsp_from_module(child)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMyModule\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv1\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv2\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv3\u001b[1m)\u001b[0m: \u001b[1;35mConvTranspose2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMyModule\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv1\u001b[1m)\u001b[0m: \u001b[1;35mTensorSplittingConv\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mnum_splits\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv2\u001b[1m)\u001b[0m: \u001b[1;35mTensorSplittingConv\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mnum_splits\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv3\u001b[1m)\u001b[0m: \u001b[1;35mConvTranspose2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMyModule\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv1\u001b[1m)\u001b[0m: \u001b[1;35mTensorSplittingConv\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mnum_splits\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv2\u001b[1m)\u001b[0m: \u001b[1;35mTensorSplittingConv\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mnum_splits\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv3\u001b[1m)\u001b[0m: \u001b[1;35mConvTranspose2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMyModule\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv1\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv2\u001b[1m)\u001b[0m: \u001b[1;35mConv3d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[35msame\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv3\u001b[1m)\u001b[0m: \u001b[1;35mConvTranspose2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv3d(3, 16, 3, padding=\"same\")\n",
    "        self.conv3 = nn.ConvTranspose2d(3, 16, 3)\n",
    "\n",
    "\n",
    "my_module = MyModule().to(device)\n",
    "display(my_module)\n",
    "add_tsp_to_module(my_module, num_splits_2d=(2, 3), num_splits_3d=(4, 5, 6))\n",
    "display(my_module)\n",
    "add_tsp_to_module(my_module, num_splits_2d=(2, 3), num_splits_3d=(4, 5, 6))\n",
    "display(my_module)\n",
    "remove_tsp_from_module(my_module)\n",
    "display(my_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMyModule\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv1\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv2\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv3\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-27 19:56:23.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_tsp_to_module\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mCould not convert conv1 to TensorSplittingConv. Error: Stride must be 1 for tensor splitting convolution.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-27 19:56:23.442\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_tsp_to_module\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mCould not convert conv2 to TensorSplittingConv. Error: Padding must be 'same' for tensor splitting convolution.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMyModule\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv1\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv2\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mconv3\u001b[1m)\u001b[0m: \u001b[1;35mTensorSplittingConv\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mnum_splits\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mconv\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mpadding\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(3, 16, 3, padding=2)\n",
    "        self.conv3 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "\n",
    "\n",
    "my_module = MyModule().to(device)\n",
    "display(my_module)\n",
    "add_tsp_to_module(my_module, num_splits_2d=(2, 3), strict=False)\n",
    "display(my_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
