{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp blocks/transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.layers.attention import Attention1D, Attention1DConfig, Attention3D, Attention3DConfig\n",
    "from vision_architectures.layers.embeddings import RelativePositionEmbeddings\n",
    "from vision_architectures.utils.activation_checkpointing import ActivationCheckpointing\n",
    "from vision_architectures.utils.activations import get_act_layer\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field\n",
    "from vision_architectures.utils.rearrange import rearrange_channels\n",
    "from vision_architectures.utils.residuals import Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Attention1DMLPConfig(CustomBaseModel):\n",
    "    dim: int = Field(..., description=\"Dimension of the input and output features.\")\n",
    "    mlp_ratio: int = Field(4, description=\"Ratio of the hidden dimension in the MLP to the input dimension.\")\n",
    "    activation: str = Field(\"gelu\", description=\"Activation function for the MLP.\")\n",
    "    mlp_drop_prob: float = Field(0.0, description=\"Dropout probability for the MLP.\")\n",
    "\n",
    "\n",
    "class Attention3DMLPConfig(Attention1DMLPConfig):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Attention1DWithMLPConfig(Attention1DMLPConfig, Attention1DConfig):\n",
    "    norm_location: Literal[\"pre\", \"post\"] = Field(\n",
    "        \"post\",\n",
    "        description=\"Location of the normalization layer in the attention block. Pre-normalization implies \"\n",
    "        \"normalization before the attention operation, while post-normalization applies it after.\",\n",
    "    )\n",
    "    layer_norm_eps: float = Field(1e-6, description=\"Epsilon value for the layer normalization.\")\n",
    "\n",
    "\n",
    "class Attention3DWithMLPConfig(Attention3DMLPConfig, Attention3DConfig):\n",
    "    norm_location: Literal[\"pre\", \"post\"] = Field(\n",
    "        \"post\",\n",
    "        description=\"Location of the normalization layer in the attention block. Pre-normalization implies \"\n",
    "        \"normalization before the attention operation, while post-normalization applies it after.\",\n",
    "    )\n",
    "    layer_norm_eps: float = Field(1e-6, description=\"Epsilon value for the layer normalization.\")\n",
    "\n",
    "\n",
    "TransformerEncoderBlock1DConfig = Attention1DWithMLPConfig\n",
    "TransformerEncoderBlock3DConfig = Attention3DWithMLPConfig\n",
    "TransformerDecoderBlock1DConfig = Attention1DWithMLPConfig\n",
    "TransformerDecoderBlock3DConfig = Attention3DWithMLPConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class Attention1DMLP(nn.Module):\n",
    "    \"\"\"The MLP that is usually used after performing attention. {CLASS_DESCRIPTION_1D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: Attention1DMLPConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize an Attention1DMLP block. Activation checkpointing level 2.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Attention1DMLPConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        mlp_ratio = self.config.mlp_ratio\n",
    "        activation = self.config.activation\n",
    "        mlp_drop_prob = self.config.mlp_drop_prob\n",
    "\n",
    "        self.dense1 = nn.Linear(dim, dim * mlp_ratio)\n",
    "\n",
    "        if isinstance(activation, nn.Module):\n",
    "            self.act = activation\n",
    "        else:\n",
    "            self.act = get_act_layer(activation)\n",
    "\n",
    "        self.dense2 = nn.Linear(dim * mlp_ratio, dim)\n",
    "        self.dropout = nn.Dropout(mlp_drop_prob)\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)\n",
    "\n",
    "    def _forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Attention1DMLP block.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: {INPUT_1D_DOC}\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_1D_DOC}\n",
    "        \"\"\"\n",
    "\n",
    "        # hidden_states: (b, T, dim)\n",
    "        def first_half(hidden_states):\n",
    "            hidden_states = self.dense1(hidden_states)\n",
    "            hidden_states = self.act(hidden_states)\n",
    "            return hidden_states\n",
    "\n",
    "        def second_half(hidden_states):\n",
    "            hidden_states = self.dense2(hidden_states)\n",
    "            hidden_states = self.dropout(hidden_states)\n",
    "            return hidden_states\n",
    "\n",
    "        hidden_states = self.checkpointing_level1(first_half, hidden_states)\n",
    "        hidden_states = self.checkpointing_level1(second_half, hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level2(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m28\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention1DMLP(dim=64, activation=\"relu\", mlp_drop_prob=0.2)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 28, 64)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class Attention3DMLP(Attention1DMLP):\n",
    "    \"\"\"The MLP that is usually used after performing attention. {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: Attention3DMLPConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize an Attention3DMLP block. Activation checkpointing level 2.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__(config, checkpointing_level, **kwargs)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(self, hidden_states: torch.Tensor, channels_first: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Attention3DMLP block.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: {INPUT_3D_DOC}\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_3D_DOC}\n",
    "        \"\"\"\n",
    "        # hidden_states: (b, dim, z, y, x) or (b, z, y, x, dim)\n",
    "        hidden_states = rearrange_channels(hidden_states, channels_first, False)\n",
    "        hidden_states = super()._forward(hidden_states)\n",
    "        hidden_states = rearrange_channels(hidden_states, False, channels_first)\n",
    "        return hidden_states\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level2(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m256\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention3DMLP(dim=64)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 64, 4, 4, 4)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class Attention1DWithMLP(nn.Module):\n",
    "    \"\"\"An attention block with an MLP. {CLASS_DESCRIPTION_1D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Attention1DWithMLPConfig = {},\n",
    "        relative_position_bias: RelativePositionEmbeddings | None = None,\n",
    "        logit_scale: float | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize an Attention1DWithMLP block. Activation checkpointing level 3.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            relative_position_bias: {RELATIVE_POSITION_BIAS_DOC}\n",
    "            logit_scale: {LOGIT_SCALE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Attention1DWithMLPConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim_qk = self.config.dim_qk\n",
    "        layer_norm_eps = self.config.layer_norm_eps\n",
    "\n",
    "        self.attn = Attention1D(\n",
    "            self.config,\n",
    "            relative_position_bias=relative_position_bias,\n",
    "            logit_scale=logit_scale,\n",
    "            checkpointing_level=checkpointing_level,\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)\n",
    "        self.mlp = Attention1DMLP(self.config, checkpointing_level=checkpointing_level)\n",
    "        self.layernorm2 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)\n",
    "\n",
    "        self.residual = Residual()\n",
    "\n",
    "        self.checkpointing_level3 = ActivationCheckpointing(3, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Attention1DWithMLP block.\n",
    "\n",
    "        Args:\n",
    "            query: {INPUT_1D_DOC}\n",
    "            key: {INPUT_1D_DOC}\n",
    "            value: {INPUT_1D_DOC}\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_1D_DOC}\n",
    "        \"\"\"\n",
    "        # Each is (b, T, dim)\n",
    "        res_connection1 = query\n",
    "        # (b, T, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            query = self.layernorm1(query)\n",
    "            key = self.layernorm1(key)\n",
    "            value = self.layernorm1(value)\n",
    "            # (b, T, dim)\n",
    "\n",
    "        hidden_states = self.attn(query, key, value)\n",
    "        # (b, T, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm1(hidden_states)\n",
    "            # (b, T, dim)\n",
    "\n",
    "        hidden_states = self.residual(hidden_states, res_connection1)\n",
    "        res_connection2 = hidden_states\n",
    "        # (b, T, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, T, dim)\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # (b, T, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, T, dim)\n",
    "\n",
    "        hidden_states = self.residual(hidden_states, res_connection2)\n",
    "        # (b, T, dim)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level3(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention1DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m216\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m216\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention1DWithMLP(dim=54, num_heads=3)\n",
    "output = torch.randn(2, 64, 54)\n",
    "\n",
    "display(test)\n",
    "display(test(output, output, output).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class Attention3DWithMLP(nn.Module):\n",
    "    \"\"\"An attention block with an MLP. {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Attention3DWithMLPConfig = {},\n",
    "        relative_position_bias: RelativePositionEmbeddings | None = None,\n",
    "        logit_scale: float | None = None,\n",
    "        checkpointing_level: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize an Attention3DWithMLP block. Activation checkpointing level 3.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            relative_position_bias: {RELATIVE_POSITION_BIAS_DOC}\n",
    "            logit_scale: {LOGIT_SCALE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Attention3DWithMLPConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim_qk = self.config.dim_qk\n",
    "        layer_norm_eps = self.config.layer_norm_eps\n",
    "\n",
    "        self.attn = Attention3D(\n",
    "            self.config,\n",
    "            relative_position_bias=relative_position_bias,\n",
    "            logit_scale=logit_scale,\n",
    "            checkpointing_level=checkpointing_level,\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)\n",
    "        self.mlp = Attention3DMLP(self.config, checkpointing_level=checkpointing_level)\n",
    "        self.layernorm2 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)\n",
    "\n",
    "        self.residual = Residual()\n",
    "\n",
    "        self.checkpointing_level3 = ActivationCheckpointing(3, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        channels_first: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Attention3DWithMLP block.\n",
    "\n",
    "        Args:\n",
    "            query: {INPUT_3D_DOC}\n",
    "            key: {INPUT_3D_DOC}\n",
    "            value: {INPUT_3D_DOC}\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_3D_DOC}\n",
    "        \"\"\"\n",
    "        # Each is (b, [dim], tokens_z, tokens_y, tokens_x, [dim])\n",
    "\n",
    "        query = rearrange_channels(query, channels_first, False)\n",
    "        key = rearrange_channels(key, channels_first, False)\n",
    "        value = rearrange_channels(value, channels_first, False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        res_connection1 = query\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            query = self.layernorm1(query)\n",
    "            key = self.layernorm1(key)\n",
    "            value = self.layernorm1(value)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.attn(query, key, value, channels_first=False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm1(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.residual(hidden_states, res_connection1)\n",
    "        res_connection2 = hidden_states\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states, channels_first=False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.residual(hidden_states, res_connection2)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = rearrange_channels(hidden_states, False, channels_first)\n",
    "        # (b, [dim], tokens_z, tokens_y, tokens_x, [dim])\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level3(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mAttention3DWithMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m216\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m216\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m54\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Attention3DWithMLP(dim=54, num_heads=3)\n",
    "output = torch.randn(2, 54, 4, 4, 4)\n",
    "\n",
    "display(test)\n",
    "display(test(output, output, output, channels_first=True).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class TransformerEncoderBlock1D(Attention1DWithMLP):\n",
    "    \"\"\"A self attention transformer block. {CLASS_DESCRIPTION_1D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.Tensor | None = None,\n",
    "        *args,\n",
    "        q: torch.Tensor | None = None,\n",
    "        k: torch.Tensor | None = None,\n",
    "        v: torch.Tensor | None = None,\n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the TransformerEncoderBlock1D block. Activation checkpointing level 3.\n",
    "\n",
    "        Args:\n",
    "            qkv: {INPUT_1D_DOC} If provided, the same tensor is used for query, key, and value. Else, `q`, `k`, and `v`\n",
    "                are considered.\n",
    "            q: {INPUT_1D_DOC} This is used only if `qkv` is not provided. This represents queries and is required.\n",
    "            k: {INPUT_1D_DOC} This is used only if `qkv` is not provided. This represents keys. If not provided, it is\n",
    "                assumed to be the same as `q`.\n",
    "            v: {INPUT_1D_DOC} This is used only if `qkv` is not provided. This represents values. If not provided, it is\n",
    "                assumed to be the same as `k`.\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_1D_DOC}\n",
    "        \"\"\"\n",
    "        if qkv is not None:\n",
    "            q, k, v = qkv, qkv, qkv\n",
    "        else:\n",
    "            if q is None:\n",
    "                raise ValueError(\"If `qkv` is not provided, `q` must be provided.\")\n",
    "            if k is None:\n",
    "                k = q\n",
    "            if v is None:\n",
    "                v = k\n",
    "\n",
    "        # qkv: (b, num_tokens, dim)\n",
    "        return super().forward(q, k, v, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTransformerEncoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = TransformerEncoderBlock1D(dim=54, num_heads=6, mlp_ratio=2)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 64, 54))\n",
    "o = test(q=torch.randn(2, 64, 54), k=torch.randn(2, 64, 54))\n",
    "o = test(q=torch.randn(2, 64, 54), k=torch.randn(2, 64, 54), v=torch.randn(2, 64, 54))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class TransformerEncoderBlock3D(Attention3DWithMLP):\n",
    "    \"\"\"A self attention transformer block. {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.Tensor | None = None,\n",
    "        *args,\n",
    "        q: torch.Tensor | None = None,\n",
    "        k: torch.Tensor | None = None,\n",
    "        v: torch.Tensor | None = None,\n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the TransformerEncoderBlock3D block. Activation checkpointing level 3.\n",
    "\n",
    "        Args:\n",
    "            qkv: {INPUT_3D_DOC} If provided, the same tensor is used for query, key, and value. Else, `q`, `k`, and `v`\n",
    "                are considered.\n",
    "            q: {INPUT_3D_DOC} This is used only if `qkv` is not provided. This represents queries and is required.\n",
    "            k: {INPUT_3D_DOC} This is used only if `qkv` is not provided. This represents keys. If not provided, it is\n",
    "                assumed to be the same as `q`.\n",
    "            v: {INPUT_3D_DOC} This is used only if `qkv` is not provided. This represents values. If not provided, it is\n",
    "                assumed to be the same as `k`.\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_3D_DOC}\n",
    "        \"\"\"\n",
    "        if qkv is not None:\n",
    "            q, k, v = qkv, qkv, qkv\n",
    "        else:\n",
    "            if q is None:\n",
    "                raise ValueError(\"If `qkv` is not provided, `q` must be provided.\")\n",
    "            if k is None:\n",
    "                k = q\n",
    "            if v is None:\n",
    "                v = k\n",
    "\n",
    "        # qkv: (b, [dim], tokens_z, tokens_y, tokens_x, [dim])\n",
    "        return super().forward(q, k, v, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTransformerEncoderBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m108\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m54\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m54\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = TransformerEncoderBlock3D(dim=54, num_heads=6, mlp_ratio=2)\n",
    "display(test)\n",
    "o = test(torch.randn(2, 54, 4, 4, 4))\n",
    "display(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class TransformerDecoderBlock1D(nn.Module):\n",
    "    \"\"\"A cross attention transformer block. {CLASS_DESCRIPTION_1D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: Attention1DWithMLPConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize a TransformerDecoderBlock1D block. Activation checkpointing level 3.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Attention1DWithMLPConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        num_heads = self.config.num_heads\n",
    "        mlp_ratio = self.config.mlp_ratio\n",
    "        layer_norm_eps = self.config.layer_norm_eps\n",
    "        attn_drop_prob = self.config.attn_drop_prob\n",
    "        proj_drop_prob = self.config.proj_drop_prob\n",
    "        mlp_drop_prob = self.config.mlp_drop_prob\n",
    "\n",
    "        self.attn1 = Attention1D(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            attn_drop_prob=attn_drop_prob,\n",
    "            proj_drop_prob=proj_drop_prob,\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(dim, eps=layer_norm_eps)\n",
    "        self.attn2 = Attention1D(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            attn_drop_prob=attn_drop_prob,\n",
    "            proj_drop_prob=proj_drop_prob,\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(dim, eps=layer_norm_eps)\n",
    "        self.mlp = Attention1DMLP(dim=dim, mlp_ratio=mlp_ratio, mlp_drop_prob=mlp_drop_prob)\n",
    "        self.layernorm3 = nn.LayerNorm(dim, eps=layer_norm_eps)\n",
    "\n",
    "        self.residual = Residual()\n",
    "\n",
    "        self.checkpointing_level3 = ActivationCheckpointing(3, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self,\n",
    "        q: torch.Tensor | None = None,\n",
    "        kv: torch.Tensor | None = None,\n",
    "        *,\n",
    "        q1: torch.Tensor | None = None,\n",
    "        k1: torch.Tensor | None = None,\n",
    "        v1: torch.Tensor | None = None,\n",
    "        k2: torch.Tensor | None = None,\n",
    "        v2: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the TransformerDecoderBlock1D block.\n",
    "\n",
    "        Args:\n",
    "            q: {INPUT_1D_DOC} The query tensor used for self-attention. Either this or `q1` should be provided.\n",
    "            kv: {INPUT_1D_DOC} The key and value tensors used for cross-attention. Either this or `k1` and/or `v1`\n",
    "                should be provided.\n",
    "            q1: {INPUT_1D_DOC} The query tensor used for self-attention. Either this or `q` should be provided.\n",
    "            k1: {INPUT_1D_DOC} The key tensor used for self-attention. If not provided, this defaults to `q` or `q1`.\n",
    "            v1: {INPUT_1D_DOC} The value tensor used for self-attention. If not provided, this defaults to `q` or `q1`.\n",
    "            k2: {INPUT_1D_DOC} The key tensor used for cross-attention. Either this or `kv` should be provided.\n",
    "            v2: {INPUT_1D_DOC} The value tensor used for cross-attention. If not provided, this defaults to `kv` or `k2`.\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_1D_DOC}\n",
    "        \"\"\"\n",
    "        # q, q1: (b, num_tokens_in_q, dim)\n",
    "        # kv, k1, k2, v1, v2: (b, num_tokens_in_kv, dim)\n",
    "\n",
    "        # Set parameters for self-attention\n",
    "        if q1 is None:\n",
    "            q1 = q\n",
    "        if k1 is None:\n",
    "            k1 = q1\n",
    "        if v1 is None:\n",
    "            v1 = k1\n",
    "\n",
    "        # Set parameters for cross-attention\n",
    "        if k2 is None:\n",
    "            k2 = kv\n",
    "        if v2 is None:\n",
    "            v2 = k2\n",
    "\n",
    "        res_connection1 = q1\n",
    "        # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            q1 = self.layernorm1(q1)\n",
    "            # (b, num_tokens_in_q, dim)\n",
    "            k1 = self.layernorm1(k1)\n",
    "            # (b, num_tokens_in_kv, dim)\n",
    "            v1 = self.layernorm1(v1)\n",
    "            # (b, num_tokens_in_kv, dim)\n",
    "\n",
    "        q2 = self.attn1(q1, k1, v1)\n",
    "        # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            q2 = self.layernorm1(q2)\n",
    "            # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        q2 = self.residual(q2, res_connection1)\n",
    "        res_connection2 = q2\n",
    "        # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            q2 = self.layernorm2(q2)\n",
    "            # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        hidden_states = self.attn2(q2, k2, v2)\n",
    "        # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        hidden_states = self.residual(hidden_states, res_connection2)\n",
    "        res_connection3 = hidden_states\n",
    "        # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            hidden_states = self.layernorm3(hidden_states)\n",
    "            # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm3(hidden_states)\n",
    "            # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        hidden_states = self.residual(hidden_states, res_connection3)\n",
    "        # (b, num_tokens_in_q, dim)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level3(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTransformerDecoderBlock1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m52\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention1D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m52\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention1DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m104\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m104\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m52\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m52\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m52\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m52\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = TransformerDecoderBlock1D(dim=52, num_heads=4, mlp_ratio=2)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 64, 52), torch.randn(2, 64, 52)).shape)\n",
    "display(test(q1=torch.randn(2, 64, 52), k2=torch.randn(2, 64, 52)).shape)\n",
    "display(test(q1=torch.randn(2, 64, 52), k2=torch.randn(2, 64, 52), v2=torch.randn(2, 64, 52)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@populate_docstring\n",
    "class TransformerDecoderBlock3D(nn.Module):\n",
    "    \"\"\"A cross attention transformer block. {CLASS_DESCRIPTION_3D_DOC}\"\"\"\n",
    "\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: Attention3DWithMLPConfig = {}, checkpointing_level: int = 0, **kwargs):\n",
    "        \"\"\"Initialize a TransformerDecoderBlock3D block. Activation checkpointing level 3.\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = Attention3DWithMLPConfig.model_validate(config | kwargs)\n",
    "\n",
    "        dim = self.config.dim\n",
    "        num_heads = self.config.num_heads\n",
    "        mlp_ratio = self.config.mlp_ratio\n",
    "        layer_norm_eps = self.config.layer_norm_eps\n",
    "        attn_drop_prob = self.config.attn_drop_prob\n",
    "        proj_drop_prob = self.config.proj_drop_prob\n",
    "        mlp_drop_prob = self.config.mlp_drop_prob\n",
    "\n",
    "        self.attn1 = Attention3D(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            attn_drop_prob=attn_drop_prob,\n",
    "            proj_drop_prob=proj_drop_prob,\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(dim, eps=layer_norm_eps)\n",
    "        self.attn2 = Attention3D(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            attn_drop_prob=attn_drop_prob,\n",
    "            proj_drop_prob=proj_drop_prob,\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(dim, eps=layer_norm_eps)\n",
    "        self.mlp = Attention3DMLP(dim=dim, mlp_ratio=mlp_ratio, mlp_drop_prob=mlp_drop_prob)\n",
    "        self.layernorm3 = nn.LayerNorm(dim, eps=layer_norm_eps)\n",
    "\n",
    "        self.residual = Residual()\n",
    "\n",
    "        self.checkpointing_level3 = ActivationCheckpointing(3, checkpointing_level)\n",
    "\n",
    "    @populate_docstring\n",
    "    def _forward(\n",
    "        self,\n",
    "        q: torch.Tensor | None = None,\n",
    "        kv: torch.Tensor | None = None,\n",
    "        *,\n",
    "        q1: torch.Tensor | None = None,\n",
    "        k1: torch.Tensor | None = None,\n",
    "        v1: torch.Tensor | None = None,\n",
    "        k2: torch.Tensor | None = None,\n",
    "        v2: torch.Tensor | None = None,\n",
    "        channels_first: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the TransformerDecoderBlock3D block.\n",
    "\n",
    "        Args:\n",
    "            q: {INPUT_3D_DOC} The query tensor used for self-attention. Either this or `q1` should be provided.\n",
    "            kv: {INPUT_3D_DOC} The key and value tensors used for cross-attention. Either this or `k1` and/or `v1`\n",
    "                should be provided.\n",
    "            q1: {INPUT_3D_DOC} The query tensor used for self-attention. Either this or `q` should be provided.\n",
    "            k1: {INPUT_3D_DOC} The key tensor used for self-attention. If not provided, this defaults to `q` or `q1`.\n",
    "            v1: {INPUT_3D_DOC} The value tensor used for self-attention. If not provided, this defaults to `q` or `q1`.\n",
    "            k2: {INPUT_3D_DOC} The key tensor used for cross-attention. Either this or `kv` should be provided.\n",
    "            v2: {INPUT_3D_DOC} The value tensor used for cross-attention. If not provided, this defaults to `kv` or `k2`.\n",
    "            channels_first: {CHANNELS_FIRST_DOC}\n",
    "\n",
    "        Returns:\n",
    "            {OUTPUT_3D_DOC}\n",
    "        \"\"\"\n",
    "        # Each is (b, [dim], tokens_z, tokens_y, tokens_x, [dim])\n",
    "\n",
    "        # Set parameters for self-attention\n",
    "        if q1 is None:\n",
    "            q1 = q\n",
    "        if k1 is None:\n",
    "            k1 = q1\n",
    "        if v1 is None:\n",
    "            v1 = k1\n",
    "\n",
    "        # Set parameters for cross-attention\n",
    "        if k2 is None:\n",
    "            k2 = kv\n",
    "        if v2 is None:\n",
    "            v2 = k2\n",
    "\n",
    "        q1 = rearrange_channels(q1, channels_first, False)\n",
    "        k1 = rearrange_channels(k1, channels_first, False)\n",
    "        v1 = rearrange_channels(v1, channels_first, False)\n",
    "        k2 = rearrange_channels(k2, channels_first, False)\n",
    "        v2 = rearrange_channels(v2, channels_first, False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        res_connection1 = q1\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            q1 = self.layernorm1(q1)\n",
    "            k1 = self.layernorm1(k1)\n",
    "            v1 = self.layernorm1(v1)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        q2: torch.Tensor = self.attn1(q1, k1, v1, channels_first=False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            q2 = self.layernorm1(q2)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        q2 = self.residual(q2, res_connection1)\n",
    "        res_connection2 = q2\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            q2 = self.layernorm2(q2)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.attn2(q2, k2, v2, channels_first=False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm2(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.residual(hidden_states, res_connection2)\n",
    "        res_connection3 = hidden_states\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"pre\":\n",
    "            hidden_states = self.layernorm3(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states, channels_first=False)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        if self.config.norm_location == \"post\":\n",
    "            hidden_states = self.layernorm3(hidden_states)\n",
    "            # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = self.residual(hidden_states, res_connection3)\n",
    "        # (b, tokens_z, tokens_y, tokens_x, dim)\n",
    "\n",
    "        hidden_states = rearrange_channels(hidden_states, False, channels_first)\n",
    "        # (b, [dim], tokens_z, tokens_y, tokens_x, [dim])\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    @wraps(_forward)\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.checkpointing_level3(self._forward, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTransformerDecoderBlock3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn1\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m52\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mattn2\u001b[1m)\u001b[0m: \u001b[1;35mAttention3D\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_k\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mW_v\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m52\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mAttention3DMLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m104\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdense2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m104\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m52\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level1\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mcheckpointing_level2\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mlayernorm3\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m52\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-06\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mresidual\u001b[1m)\u001b[0m: \u001b[1;35mResidual\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcheckpointing_level3\u001b[1m)\u001b[0m: \u001b[1;35mActivationCheckpointing\u001b[0m\u001b[1m(\u001b[0m\u001b[33menabled\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m52\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m52\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m52\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = TransformerDecoderBlock3D(dim=52, num_heads=4, mlp_ratio=2)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 52, 4, 4, 4), torch.randn(2, 52, 4, 4, 4)).shape)\n",
    "display(test(torch.randn(2, 52, 4, 4, 4), k2=torch.randn(2, 52, 4, 4, 4)).shape)\n",
    "display(test(torch.randn(2, 52, 4, 4, 4), k2=torch.randn(2, 52, 4, 4, 4), v2=torch.randn(2, 52, 4, 4, 4)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
