{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils/activation_checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ActivationCheckpointing:\n",
    "    \"\"\"Activation checkpointing levels:\n",
    "    Level 0: No checkpointing\n",
    "    Level 1: Single layers are checkpointed e.g. linear layer + activation, conv layer + dropout\n",
    "    Level 2: Small blocks are checkpointed e.g. residual blocks, attention blocks, MLP blocks\n",
    "    Level 3: Medium-sized modules are checkpointed e.g. transformer layers, decoder blocks\n",
    "    Level 4: Large modules are checkpointed e.g. groups of transformer layers, decoder stages\n",
    "    Level 5: Very large modules are checkpointed e.g. entire encoders, decoders etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fn_checkpoint_level: int, training_checkpoint_level: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.perform_checkpointing = fn_checkpoint_level <= training_checkpoint_level\n",
    "\n",
    "    def __call__(self, fn: Callable, *args, **kwargs):\n",
    "        if self.perform_checkpointing:\n",
    "            return checkpoint(lambda: fn(*args, **kwargs), use_reentrant=False)\n",
    "        return fn(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test memory savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleModule(nn.Module):\n",
    "    def __init__(self, checkpointing_level):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequences_level1 = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10), nn.ReLU()) for I in range(100)]\n",
    "        )\n",
    "\n",
    "        self.sequences_level2 = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10), nn.ReLU()) for I in range(100)]\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)\n",
    "\n",
    "    def run_sequences(self, x):\n",
    "        sequence_out = x\n",
    "        for sequence_level1, sequence_level2 in zip(self.sequences_level1, self.sequences_level2):\n",
    "            sequence_out = self.checkpointing_level1(sequence_level1, sequence_out)\n",
    "            sequence_out = sequence_level2(sequence_out)\n",
    "        return sequence_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.checkpointing_level2(self.run_sequences, x)\n",
    "\n",
    "    def loss_fn(self, output):  # Arbitrary value so that we can run backward\n",
    "        return output.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(50000, 10, requires_grad=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation checkpointing level = 2\n",
      "Memory used:  0.20971155166625977 GB\n",
      "Time taken for backward:  0.14360837265849113 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "model = SampleModule(2).cuda()\n",
    "print(\"Activation checkpointing level = 2\")\n",
    "\n",
    "output = model(sample_input)\n",
    "\n",
    "print(\"Memory used: \", torch.cuda.max_memory_allocated() / 2**30, \"GB\")\n",
    "\n",
    "loss = model.loss_fn(output)\n",
    "tic = perf_counter()\n",
    "loss.backward()\n",
    "toc = perf_counter()\n",
    "print(\"Time taken for backward: \", toc - tic, 's')\n",
    "\n",
    "del model, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation checkpointing level = 1\n",
      "Memory used:  0.6066136360168457 GB\n",
      "Time taken for backward:  0.07752787880599499 s\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "model = SampleModule(1).cuda()\n",
    "print(\"Activation checkpointing level = 1\")\n",
    "\n",
    "output = model(sample_input)\n",
    "\n",
    "print(\"Memory used: \", torch.cuda.max_memory_allocated() / 2**30, \"GB\")\n",
    "\n",
    "loss = model.loss_fn(output)\n",
    "tic = perf_counter()\n",
    "loss.backward()\n",
    "toc = perf_counter()\n",
    "print(\"Time taken for backward: \", toc - tic, 's')\n",
    "\n",
    "del model, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation checkpointing level = 0\n",
      "Memory used:  0.8019261360168457 GB\n",
      "Time taken for backward:  0.035065365955233574 s\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "model = SampleModule(0).cuda()\n",
    "print(\"Activation checkpointing level = 0\")\n",
    "\n",
    "output = model(sample_input)\n",
    "\n",
    "print(\"Memory used: \", torch.cuda.max_memory_allocated() / 2**30, \"GB\")\n",
    "\n",
    "loss = model.loss_fn(output)\n",
    "tic = perf_counter()\n",
    "loss.backward()\n",
    "toc = perf_counter()\n",
    "print(\"Time taken for backward: \", toc - tic, 's')\n",
    "\n",
    "del model, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
