{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils/activation_checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ActivationCheckpointing(nn.Module):\n",
    "    \"\"\"This class is used to perform activation checkpointing during training. Users can set a level of checkpointing\n",
    "    for each module / function in their architecture. While training, the module / function will be checkpointed if the\n",
    "    training checkpoint level is greater than or equal to the checkpoint level set for the module / function.\n",
    "\n",
    "    A general guide of the Activation checkpointing levels in this repository:\n",
    "\n",
    "    - **Level 0**: No checkpointing\n",
    "    - **Level 1**: Single layers are checkpointed e.g. linear layer + activation, conv layer + dropout\n",
    "    - **Level 2**: Small blocks are checkpointed e.g. residual blocks, attention blocks, MLP blocks\n",
    "    - **Level 3**: Medium-sized modules are checkpointed e.g. transformer layers, decoder blocks\n",
    "    - **Level 4**: Large modules are checkpointed e.g. groups of transformer layers, decoder stages\n",
    "    - **Level 5**: Very large modules are checkpointed e.g. entire encoders, decoders etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fn_checkpoint_level: int, training_checkpoint_level: int):\n",
    "        \"\"\"Initialize the ActivationCheckpointing class.\n",
    "\n",
    "        Args:\n",
    "            fn_checkpoint_level: Level at which the module / function should be checkpointed\n",
    "            training_checkpoint_level: Checkpointing level at which the model is being trained\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                class MyModel(nn.Module):\n",
    "                    def __init__(self, training_checkpointing_level: int = 0):\n",
    "                        super().__init__()\n",
    "                        my_network = nn.Sequential(\n",
    "                            nn.Linear(784, 256),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(256, 10)\n",
    "                        )\n",
    "\n",
    "                        self.activation_checkpointing_level2 = ActivationCheckpointing(2, training_checkpointing_level)\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        y = self.activation_checkpointing_level2(self.my_network, x)\n",
    "                        return y\n",
    "\n",
    "            In this example, a ``training_checkpointing_level`` of greater than or equal to 2 will checkpoint ``my_network``\n",
    "            during training. If it's less than 2, the network will not be checkpointed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fn_checkpoint_level = fn_checkpoint_level\n",
    "        self.training_checkpoint_level = training_checkpoint_level\n",
    "\n",
    "        self.perform_checkpointing = fn_checkpoint_level <= training_checkpoint_level\n",
    "\n",
    "    def __call__(self, fn: Callable, *fn_args, use_reentrant: bool = False, **fn_kwargs):\n",
    "        \"\"\"Checkpoint the module / function if the checkpointing level is greater than or equal to the training\n",
    "        checkpoint level.\n",
    "\n",
    "        Args:\n",
    "            fn: The module / function to checkpoint\n",
    "            use_reentrant: Passed on to torch.utils.checkpoint.checkpoint. Defaults to False.\n",
    "            *fn_args: Arguments to pass to the module / function\n",
    "            **fn_kwargs: Keyword arguments to pass to the module / function\n",
    "\n",
    "        Returns:\n",
    "            The checkpointed module / function if checkpointing is performed, else the module / function itself.\n",
    "        \"\"\"\n",
    "        if self.training and self.perform_checkpointing:\n",
    "            return checkpoint(lambda: fn(*fn_args, **fn_kwargs), use_reentrant=use_reentrant)\n",
    "        return fn(*fn_args, **fn_kwargs)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"enabled={self.perform_checkpointing}, checkpointing_level={self.fn_checkpoint_level}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test memory savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleModule(nn.Module):\n",
    "    def __init__(self, checkpointing_level):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequences_level1 = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10), nn.ReLU()) for _ in range(100)]\n",
    "        )\n",
    "\n",
    "        self.sequences_level2 = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10), nn.ReLU()) for _ in range(100)]\n",
    "        )\n",
    "\n",
    "        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)\n",
    "        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)\n",
    "\n",
    "    def run_sequences(self, x):\n",
    "        sequence_out = x\n",
    "        for sequence_level1, sequence_level2 in zip(self.sequences_level1, self.sequences_level2):\n",
    "            sequence_out = self.checkpointing_level1(sequence_level1, sequence_out)\n",
    "            sequence_out = sequence_level2(sequence_out)\n",
    "        return sequence_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.checkpointing_level2(self.run_sequences, x)\n",
    "\n",
    "    def loss_fn(self, output):  # Arbitrary value so that we can run backward\n",
    "        return output.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(50000, 10, requires_grad=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation checkpointing level = 2\n",
      "Memory used:  0.2188282012939453 GB\n",
      "Time taken for backward:  0.08570726797915995 s\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "model = SampleModule(2).cuda()\n",
    "print(\"Activation checkpointing level = 2\")\n",
    "\n",
    "output = model(sample_input)\n",
    "\n",
    "print(\"Memory used: \", torch.cuda.max_memory_allocated() / 2**30, \"GB\")\n",
    "\n",
    "loss = model.loss_fn(output)\n",
    "tic = perf_counter()\n",
    "loss.backward()\n",
    "toc = perf_counter()\n",
    "print(\"Time taken for backward: \", toc - tic, \"s\")\n",
    "\n",
    "del model, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation checkpointing level = 1\n",
      "Memory used:  0.6066136360168457 GB\n",
      "Time taken for backward:  0.07214503700379282 s\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "model = SampleModule(1).cuda()\n",
    "print(\"Activation checkpointing level = 1\")\n",
    "\n",
    "output = model(sample_input)\n",
    "\n",
    "print(\"Memory used: \", torch.cuda.max_memory_allocated() / 2**30, \"GB\")\n",
    "\n",
    "loss = model.loss_fn(output)\n",
    "tic = perf_counter()\n",
    "loss.backward()\n",
    "toc = perf_counter()\n",
    "print(\"Time taken for backward: \", toc - tic, \"s\")\n",
    "\n",
    "del model, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation checkpointing level = 0\n",
      "Memory used:  0.8019261360168457 GB\n",
      "Time taken for backward:  0.03969525604043156 s\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "model = SampleModule(0).cuda()\n",
    "print(\"Activation checkpointing level = 0\")\n",
    "\n",
    "output = model(sample_input)\n",
    "\n",
    "print(\"Memory used: \", torch.cuda.max_memory_allocated() / 2**30, \"GB\")\n",
    "\n",
    "loss = model.loss_fn(output)\n",
    "tic = perf_counter()\n",
    "loss.backward()\n",
    "toc = perf_counter()\n",
    "print(\"Time taken for backward: \", toc - tic, \"s\")\n",
    "\n",
    "del model, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
