{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils/normalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DyT(nn.Module):\n",
    "    # As proposed in Transformers without Normalization: https://arxiv.org/pdf/2503.10622\n",
    "\n",
    "    def __init__(self, normalized_shape: int | list[int], alpha0: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.alpha0 = alpha0\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.tensor([alpha0]))\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape, dtype=torch.float32))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.tanh(self.alpha * x)\n",
    "        x = x * self.weight + self.bias\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"normalized_shape={self.normalized_shape}, alpha0={self.alpha0}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mDyT\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnormalized_shape\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m20\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m, \u001b[33malpha0\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m-1\u001b[0m., \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMinBackward1\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;36m1.0000\u001b[0m\u001b[39m, \u001b[0m\u001b[33mgrad_fn\u001b[0m\u001b[39m=<MaxBackward1\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input = 5 * torch.randn(1, 30, 20, 10)\n",
    "test = DyT((20, 10))\n",
    "output = test(sample_input)\n",
    "test, output.shape, output.min(), output.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_norm_layer(normalization_name: str, *args, **kwargs):\n",
    "    if normalization_name == \"layernorm\":\n",
    "        norm_layer = nn.LayerNorm(*args, **kwargs)\n",
    "    elif normalization_name == \"batchnorm\" or normalization_name == \"batchnorm1d\":\n",
    "        norm_layer = nn.BatchNorm1d(*args, **kwargs)\n",
    "    elif normalization_name == \"batchnorm2d\":\n",
    "        norm_layer = nn.BatchNorm2d(*args, **kwargs)\n",
    "    elif normalization_name == \"batchnorm3d\":\n",
    "        norm_layer = nn.BatchNorm3d(*args, **kwargs)\n",
    "    elif normalization_name == \"dyt\":\n",
    "        norm_layer = DyT(*args, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Normalization {normalization_name} not implemented\")\n",
    "\n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m10\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_norm_layer(\"layernorm\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
