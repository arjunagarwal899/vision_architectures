{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b20752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp image_readers/safetensors_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c09c5e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from monai.data import ImageReader, MetaTensor, is_supported_format\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfee9e",
   "metadata": {},
   "source": [
    "# Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aece7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SafetensorsReader(ImageReader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_key: str,\n",
    "        spacing_key: str | None = None,\n",
    "        other_keys: set[str] | None = None,\n",
    "        add_channel_dim: bool = True,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        \"\"\"Reader for Safetensors image files.\n",
    "\n",
    "        Args:\n",
    "            iamge_key: Key to access the image tensor in the safetensors file.\n",
    "            spacing_key: Key to access the spacing tensor in the safetensors file. Leave blank if not applicable.\n",
    "            other_keys: Set of keys to access other tensors in the safetensors file. Leave blank if not applicable.\n",
    "            add_channel_dim: Whether to add a channel dimension to the image tensor.\n",
    "            dtype: Desired data type for the image tensor.\n",
    "        \"\"\"\n",
    "        self.image_key = image_key\n",
    "        self.spacing_key = spacing_key\n",
    "        self.other_keys = other_keys\n",
    "        self.add_channel_dim = add_channel_dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def verify_suffix(self, filename):\n",
    "        \"\"\"Ensure the file has a supported safetensors suffix.\"\"\"\n",
    "        return is_supported_format(filename, [\"safetensors\"])\n",
    "\n",
    "    def read(self, filepath) -> dict[str, torch.Tensor | Any]:\n",
    "        \"\"\"Read image data from a safetensors file.\"\"\"\n",
    "        if isinstance(filepath, (list, tuple)):\n",
    "            return [self.read(fp) for fp in filepath]\n",
    "\n",
    "        with safe_open(filepath, \"pt\") as f:\n",
    "            image = f.get_tensor(self.image_key)\n",
    "            spacing = f.get_tensor(self.spacing_key) if self.spacing_key else None\n",
    "            others = {key: f.get_tensor(key) for key in self.other_keys} if self.other_keys else {}\n",
    "\n",
    "        return {\"image\": image, \"spacing\": spacing, \"others\": others}\n",
    "\n",
    "    def get_data(self, datapoint):\n",
    "        \"\"\"Extract and process image data from the datapoint.\"\"\"\n",
    "        datapoint = datapoint[0]\n",
    "\n",
    "        image = datapoint[\"image\"].to(self.dtype)\n",
    "        spacing = datapoint[\"spacing\"]\n",
    "        others = datapoint[\"others\"]\n",
    "\n",
    "        if self.add_channel_dim:\n",
    "            image = image.unsqueeze(0)\n",
    "\n",
    "        image = MetaTensor(image.type(torch.float32), affine=self._spacing_to_affine(spacing))\n",
    "\n",
    "        return image, others\n",
    "\n",
    "    @staticmethod\n",
    "    def _spacing_to_affine(spacing):\n",
    "        \"\"\"Convert spacing tensor to affine matrix according to Metatensor notation.\"\"\"\n",
    "        if spacing is None:\n",
    "            spacing = torch.ones(3)\n",
    "        return torch.diag(torch.cat([spacing, torch.zeros(1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415dff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m2\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m3\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacing = torch.tensor([1, 2, 3])\n",
    "SafetensorsReader._spacing_to_affine(spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f587c9",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6972d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
