{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp swinv2_3d_with_flash_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vision_architectures.swinv2_3d import (\n",
    "    populate_and_validate_config,\n",
    "    get_coords_grid,\n",
    "    SwinV23DMHSA as SwinV23DMHSAWithoutFlashAttention,\n",
    "    SwinV23DLayerMLP,\n",
    "    SwinV23DLayer as SwinV23DLayerWithoutFlashAttention,\n",
    "    SwinV23DBlock as SwinV23DBlockWithoutFlashAttention,\n",
    "    SwinV23DPatchMerging,\n",
    "    SwinV23DStage as SwinV23DStageWithoutFlashAttention,\n",
    "    SwinV23DEncoder as SwinV23DEncoderWithoutFlashAttention,\n",
    "    SwinV23DPatchEmbeddings,\n",
    "    get_3d_position_embeddings,\n",
    "    embed_spacings_in_position_embeddings,\n",
    "    SwinV23DEmbeddings,\n",
    "    SwinV23DModel as SwinV23DModelWithoutFlashAttention,\n",
    "    SwinV23DMIMDecoder,\n",
    "    SwinV23DMIM as SwinV23DMIMWithoutFlashAttention,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DMHSA(SwinV23DMHSAWithoutFlashAttention):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        use_relative_position_bias,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__(dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob)\n",
    "\n",
    "        # Remove attention dropout layer as that is handled automatically, but store the dropout for later\n",
    "        del self.attn_drop\n",
    "        self.attn_drop_prob = attn_drop_prob\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        # hidden_states: (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "        _, num_patches_z, num_patches_y, num_patches_x, _ = hidden_states.shape\n",
    "\n",
    "        query, key, value = rearrange(\n",
    "            self.W_qkv(hidden_states),\n",
    "            \"b nz ny nx (n num_heads d) -> n b num_heads (nz ny nx) d\",\n",
    "            n=3,\n",
    "            num_heads=self.num_heads,\n",
    "        )\n",
    "        # num_patches = window_size_z * window_size_y * window_size_x\n",
    "        # Each is (windowed_b, num_heads, num_patches, per_head_dim)\n",
    "\n",
    "        query_normalized = F.normalize(query, dim=-1)\n",
    "        key_normalized = F.normalize(key, dim=-1)\n",
    "\n",
    "        relative_position_bias = None\n",
    "        if self.use_relative_position_bias:\n",
    "            relative_position_bias = self.calculate_relative_position_bias()\n",
    "\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=np.log(1.0 / 0.01)).exp()\n",
    "\n",
    "        context = F.scaled_dot_product_attention(\n",
    "            query_normalized,\n",
    "            key_normalized,\n",
    "            value,\n",
    "            attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias\n",
    "            dropout_p=self.attn_drop_prob,\n",
    "            is_causal=False,\n",
    "            # scale=logit_scale, # TODO: Allow learnable scaling per head, otherwise it won't work\n",
    "        )\n",
    "        # (windowed_b, num_heads, num_patches, per_head_dim)\n",
    "        context = rearrange(\n",
    "            context,\n",
    "            \"b num_heads (num_patches_z num_patches_y num_patches_x) d -> \"\n",
    "            \"b num_patches_z num_patches_y num_patches_x (num_heads d)\",\n",
    "            num_patches_z=num_patches_z,\n",
    "            num_patches_y=num_patches_y,\n",
    "            num_patches_x=num_patches_x,\n",
    "        )\n",
    "        # (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "\n",
    "        context = self.proj(context)\n",
    "        context = self.proj_drop(context)\n",
    "        # (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mSwinV23DMHSA\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_qkv\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m162\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcpb_mlp\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33minplace\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = SwinV23DMHSA(54, 6, (4, 4, 4), True)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 4, 4, 4, 54)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify other classes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DLayer(SwinV23DLayerWithoutFlashAttention):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        intermediate_ratio,\n",
    "        layer_norm_eps,\n",
    "        window_size,\n",
    "        use_relative_position_bias,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "        mlp_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim,\n",
    "            num_heads,\n",
    "            intermediate_ratio,\n",
    "            layer_norm_eps,\n",
    "            window_size,\n",
    "            use_relative_position_bias,\n",
    "            attn_drop_prob,\n",
    "            proj_drop_prob,\n",
    "            mlp_drop_prob,\n",
    "        )\n",
    "\n",
    "        self.mhsa = SwinV23DMHSA(\n",
    "            dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DBlock(SwinV23DBlockWithoutFlashAttention):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__(stage_config)\n",
    "\n",
    "        self.stage_config = stage_config\n",
    "        self.w_layer = SwinV23DLayer(\n",
    "            stage_config[\"_out_dim\"],\n",
    "            stage_config[\"num_heads\"],\n",
    "            stage_config[\"intermediate_ratio\"],\n",
    "            stage_config[\"layer_norm_eps\"],\n",
    "            stage_config[\"window_size\"],\n",
    "            stage_config[\"use_relative_position_bias\"],\n",
    "            stage_config.get(\"attn_drop_prob\", 0.0),\n",
    "            stage_config.get(\"proj_drop_prob\", 0.0),\n",
    "            stage_config.get(\"mlp_drop_prob\", 0.0),\n",
    "        )\n",
    "        self.sw_layer = SwinV23DLayer(\n",
    "            stage_config[\"_out_dim\"],\n",
    "            stage_config[\"num_heads\"],\n",
    "            stage_config[\"intermediate_ratio\"],\n",
    "            stage_config[\"layer_norm_eps\"],\n",
    "            stage_config[\"window_size\"],\n",
    "            stage_config[\"use_relative_position_bias\"],\n",
    "            stage_config.get(\"attn_drop_prob\", 0.0),\n",
    "            stage_config.get(\"proj_drop_prob\", 0.0),\n",
    "            stage_config.get(\"mlp_drop_prob\", 0.0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DStage(SwinV23DStageWithoutFlashAttention):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__(stage_config)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [SwinV23DBlock(stage_config) for _ in range(stage_config[\"depth\"])],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DEncoder(SwinV23DEncoderWithoutFlashAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.stages = nn.ModuleList([SwinV23DStage(stage_config) for stage_config in config[\"stages\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DModel(SwinV23DModelWithoutFlashAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.encoder = SwinV23DEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinV23DMIM(SwinV23DMIMWithoutFlashAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.swin = SwinV23DModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1183892\u001b[0m, \u001b[1;36m197632\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "sample_spacings = torch.tensor([[1, 0.1, 0.1], [2, 0.2, 0.2], [3, 0.3, 0.3], [4, 0.4, 0.4], [5, 0.5, 0.5]])\n",
    "sample_batch = torch.rand(3, 1, 16, 128, 128)\n",
    "sample_config = populate_and_validate_config(\n",
    "    {\n",
    "        \"patch_size\": (1, 4, 4),\n",
    "        \"dim\": 12,\n",
    "        \"in_channels\": 1,\n",
    "        \"use_absolute_position_embeddings\": True,\n",
    "        \"learnable_absolute_position_embeddings\": False,\n",
    "        \"embed_spacing_info\": False,\n",
    "        \"image_size\": (16, 128, 128),\n",
    "        \"drop_prob\": 0.2,\n",
    "        \"stages\": [\n",
    "            {\n",
    "                \"patch_merging\": None,\n",
    "                \"depth\": 1,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "                \"attn_drop_prob\": 0.2,\n",
    "                \"proj_drop_prob\": 0.2,\n",
    "                \"mlp_drop_prob\": 0.2,\n",
    "            },\n",
    "            {\n",
    "                \"patch_merging\": {\n",
    "                    \"merge_window_size\": (2, 2, 2),\n",
    "                    \"out_dim_ratio\": 4,\n",
    "                },\n",
    "                \"depth\": 3,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "            },\n",
    "            {\n",
    "                \"patch_merging\": {\n",
    "                    \"merge_window_size\": (2, 2, 2),\n",
    "                    \"out_dim_ratio\": 4,\n",
    "                },\n",
    "                \"depth\": 1,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "            },\n",
    "        ],\n",
    "        \"mim\": {\n",
    "            \"mask_ratio\": 0.7,\n",
    "            \"mask_grid_size\": (8, 8, 8),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "model = SwinV23DMIM(sample_config)\n",
    "\n",
    "sum(x.numel() for x in model.swin.parameters()), sum(x.numel() for x in model.decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1,381,536\n",
      "+---------------------------------------------------------------+------------+\n",
      "|                             Module                            | Parameters |\n",
      "+---------------------------------------------------------------+------------+\n",
      "|                           mask_token                          |     12     |\n",
      "|    swin.embeddings.patch_embeddings.patch_embeddings.weight   |    192     |\n",
      "|     swin.embeddings.patch_embeddings.patch_embeddings.bias    |     12     |\n",
      "|               swin.embeddings.layer_norm.weight               |     12     |\n",
      "|                swin.embeddings.layer_norm.bias                |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.W_qkv.weight   |    432     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mhsa.W_qkv.bias    |     36     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.proj.weight    |    144     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mhsa.proj.bias     |     12     |\n",
      "|  swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.layernorm1.weight   |     12     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.layernorm1.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mlp.dense1.weight   |    576     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mlp.dense1.bias    |     48     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mlp.dense2.weight   |    576     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mlp.dense2.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.layernorm2.weight   |     12     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.layernorm2.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mhsa.W_qkv.weight   |    432     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.W_qkv.bias    |     36     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.proj.weight   |    144     |\n",
      "|     swin.encoder.stages.0.blocks.0.sw_layer.mhsa.proj.bias    |     12     |\n",
      "| swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.layernorm1.weight   |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.layernorm1.bias    |     12     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense1.weight   |    576     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense1.bias    |     48     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense2.weight   |    576     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense2.bias    |     12     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.layernorm2.weight   |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.layernorm2.bias    |     12     |\n",
      "|     swin.encoder.stages.1.patch_merging.layer_norm.weight     |     96     |\n",
      "|      swin.encoder.stages.1.patch_merging.layer_norm.bias      |     96     |\n",
      "|        swin.encoder.stages.1.patch_merging.proj.weight        |   4,608    |\n",
      "|         swin.encoder.stages.1.patch_merging.proj.bias         |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.0.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.1.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.2.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.layernorm2.bias    |     48     |\n",
      "|     swin.encoder.stages.2.patch_merging.layer_norm.weight     |    384     |\n",
      "|      swin.encoder.stages.2.patch_merging.layer_norm.bias      |    384     |\n",
      "|        swin.encoder.stages.2.patch_merging.proj.weight        |   73,728   |\n",
      "|         swin.encoder.stages.2.patch_merging.proj.bias         |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.W_qkv.weight   |  110,592   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mhsa.W_qkv.bias    |    576     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.proj.weight    |   36,864   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mhsa.proj.bias     |    192     |\n",
      "|  swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.layernorm1.weight   |    192     |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.layernorm1.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mlp.dense1.weight   |  147,456   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mlp.dense1.bias    |    768     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mlp.dense2.weight   |  147,456   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mlp.dense2.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.layernorm2.weight   |    192     |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.layernorm2.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mhsa.W_qkv.weight   |  110,592   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.W_qkv.bias    |    576     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.proj.weight   |   36,864   |\n",
      "|     swin.encoder.stages.2.blocks.0.sw_layer.mhsa.proj.bias    |    192     |\n",
      "| swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.layernorm1.weight   |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.layernorm1.bias    |    192     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense1.weight   |  147,456   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense1.bias    |    768     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense2.weight   |  147,456   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense2.bias    |    192     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.layernorm2.weight   |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.layernorm2.bias    |    192     |\n",
      "|                     decoder.decoder.weight                    |  196,608   |\n",
      "|                      decoder.decoder.bias                     |   1,024    |\n",
      "+---------------------------------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from neuro_utils.describe import describe_model\n",
    "\n",
    "describe_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "sample_spacings = sample_spacings.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a324434b2844a56b2a541f305e43012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.210407\tLR: 0.500000\n",
      "Loss: 3.960272\tLR: 0.500000\n",
      "Loss: 5.666294\tLR: 0.500000\n",
      "Loss: 3.388160\tLR: 0.500000\n",
      "Loss: 3.186559\tLR: 0.500000\n",
      "Loss: 2.869438\tLR: 0.450000\n",
      "Loss: 2.782217\tLR: 0.450000\n",
      "Loss: 2.372935\tLR: 0.450000\n",
      "Loss: 1.711966\tLR: 0.450000\n",
      "Loss: 1.976822\tLR: 0.450000\n",
      "Loss: 1.917054\tLR: 0.405000\n",
      "Loss: 1.574714\tLR: 0.405000\n",
      "Loss: 1.528946\tLR: 0.405000\n",
      "Loss: 1.693159\tLR: 0.405000\n",
      "Loss: 1.569976\tLR: 0.405000\n",
      "Loss: 1.565363\tLR: 0.364500\n",
      "Loss: 1.387463\tLR: 0.364500\n",
      "Loss: 1.574689\tLR: 0.364500\n",
      "Loss: 1.272892\tLR: 0.364500\n",
      "Loss: 1.371980\tLR: 0.364500\n",
      "Loss: 1.410923\tLR: 0.328050\n",
      "Loss: 1.224110\tLR: 0.328050\n",
      "Loss: 1.263726\tLR: 0.328050\n",
      "Loss: 1.214210\tLR: 0.328050\n",
      "Loss: 1.185062\tLR: 0.328050\n",
      "Loss: 1.291223\tLR: 0.295245\n",
      "Loss: 1.057200\tLR: 0.295245\n",
      "Loss: 1.117633\tLR: 0.295245\n",
      "Loss: 1.109142\tLR: 0.295245\n",
      "Loss: 1.088373\tLR: 0.295245\n",
      "Loss: 1.038932\tLR: 0.265721\n",
      "Loss: 0.964423\tLR: 0.265721\n",
      "Loss: 1.012183\tLR: 0.265721\n",
      "Loss: 1.008810\tLR: 0.265721\n",
      "Loss: 1.001049\tLR: 0.265721\n",
      "Loss: 0.943001\tLR: 0.239148\n",
      "Loss: 0.927310\tLR: 0.239148\n",
      "Loss: 0.936613\tLR: 0.239148\n",
      "Loss: 1.000140\tLR: 0.239148\n",
      "Loss: 0.955273\tLR: 0.239148\n",
      "Loss: 0.971250\tLR: 0.215234\n",
      "Loss: 0.843511\tLR: 0.215234\n",
      "Loss: 0.820630\tLR: 0.215234\n",
      "Loss: 0.801209\tLR: 0.215234\n",
      "Loss: 0.796608\tLR: 0.215234\n",
      "Loss: 0.791659\tLR: 0.193710\n",
      "Loss: 0.776350\tLR: 0.193710\n",
      "Loss: 0.768157\tLR: 0.193710\n",
      "Loss: 0.762648\tLR: 0.193710\n",
      "Loss: 0.760111\tLR: 0.193710\n",
      "Loss: 0.758090\tLR: 0.174339\n",
      "Loss: 0.753666\tLR: 0.174339\n",
      "Loss: 0.751789\tLR: 0.174339\n",
      "Loss: 0.750438\tLR: 0.174339\n",
      "Loss: 0.749911\tLR: 0.174339\n",
      "Loss: 0.749620\tLR: 0.156905\n",
      "Loss: 0.749457\tLR: 0.156905\n",
      "Loss: 0.749370\tLR: 0.156905\n",
      "Loss: 0.749737\tLR: 0.156905\n",
      "Loss: 0.749580\tLR: 0.156905\n",
      "Loss: 0.749263\tLR: 0.141215\n",
      "Loss: 0.749809\tLR: 0.141215\n",
      "Loss: 0.749702\tLR: 0.141215\n",
      "Loss: 0.749577\tLR: 0.141215\n",
      "Loss: 0.749286\tLR: 0.141215\n",
      "Loss: 0.749469\tLR: 0.127093\n",
      "Loss: 0.749370\tLR: 0.127093\n",
      "Loss: 0.749369\tLR: 0.127093\n",
      "Loss: 0.749407\tLR: 0.127093\n",
      "Loss: 0.749662\tLR: 0.127093\n",
      "Loss: 0.749370\tLR: 0.114384\n",
      "Loss: 0.749507\tLR: 0.114384\n",
      "Loss: 0.749456\tLR: 0.114384\n",
      "Loss: 0.749015\tLR: 0.114384\n",
      "Loss: 0.749382\tLR: 0.114384\n",
      "Loss: 0.749205\tLR: 0.102946\n",
      "Loss: 0.749400\tLR: 0.102946\n",
      "Loss: 0.749354\tLR: 0.102946\n",
      "Loss: 0.749619\tLR: 0.102946\n",
      "Loss: 0.749240\tLR: 0.102946\n",
      "Loss: 0.749165\tLR: 0.092651\n",
      "Loss: 0.749069\tLR: 0.092651\n",
      "Loss: 0.749338\tLR: 0.092651\n",
      "Loss: 0.749201\tLR: 0.092651\n",
      "Loss: 0.749260\tLR: 0.092651\n",
      "Loss: 0.749097\tLR: 0.083386\n",
      "Loss: 0.749217\tLR: 0.083386\n",
      "Loss: 0.749361\tLR: 0.083386\n",
      "Loss: 0.749721\tLR: 0.083386\n",
      "Loss: 0.749396\tLR: 0.083386\n",
      "Loss: 0.749701\tLR: 0.075047\n",
      "Loss: 0.749443\tLR: 0.075047\n",
      "Loss: 0.749156\tLR: 0.075047\n",
      "Loss: 0.749599\tLR: 0.075047\n",
      "Loss: 0.749630\tLR: 0.075047\n",
      "Loss: 0.749033\tLR: 0.067543\n",
      "Loss: 0.749257\tLR: 0.067543\n",
      "Loss: 0.749689\tLR: 0.067543\n",
      "Loss: 0.749303\tLR: 0.067543\n",
      "Loss: 0.749359\tLR: 0.067543\n",
      "Loss: 0.749294\tLR: 0.060788\n",
      "Loss: 0.749384\tLR: 0.060788\n",
      "Loss: 0.749211\tLR: 0.060788\n",
      "Loss: 0.749282\tLR: 0.060788\n",
      "Loss: 0.749265\tLR: 0.060788\n",
      "Loss: 0.749514\tLR: 0.054709\n",
      "Loss: 0.749017\tLR: 0.054709\n",
      "Loss: 0.749413\tLR: 0.054709\n",
      "Loss: 0.749490\tLR: 0.054709\n",
      "Loss: 0.749495\tLR: 0.054709\n",
      "Loss: 0.749339\tLR: 0.049239\n",
      "Loss: 0.749508\tLR: 0.049239\n",
      "Loss: 0.749322\tLR: 0.049239\n",
      "Loss: 0.749315\tLR: 0.049239\n",
      "Loss: 0.749417\tLR: 0.049239\n",
      "Loss: 0.749325\tLR: 0.044315\n",
      "Loss: 0.749195\tLR: 0.044315\n",
      "Loss: 0.749414\tLR: 0.044315\n",
      "Loss: 0.749326\tLR: 0.044315\n",
      "Loss: 0.749317\tLR: 0.044315\n",
      "Loss: 0.749396\tLR: 0.039883\n",
      "Loss: 0.749670\tLR: 0.039883\n",
      "Loss: 0.749333\tLR: 0.039883\n",
      "Loss: 0.749359\tLR: 0.039883\n",
      "Loss: 0.749253\tLR: 0.039883\n",
      "Loss: 0.749383\tLR: 0.035895\n",
      "Loss: 0.749390\tLR: 0.035895\n",
      "Loss: 0.749691\tLR: 0.035895\n",
      "Loss: 0.749260\tLR: 0.035895\n",
      "Loss: 0.749487\tLR: 0.035895\n",
      "Loss: 0.749095\tLR: 0.032305\n",
      "Loss: 0.749500\tLR: 0.032305\n",
      "Loss: 0.748939\tLR: 0.032305\n",
      "Loss: 0.749536\tLR: 0.032305\n",
      "Loss: 0.748939\tLR: 0.032305\n",
      "Loss: 0.749129\tLR: 0.029075\n",
      "Loss: 0.749249\tLR: 0.029075\n",
      "Loss: 0.749294\tLR: 0.029075\n",
      "Loss: 0.748974\tLR: 0.029075\n",
      "Loss: 0.749192\tLR: 0.029075\n",
      "Loss: 0.749698\tLR: 0.026167\n",
      "Loss: 0.749478\tLR: 0.026167\n",
      "Loss: 0.749320\tLR: 0.026167\n",
      "Loss: 0.749182\tLR: 0.026167\n",
      "Loss: 0.749419\tLR: 0.026167\n",
      "Loss: 0.749394\tLR: 0.023551\n",
      "Loss: 0.749651\tLR: 0.023551\n",
      "Loss: 0.749089\tLR: 0.023551\n",
      "Loss: 0.749464\tLR: 0.023551\n",
      "Loss: 0.749495\tLR: 0.023551\n",
      "Loss: 0.749640\tLR: 0.021196\n",
      "Loss: 0.749571\tLR: 0.021196\n",
      "Loss: 0.749806\tLR: 0.021196\n",
      "Loss: 0.749218\tLR: 0.021196\n",
      "Loss: 0.749260\tLR: 0.021196\n",
      "Loss: 0.748987\tLR: 0.019076\n",
      "Loss: 0.749317\tLR: 0.019076\n",
      "Loss: 0.749147\tLR: 0.019076\n",
      "Loss: 0.749358\tLR: 0.019076\n",
      "Loss: 0.749339\tLR: 0.019076\n",
      "Loss: 0.749529\tLR: 0.017168\n",
      "Loss: 0.749569\tLR: 0.017168\n",
      "Loss: 0.749644\tLR: 0.017168\n",
      "Loss: 0.749104\tLR: 0.017168\n",
      "Loss: 0.749338\tLR: 0.017168\n",
      "Loss: 0.749428\tLR: 0.015452\n",
      "Loss: 0.749278\tLR: 0.015452\n",
      "Loss: 0.749185\tLR: 0.015452\n",
      "Loss: 0.748944\tLR: 0.015452\n",
      "Loss: 0.749318\tLR: 0.015452\n",
      "Loss: 0.749670\tLR: 0.013906\n",
      "Loss: 0.749490\tLR: 0.013906\n",
      "Loss: 0.748991\tLR: 0.013906\n",
      "Loss: 0.749148\tLR: 0.013906\n",
      "Loss: 0.749136\tLR: 0.013906\n",
      "Loss: 0.749287\tLR: 0.012516\n",
      "Loss: 0.749277\tLR: 0.012516\n",
      "Loss: 0.749426\tLR: 0.012516\n",
      "Loss: 0.749143\tLR: 0.012516\n",
      "Loss: 0.748971\tLR: 0.012516\n",
      "Loss: 0.749180\tLR: 0.011264\n",
      "Loss: 0.749392\tLR: 0.011264\n",
      "Loss: 0.749316\tLR: 0.011264\n",
      "Loss: 0.749831\tLR: 0.011264\n",
      "Loss: 0.749418\tLR: 0.011264\n",
      "Loss: 0.749312\tLR: 0.010138\n",
      "Loss: 0.749504\tLR: 0.010138\n",
      "Loss: 0.749392\tLR: 0.010138\n",
      "Loss: 0.749230\tLR: 0.010138\n",
      "Loss: 0.748954\tLR: 0.010138\n",
      "Loss: 0.749038\tLR: 0.009124\n",
      "Loss: 0.749313\tLR: 0.009124\n",
      "Loss: 0.749291\tLR: 0.009124\n",
      "Loss: 0.749379\tLR: 0.009124\n",
      "Loss: 0.749190\tLR: 0.009124\n",
      "Loss: 0.749167\tLR: 0.008212\n",
      "Loss: 0.749422\tLR: 0.008212\n",
      "Loss: 0.749297\tLR: 0.008212\n",
      "Loss: 0.749396\tLR: 0.008212\n",
      "Loss: 0.749400\tLR: 0.008212\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch, sample_spacings)\n",
    "    print(f\"Loss: {output[1]:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    output[1].backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin.encoder.stages.0.blocks.0.w_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.0.blocks.0.sw_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.1.blocks.0.w_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.1.blocks.0.sw_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.1.blocks.1.w_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.1.blocks.1.sw_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.1.blocks.2.w_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.1.blocks.2.sw_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.2.blocks.0.w_layer.mhsa.logit_scale\n",
      "swin.encoder.stages.2.blocks.0.sw_layer.mhsa.logit_scale\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "coords_h = torch.arange(4)\n",
    "coords_w = torch.arange(4)\n",
    "coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "coords_flatten = torch.flatten(coords, 1)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += 4 - 1\n",
    "relative_coords[:, :, 1] += 4 - 1\n",
    "relative_coords[:, :, 0] *= 2 * 4 - 1\n",
    "relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "window_size = (4, 4, 4)\n",
    "relative_limits = (7, 7, 7)\n",
    "\n",
    "coords = get_coords_grid(window_size)\n",
    "coords_flatten = rearrange(coords, \"three_dimensional d h w -> three_dimensional (d h w)\", three_dimensional=3)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += window_size[0] - 1\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "relative_coords[:, :, 2] += window_size[2] - 1\n",
    "relative_position_index: torch.Tensor = (\n",
    "    relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 2]\n",
    ")\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "window_size = (2, 2, 2)\n",
    "relative_limits = (2 * window_size[0] - 1, 2 * window_size[1] - 1, 2 * window_size[2] - 1)\n",
    "\n",
    "relative_coords_table = get_coords_grid(relative_limits)\n",
    "relative_coords_table[0] -= window_size[0] - 1\n",
    "relative_coords_table[1] -= window_size[1] - 1\n",
    "relative_coords_table[2] -= window_size[2] - 1\n",
    "relative_coords_table = relative_coords_table.permute(1, 2, 3, 0).contiguous()\n",
    "relative_coords_table[0, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
