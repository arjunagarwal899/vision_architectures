{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp losses/class_balanced_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5958da",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from vision_architectures.docstrings import populate_docstring\n",
    "from vision_architectures.utils.custom_base_model import CustomBaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c5524",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ClassBalancedCrossEntropyLossConfig(CustomBaseModel):\n",
    "    num_classes: int = Field(..., description=\"Number of classes to weight cross entropy loss.\")\n",
    "    ema_decay: float = Field(\n",
    "        0.99, description=\"Exponential moving average decay. By default 0.99 is used which has a half life of ~69 steps\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d3d45",
   "metadata": {},
   "source": [
    "# The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f20615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ClassBalancedCrossEntropyLoss(nn.Module):\n",
    "    @populate_docstring\n",
    "    def __init__(self, config: ClassBalancedCrossEntropyLossConfig = {}, **kwargs):\n",
    "        \"\"\"Class-balanced cross-entropy loss with running prevalence estimation.\n",
    "\n",
    "        This loss reweights the standard multi-class cross-entropy by the inverse of\n",
    "        the observed class prevalences in the training data. Class prevalences are\n",
    "        estimated online via an exponential moving average (EMA) using class counts\n",
    "        from the incoming targets.\n",
    "\n",
    "        Notes:\n",
    "            - Targets must be discrete integer class indices in [0, num_classes-1].\n",
    "              Probabilistic/soft labels are not supported.\n",
    "            - For classes that haven't been observed yet, their prevalence is treated\n",
    "              as NaN and replaced in the weight vector by the mean of observed weights\n",
    "              (then clamped within 3 standard deviations to avoid extreme values).\n",
    "\n",
    "        Args:\n",
    "            config: {CONFIG_INSTANCE_DOC}\n",
    "            **kwargs: {CONFIG_KWARGS_DOC}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = ClassBalancedCrossEntropyLossConfig.model_validate(config | kwargs)\n",
    "\n",
    "        # Save class prevalence percentages to weight the cross entropy loss\n",
    "        self.class_prevalences = [None] * self.config.num_classes\n",
    "        # Initialized with None as we don't know the initial class prevalence estimates.\n",
    "\n",
    "    def _update_class_prevalences(self, target: torch.Tensor):\n",
    "        \"\"\"Update the running class-prevalence estimates from a target tensor.\n",
    "\n",
    "        The method counts class occurrences in the provided target, converts counts\n",
    "        to per-batch prevalences, then updates the internal EMA-tracked prevalence\n",
    "        vector for each class.\n",
    "\n",
    "        Args:\n",
    "            target: A tensor of integer class indices with any shape, typically\n",
    "                (N,) or (N, ...) for segmentation. Values outside\n",
    "                [0, num_classes-1] are ignored.\n",
    "        \"\"\"\n",
    "        # Count the number of times each class is encountered\n",
    "        class_counts = Counter(target.flatten().int().tolist())\n",
    "\n",
    "        # Remove those counts that exceed the number of classes that are being tracked\n",
    "        class_counts = {k: v for k, v in class_counts.items() if k < self.config.num_classes}\n",
    "\n",
    "        # Calculate prevalence of each class\n",
    "        total_count = sum(class_counts.values())\n",
    "        new_prevalences = (\n",
    "            {class_id: count / total_count for class_id, count in class_counts.items()} if total_count > 0 else {}\n",
    "        )\n",
    "\n",
    "        # Update current prevalences using EMA to allow for distribution shift in data\n",
    "        decay = self.config.ema_decay\n",
    "        for i in range(self.config.num_classes):\n",
    "            if self.class_prevalences[i] is None:\n",
    "                # if encountered for the first time\n",
    "                if new_prevalences.get(i, 0.0) > 0.0:\n",
    "                    self.class_prevalences[i] = new_prevalences.get(i, 0.0)\n",
    "                # otherwise let it stay None\n",
    "            else:\n",
    "                # update to new value using EMA\n",
    "                self.class_prevalences[i] = self.class_prevalences[i] * decay + new_prevalences.get(i, 0.0) * (\n",
    "                    1 - decay\n",
    "                )\n",
    "\n",
    "    def get_class_prevalences(self, device=torch.device(\"cpu\")) -> torch.Tensor:\n",
    "        \"\"\"Return the current vector of class prevalences as a tensor.\n",
    "\n",
    "        For classes that haven't been observed yet, the corresponding entry will be\n",
    "        NaN. This method does not perform any imputation or normalization beyond\n",
    "        returning the current EMA state.\n",
    "\n",
    "        Args:\n",
    "            device: The device on which to place the returned tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (num_classes,) with dtype float32 containing per-class\n",
    "            prevalence estimates in [0, 1] or NaN for unseen classes.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get class prevalences and replace None with nan\n",
    "        class_prevalences = [\n",
    "            prevalence if prevalence is not None else torch.nan for prevalence in self.class_prevalences\n",
    "        ]\n",
    "\n",
    "        # Calculate weights as inverse of class prevalences\n",
    "        class_prevalences = torch.tensor(class_prevalences, dtype=torch.float32, device=device)\n",
    "\n",
    "        return class_prevalences\n",
    "\n",
    "    def get_class_weights(self, device=torch.device(\"cpu\")) -> torch.Tensor:\n",
    "        \"\"\"Compute per-class weights as the inverse of prevalences with safeguards.\n",
    "\n",
    "        Steps:\n",
    "            1) Convert current EMA prevalences to a tensor with NaNs for unseen classes.\n",
    "            2) Take the inverse to obtain raw weights (higher weight for rarer classes).\n",
    "            3) Replace NaNs by the mean of observed weights to avoid biasing toward\n",
    "               unseen classes, then clamp to mean ± 3·std to prevent extreme values.\n",
    "            4) Renormalize weights to sum to num_classes (so the average weight is 1).\n",
    "\n",
    "        Args:\n",
    "            device: The device on which to place the returned tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (num_classes,) containing normalized class weights.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get class prevalences\n",
    "        class_prevalences = self.get_class_prevalences(device)\n",
    "\n",
    "        # Calculate weights as inverse of class prevalences\n",
    "        weights = 1 / torch.tensor(class_prevalences, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Substitute nan values with mean and clamp weights to a limit\n",
    "        # Assumption: all classes are visited at least once in the dataset\n",
    "        mu, std = weights.nanmean(), weights[~weights.isnan()].std()\n",
    "        weights = weights.nan_to_num(mu)\n",
    "        weights = weights.clamp(mu - 3 * std, mu + 3 * std)\n",
    "\n",
    "        # Normalize weights to sum to self.config.num_classes\n",
    "        weights = self.config.num_classes * weights / weights.sum()\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def forward(\n",
    "        self, input: torch.Tensor, target: torch.Tensor, return_class_weights: bool = False, *args, **kwargs\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute class-balanced cross entropy.\n",
    "\n",
    "        This updates the internal class-prevalence EMA using the provided targets,\n",
    "        then computes cross-entropy with a weight vector derived from the current\n",
    "        prevalences.\n",
    "\n",
    "        Args:\n",
    "            input: Logits of shape (N, C, ...) where C == num_classes. Any extra spatial\n",
    "                dimensions (e.g., H, W, D) are supported as long as `target` is broadcastable\n",
    "                to the same non-channel shape expected by torch.nn.functional.cross_entropy.\n",
    "            target: Integer class indices with shape matching input without the channel\n",
    "                dimension, e.g., (N, ...) with values in [0, C-1].\n",
    "            return_class_weights: If True, also return the per-class weight tensor used\n",
    "                for this call.\n",
    "            *args, **kwargs: Additional keyword args forwarded to F.cross_entropy\n",
    "                (e.g., reduction='mean').\n",
    "\n",
    "        Returns:\n",
    "            If return_class_weights is False: a scalar tensor loss.\n",
    "            If True: a tuple (loss, class_weights).\n",
    "        \"\"\"\n",
    "        # Update class prevalences\n",
    "        self._update_class_prevalences(target)\n",
    "\n",
    "        # Get class weights\n",
    "        class_weights = self.get_class_weights(input.device)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(input, target, weight=class_weights, *args, **kwargs)\n",
    "\n",
    "        if return_class_weights:\n",
    "            return loss, class_weights\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9960) tensor([1.2000, 0.6000, 1.2000])\n",
      "tensor(1.0988) tensor([1.1916, 0.6048, 1.2036])\n",
      "tensor(1.0598) tensor([1.1869, 0.6145, 1.1986])\n",
      "tensor(1.1092) tensor([1.1871, 0.6143, 1.1986])\n",
      "tensor(1.2541) tensor([1.1906, 0.6191, 1.1903])\n",
      "tensor(1.0118) tensor([1.2024, 0.6188, 1.1787])\n",
      "tensor(0.9824) tensor([1.2174, 0.6233, 1.1593])\n",
      "tensor(0.9926) tensor([1.2087, 0.6282, 1.1631])\n",
      "tensor(0.9627) tensor([1.2087, 0.6279, 1.1635])\n",
      "tensor(1.1370) tensor([1.2117, 0.6325, 1.1558])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1989923/2428882980.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weights = 1 / torch.tensor(class_prevalences, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "# Regular test\n",
    "\n",
    "\n",
    "test = ClassBalancedCrossEntropyLoss(num_classes=3)\n",
    "\n",
    "for _ in range(10):\n",
    "    example_input = torch.rand(4, 3)\n",
    "    example_target = torch.randint(0, 3, (4,))\n",
    "    loss, class_weights = test(example_input, example_target, return_class_weights=True)\n",
    "    print(loss, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2986d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5845) tensor([0.5000, 1.0000, 1.5000, 1.0000, 1.0000])\n",
      "tensor(1.5643) tensor([0.4267, 1.2802, 1.2931, 1.0000, 1.0000])\n",
      "tensor(1.5043) tensor([0.4310, 1.2717, 1.2973, 1.0000, 1.0000])\n",
      "tensor(1.7012) tensor([0.4354, 1.2761, 1.2885, 1.0000, 1.0000])\n",
      "tensor(1.7786) tensor([0.4428, 1.2725, 1.2846, 1.0000, 1.0000])\n",
      "tensor(1.6650) tensor([0.4439, 1.2595, 1.2966, 1.0000, 1.0000])\n",
      "tensor(1.7746) tensor([0.4515, 1.2685, 1.2800, 1.0000, 1.0000])\n",
      "tensor(1.6959) tensor([0.4462, 1.2712, 1.2826, 1.0000, 1.0000])\n",
      "tensor(1.5825) tensor([0.4537, 1.2551, 1.2912, 1.0000, 1.0000])\n",
      "tensor(1.6506) tensor([0.4612, 1.2517, 1.2870, 1.0000, 1.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1989923/2428882980.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weights = 1 / torch.tensor(class_prevalences, dtype=torch.float32, device=device)\n"
     ]
    }
   ],
   "source": [
    "# Test where last two classes are never encountered\n",
    "\n",
    "test = ClassBalancedCrossEntropyLoss(num_classes=5)\n",
    "\n",
    "for _ in range(10):\n",
    "    example_input = torch.rand(4, 5)\n",
    "    example_target = torch.randint(0, 3, (4,))\n",
    "    loss, class_weights = test(example_input, example_target, return_class_weights=True)\n",
    "    print(loss, class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f28624",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1089315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
