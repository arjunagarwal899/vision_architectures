{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp schedulers/noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class NoiseScheduler(nn.Module):\n",
    "    \"\"\"Base class for Gussian noise schedulers used in diffusion models\"\"\"\n",
    "\n",
    "    def __init__(self, betas: torch.Tensor | None = None, alphas_cumprod: torch.Tensor | None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert (betas is None) != (alphas_cumprod is None), \"Either betas or alphas_cumprod should be provided\"\n",
    "\n",
    "        if betas is not None:\n",
    "            alphas = 1.0 - betas\n",
    "            alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "            sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "            sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "        elif alphas_cumprod is not None:\n",
    "            alphas = alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "            betas = 1.0 - alphas\n",
    "            sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "            sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.T = len(betas) - 1  # The first element is assumed not to be used i.e. t=0 is unused\n",
    "\n",
    "        betas[0] = 0.0\n",
    "        alphas[0] = 1.0\n",
    "        alphas_cumprod[0] = 1.0\n",
    "        sqrt_alphas_cumprod[0] = 1.0\n",
    "        sqrt_one_minus_alphas_cumprod[0] = 0.0\n",
    "\n",
    "        # Register as non-persistent buffers so that they are moved to the correct device\n",
    "        self.register_buffer(\"betas\", betas, persistent=False)\n",
    "        self.register_buffer(\"alphas\", alphas, persistent=False)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod, persistent=False)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", sqrt_alphas_cumprod, persistent=False)\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", sqrt_one_minus_alphas_cumprod, persistent=False)\n",
    "\n",
    "    def add_noise(self, x0: torch.Tensor, t: torch.Tensor, noise: torch.Tensor | None = None):\n",
    "        \"\"\"If noise is not provided, it is sampled from a standard normal distribution\"\"\"\n",
    "\n",
    "        # x0: (b, ...)\n",
    "        # t: (b,)\n",
    "        # noise: (b, ...)\n",
    "\n",
    "        assert (t <= self.T).all(), \"t should be less than T\"\n",
    "\n",
    "        noise_provided: bool = True\n",
    "        if noise is None:\n",
    "            noise_provided = False\n",
    "            noise = torch.randn_like(x0)\n",
    "\n",
    "        unsqueeze = [slice(0, None)] + [None] * (len(x0.shape) - 1)\n",
    "        xt = self.sqrt_alphas_cumprod[t][unsqueeze] * x0 + self.sqrt_one_minus_alphas_cumprod[t][unsqueeze] * noise\n",
    "\n",
    "        return_value = [xt]\n",
    "        if not noise_provided:\n",
    "            return_value.append(noise)\n",
    "        return tuple(return_value)\n",
    "\n",
    "    def remove_noise(self, xt: torch.Tensor, noise_pred: torch.Tensor, t: torch.Tensor):\n",
    "        # xt: (b, ...)\n",
    "        # noise_pred: (b, ...)\n",
    "        # t: (b,)\n",
    "\n",
    "        assert (t <= self.T).all(), \"t should be less than T\"\n",
    "\n",
    "        unsqueeze = [slice(0, None)] + [None] * (len(xt.shape) - 1)\n",
    "\n",
    "        x0_hat = (xt - (self.sqrt_one_minus_alphas_cumprod[t][unsqueeze] * noise_pred)) / self.sqrt_alphas_cumprod[t][\n",
    "            unsqueeze\n",
    "        ]\n",
    "\n",
    "        mean_batch = xt - (\n",
    "            (self.betas[t][unsqueeze] * noise_pred) / (self.sqrt_one_minus_alphas_cumprod[t][unsqueeze])\n",
    "        ) / torch.sqrt(self.alphas[t][unsqueeze])\n",
    "        variance_batch = (\n",
    "            self.betas[t][unsqueeze]\n",
    "            * (1.0 - self.alphas_cumprod[t - 1][unsqueeze])\n",
    "            / (1.0 - self.alphas_cumprod[t][unsqueeze])\n",
    "        )\n",
    "        sigma_batch = torch.sqrt(variance_batch)\n",
    "        standard_noise = torch.randn_like(xt)\n",
    "\n",
    "        xt_minus_1_hat = mean_batch + sigma_batch * standard_noise\n",
    "\n",
    "        return x0_hat, xt_minus_1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "betas = torch.linspace(0.0001, 0.02, T + 1)\n",
    "scheduler = NoiseScheduler(betas)\n",
    "\n",
    "x0 = torch.zeros(2, 3, 32, 32)\n",
    "t = torch.tensor([0, 10])\n",
    "xt, noise = scheduler.add_noise(x0, t)\n",
    "assert torch.allclose(xt[0], x0[0])\n",
    "\n",
    "x0_hat, xt_minus_1 = scheduler.remove_noise(xt, noise, t)\n",
    "assert torch.allclose(x0_hat, x0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class LinearNoiseScheduler(NoiseScheduler):\n",
    "    # https://arxiv.org/pdf/2006.11239\n",
    "    def __init__(self, T: int, min_beta: float = 0.0001, max_beta: float = 0.02):\n",
    "        betas = torch.linspace(min_beta, max_beta, T)\n",
    "        super().__init__(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "scheduler = LinearNoiseScheduler(T)\n",
    "\n",
    "x0 = torch.zeros(2, 3, 32, 32)\n",
    "t = torch.tensor([0, 10])\n",
    "xt, noise = scheduler.add_noise(x0, t)\n",
    "assert torch.allclose(xt[0], x0[0])\n",
    "\n",
    "x0_hat, xt_minus_1 = scheduler.remove_noise(xt, noise, t)\n",
    "assert torch.allclose(x0_hat, x0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CosineNoiseScheduler(NoiseScheduler):\n",
    "    # https://arxiv.org/pdf/2102.09672\n",
    "    def __init__(self, T: int, s: float = 0.008, min_alphas_cumprod: float = 1e-9):\n",
    "        self.s = s\n",
    "\n",
    "        alphas_cumprod = torch.cos(((torch.arange(T) / T + s) / (1 + s)) * (torch.pi / 2)) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        alphas_cumprod.clamp_(min=min_alphas_cumprod)\n",
    "        super().__init__(alphas_cumprod=alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "scheduler = CosineNoiseScheduler(T)\n",
    "\n",
    "x0 = torch.zeros(2, 3, 32, 32)\n",
    "t = torch.tensor([0, 10])\n",
    "xt, noise = scheduler.add_noise(x0, t)\n",
    "assert torch.allclose(xt[0], x0[0])\n",
    "\n",
    "x0_hat, xt_minus_1 = scheduler.remove_noise(xt, noise, t)\n",
    "assert torch.allclose(x0_hat, x0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SigmoidNoiseScheduler(NoiseScheduler):\n",
    "    # https://arxiv.org/pdf/2212.11972\n",
    "    def __init__(\n",
    "        self, T: int, tau: float = 1.0, sigmoid_start: int = -3, sigmoid_end: int = 3, min_alphas_cumprod: float = 1e-9\n",
    "    ):\n",
    "        self.tau = tau\n",
    "        self.sigmoid_start = sigmoid_start\n",
    "        self.sigmoid_end = sigmoid_end\n",
    "\n",
    "        start = torch.tensor(sigmoid_start)\n",
    "        end = torch.tensor(sigmoid_end)\n",
    "        v_start = torch.sigmoid(start / tau)\n",
    "        v_end = torch.sigmoid(end / tau)\n",
    "\n",
    "        alphas_cumprod = (-torch.sigmoid(((torch.arange(T) / T) * (end - start) + start) / tau) + v_end) / (\n",
    "            v_end - v_start\n",
    "        )\n",
    "        alphas_cumprod.clamp_(min=min_alphas_cumprod)\n",
    "        super().__init__(alphas_cumprod=alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "scheduler = SigmoidNoiseScheduler(T)\n",
    "\n",
    "x0 = torch.zeros(2, 3, 32, 32)\n",
    "t = torch.tensor([0, 10])\n",
    "xt, noise = scheduler.add_noise(x0, t)\n",
    "assert torch.allclose(xt[0], x0[0])\n",
    "\n",
    "x0_hat, xt_minus_1 = scheduler.remove_noise(xt, noise, t)\n",
    "assert torch.allclose(x0_hat, x0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class FibonacciNoiseScheduler(NoiseScheduler):\n",
    "    # https://arxiv.org/pdf/2009.00713\n",
    "    def __init__(self, T: int, first_element: float = 1e-6, second_element: float = 2e-6):\n",
    "        assert T > 2, \"T should be greater than 2\"\n",
    "        if T > 25:\n",
    "            print(\"Warning: This noise scheduler explodes very quickly. Be careful with large T values.\")\n",
    "        betas = [first_element, second_element]\n",
    "        for _ in range(2, T):\n",
    "            betas.append(betas[-1] + betas[-2])\n",
    "        betas = torch.Tensor(betas).clamp(0.0, 1.0)\n",
    "        super().__init__(betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: This noise scheduler explodes very quickly. Be careful with large T values.\n"
     ]
    }
   ],
   "source": [
    "T = 30\n",
    "scheduler = FibonacciNoiseScheduler(T)\n",
    "\n",
    "x0 = torch.zeros(2, 3, 32, 32)\n",
    "t = torch.tensor([0, 10])\n",
    "xt, noise = scheduler.add_noise(x0, t)\n",
    "assert torch.allclose(xt[0], x0[0])\n",
    "\n",
    "x0_hat, xt_minus_1 = scheduler.remove_noise(xt, noise, t)\n",
    "assert torch.allclose(x0_hat, x0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ExponentialNoiseScheduler(NoiseScheduler):\n",
    "    def __init__(self, T: int, beta_min: float = 0.0001, beta_max: float = 0.02):\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "\n",
    "        betas = beta_min * (beta_max / beta_min) ** ((torch.arange(T) - 1) / (T - 1))\n",
    "        super().__init__(betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30\n",
    "scheduler = ExponentialNoiseScheduler(T)\n",
    "\n",
    "x0 = torch.zeros(2, 3, 32, 32)\n",
    "t = torch.tensor([0, 10])\n",
    "xt, noise = scheduler.add_noise(x0, t)\n",
    "assert torch.allclose(xt[0], x0[0])\n",
    "\n",
    "x0_hat, xt_minus_1 = scheduler.remove_noise(xt, noise, t)\n",
    "assert torch.allclose(x0_hat, x0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SquareRootNoiseScheduler(NoiseScheduler):\n",
    "    # https://arxiv.org/pdf/2205.14217\n",
    "    def __init__(self, T: int, s: float = 0.008, min_alphas_cumprod: float = 1e-9):\n",
    "        self.s = s\n",
    "\n",
    "        alphas_cumprod = 1 - torch.sqrt(torch.arange(T) / T + s)\n",
    "        alphas_cumprod.clamp_(min=min_alphas_cumprod)\n",
    "        super().__init__(alphas_cumprod=alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30\n",
    "scheduler = SquareRootNoiseScheduler(T)\n",
    "\n",
    "x0 = torch.zeros(2, 3, 32, 32)\n",
    "t = torch.tensor([0, 10])\n",
    "xt, noise = scheduler.add_noise(x0, t)\n",
    "assert torch.allclose(xt[0], x0[0])\n",
    "\n",
    "x0_hat, xt_minus_1 = scheduler.remove_noise(xt, noise, t)\n",
    "assert torch.allclose(x0_hat, x0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: This noise scheduler explodes very quickly. Be careful with large T values.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "T = 1000\n",
    "alphas_cumprods = {\n",
    "    \"linear\": LinearNoiseScheduler(T).alphas_cumprod,\n",
    "    \"cosine\": CosineNoiseScheduler(T).alphas_cumprod,\n",
    "    \"sigmoid (tau=0.5)\": SigmoidNoiseScheduler(T, 0.5).alphas_cumprod,\n",
    "    \"sigmoid (tau=0.9)\": SigmoidNoiseScheduler(T, 0.9).alphas_cumprod,\n",
    "    \"sigmoid (tau=1.0)\": SigmoidNoiseScheduler(T).alphas_cumprod,\n",
    "    \"sigmoid (tau=1.1)\": SigmoidNoiseScheduler(T, 1.1).alphas_cumprod,\n",
    "    \"sigmoid (tau=1.5)\": SigmoidNoiseScheduler(T, 1.5).alphas_cumprod,\n",
    "    \"fibonacci\": FibonacciNoiseScheduler(T).alphas_cumprod,\n",
    "    \"exponential\": ExponentialNoiseScheduler(T).alphas_cumprod,\n",
    "    \"square root\": SquareRootNoiseScheduler(T).alphas_cumprod,\n",
    "}\n",
    "\n",
    "for key, value in alphas_cumprods.items():\n",
    "    plt.plot(torch.linspace(0, 1, T), value, label=key)\n",
    "plt.xlabel(\"t/T\")\n",
    "plt.ylabel(\"alphas_cumprod\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
