{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp schedulers/decaying_sine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.optim.lr_scheduler import LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DecayingSineScheduler:\n",
    "    def __init__(self, start_value: float, max_value: float, wavelength: float, decay: float):\n",
    "        assert 0.0 <= decay < 1.0, \"Decay must be between 0 and 1\"\n",
    "\n",
    "        self.start_value = start_value\n",
    "        self.max_value = max_value\n",
    "        self.wavelength = wavelength\n",
    "        self.decay_factor = 1 - decay\n",
    "\n",
    "        self.pseudo_max_value = max_value / (self.decay_factor**0.5)\n",
    "\n",
    "        self.x = 1\n",
    "\n",
    "    def get(self):\n",
    "        # Calculate angle based on current step and wavelength and get sine value\n",
    "        angle = (-0.5 + 2 * self.x / self.wavelength) * math.pi\n",
    "        sine = math.sin(angle)\n",
    "\n",
    "        # Scale it to the range of pseudo_max_lr and max_lr\n",
    "        scaled = (self.pseudo_max_value - self.start_value) * (1 + sine) / 2\n",
    "\n",
    "        # Apply decay to it\n",
    "        decayed = scaled * self.decay_factor ** ((self.x + 1) / self.wavelength)\n",
    "\n",
    "        # Increase it by the start_lr\n",
    "        lr = decayed + self.start_value\n",
    "\n",
    "        return lr\n",
    "\n",
    "    def step(self):\n",
    "        self.x = self.x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 0.4349480324496454\n",
      "Value: 0.7184766378640389\n",
      "Value: 0.4533281114977514\n",
      "Value: 0.10925400611220523\n",
      "Value: 0.0\n",
      "Value: 0.043494803244964526\n",
      "Value: 0.07184766378640382\n",
      "Value: 0.04533281114977513\n",
      "Value: 0.010925400611220529\n",
      "Value: 0.0\n",
      "Value: 0.004349480324496461\n",
      "Value: 0.007184766378640381\n",
      "Value: 0.0045332811149775155\n",
      "Value: 0.001092540061122056\n",
      "Value: 0.0\n"
     ]
    }
   ],
   "source": [
    "scheduler = DecayingSineScheduler(0, 1, 5, 0.9)\n",
    "\n",
    "for _ in range(15):\n",
    "    print(f\"Value: {scheduler.get()}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 0.04951556604879043\n",
      "Value: 0.18825509907063326\n",
      "Value: 0.3887395330218428\n",
      "Value: 0.6112604669781572\n",
      "Value: 0.8117449009293667\n",
      "Value: 0.9504844339512095\n",
      "Value: 1.0\n",
      "Value: 0.9504844339512095\n",
      "Value: 0.8117449009293667\n",
      "Value: 0.6112604669781573\n",
      "Value: 0.38873953302184283\n",
      "Value: 0.18825509907063348\n",
      "Value: 0.04951556604879048\n",
      "Value: 0.0\n",
      "Value: 0.04951556604879037\n"
     ]
    }
   ],
   "source": [
    "scheduler = DecayingSineScheduler(0, 1, 14, 0)\n",
    "\n",
    "for _ in range(15):\n",
    "    print(f\"Value: {scheduler.get()}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DecayingSineLR(LRScheduler):\n",
    "    def __init__(self, optimizer, start_lr, max_lr, wavelength, decay, last_epoch=-1, verbose=\"deprecated\"):\n",
    "        self.scheduler = DecayingSineScheduler(start_lr, max_lr, wavelength, decay)\n",
    "        self.scheduler.x -= 1  # To match the output of the non-LR scheduler\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = self.scheduler.get()\n",
    "        return [lr for _ in self.optimizer.param_groups]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        self.scheduler.step()\n",
    "        return super().step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [0.4349480324496454]\n",
      "Value: [0.7184766378640389]\n",
      "Value: [0.4533281114977514]\n",
      "Value: [0.10925400611220523]\n",
      "Value: [0.0]\n",
      "Value: [0.043494803244964526]\n",
      "Value: [0.07184766378640382]\n",
      "Value: [0.04533281114977513]\n",
      "Value: [0.010925400611220529]\n",
      "Value: [0.0]\n",
      "Value: [0.004349480324496461]\n",
      "Value: [0.007184766378640381]\n",
      "Value: [0.0045332811149775155]\n",
      "Value: [0.001092540061122056]\n",
      "Value: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniforge3/envs/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam([nn.Parameter()])\n",
    "scheduler = DecayingSineLR(optimizer, 0, 1, 5, 0.9)\n",
    "\n",
    "for _ in range(15):\n",
    "    print(f\"Value: {scheduler.get_lr()}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
