{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp schedulers/lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from vision_architectures.schedulers.cyclic import SineLR  # noqa:F401\n",
    "from vision_architectures.schedulers.sigmoid import SigmoidLR  # noqa:F401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ConstantLRWithWarmup(LambdaLR):\n",
    "    def __init__(self, optimizer, warmup_steps, min_lr_ratio: float = 0.001, *args, **kwargs):\n",
    "        def lambda_fn(step):\n",
    "            return min(1, min_lr_ratio + (1 - min_lr_ratio) * (step / warmup_steps))\n",
    "\n",
    "        super().__init__(optimizer, lambda_fn, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [0.001]\n",
      "Value: [0.1009]\n",
      "Value: [0.2008]\n",
      "Value: [0.30069999999999997]\n",
      "Value: [0.4006]\n",
      "Value: [0.5005]\n",
      "Value: [0.6003999999999999]\n",
      "Value: [0.7002999999999999]\n",
      "Value: [0.8002]\n",
      "Value: [0.9001]\n",
      "Value: [1]\n",
      "Value: [1]\n",
      "Value: [1]\n",
      "Value: [1]\n",
      "Value: [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniforge3/envs/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "/home/users/arjun.agarwal/miniforge3/envs/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam([nn.Parameter()], 1)\n",
    "scheduler = ConstantLRWithWarmup(optimizer, 10)\n",
    "\n",
    "for _ in range(15):\n",
    "    print(f\"Value: {scheduler.get_lr()}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
