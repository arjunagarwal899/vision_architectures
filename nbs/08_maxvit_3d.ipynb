{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp maxvit_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'svg'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "import copy\n",
    "graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# helpers\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cast_tuple(val, length = 1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * length)\n",
    "\n",
    "# helper classes\n",
    "class PreNormResidual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x)) + x\n",
    "\n",
    "class MaxViT3DMLP(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# MBConv\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),    # input C x H x W x D --> C x 1 X 1 x 1  ONE value of each channel\n",
    "            nn.Conv3d(in_channels, reduced_dim, kernel_size=1), # expansion\n",
    "            nn.SiLU(), # activation\n",
    "            nn.Conv3d(reduced_dim, in_channels, kernel_size=1), # brings it back\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.se(x)\n",
    "\n",
    "class CNNBlock3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0 , groups=1, act=True, bn=True, bias=False):\n",
    "        super(CNNBlock3d, self).__init__()\n",
    "        self.cnn = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias) #bias set to False as we are using BatchNorm\n",
    "        \n",
    "        # if groups = in_channels then it is for Depth wise convolutional; For each channel different Convolutional kernel\n",
    "        # very limited change in loss but a very high decrease in number of paramteres\n",
    "        # if groups = 1 : normal_conv kernel of size kernel_size**3\n",
    "\n",
    "        self.bn = nn.BatchNorm3d(out_channels) if bn else nn.Identity() \n",
    "        self.silu = nn.SiLU() if act else nn.Identity() ## SiLU <--> Swish same Thing\n",
    "        # 1 layer in MBConv doesn't have activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.silu(out)\n",
    "        return out\n",
    "\n",
    "#dropout\n",
    "class StochasticDepth(nn.Module):\n",
    "    def __init__(self, survival_prob=0.8):\n",
    "        super(StochasticDepth, self).__init__()\n",
    "        self.survival_prob =survival_prob\n",
    "        \n",
    "    def forward(self, x): #form of dropout , randomly remove some layers not during testing\n",
    "        if not self.training:\n",
    "            return x\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, 1, device= x.device) < self.survival_prob # maybe add 1 more here\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "    \n",
    "class MBConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, \n",
    "                 is_first = False,\n",
    "                 expand_ratio = 6, \n",
    "                 reduction = 4, #squeeze excitation 1/4 = 0.25\n",
    "                 survival_prob =0.8 # for stocastic depth\n",
    "                 ):\n",
    "        super(MBConv3d, self).__init__()\n",
    "        \n",
    "        stride = 2 if is_first else 1\n",
    "        survival_prob = 0.8\n",
    "        self.use_residual = True if not is_first else False\n",
    "        hidden_dim = int(out_channels * expand_ratio)\n",
    "        reduced_dim = int(in_channels/reduction)\n",
    "        padding = padding\n",
    "        \n",
    "        ##expansion phase\n",
    "        self.expand = nn.Identity() if (expand_ratio == 1) else CNNBlock3d(in_channels, hidden_dim, kernel_size = 1)\n",
    "        \n",
    "        ##Depthwise convolution phase\n",
    "        self.depthwise_conv = CNNBlock3d(hidden_dim, hidden_dim,\n",
    "                                        kernel_size = kernel_size, stride = stride, \n",
    "                                        padding = padding, groups = hidden_dim\n",
    "                                       )\n",
    "        \n",
    "        # Squeeze Excitation phase\n",
    "        self.se = SqueezeExcitation(hidden_dim, reduced_dim = reduced_dim)\n",
    "        \n",
    "        #output phase\n",
    "        self.pointwise_conv = CNNBlock3d(hidden_dim, out_channels, kernel_size = 1, stride = 1, act = False, padding = 0)\n",
    "        # add Sigmoid Activation as mentioned in the paper\n",
    "        \n",
    "        # drop connect\n",
    "        self.drop_layers = StochasticDepth(survival_prob = survival_prob)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.expand(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        \n",
    "        if self.use_residual:  #and self.depthwise_conv.stride[0] == 1:\n",
    "            x = self.drop_layers(x)\n",
    "            x += residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# attention related classes\n",
    "class MaxViT3DMHSA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_per_head = 32,\n",
    "        dropout = 0.,\n",
    "        window_size = (7,7,7)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % dim_per_head) == 0, 'dimension should be divisible by dimension per head'\n",
    "\n",
    "        self.heads = dim // dim_per_head\n",
    "        self.scale = dim_per_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias = False)\n",
    "\n",
    "        # self.attend = nn.Sequential(\n",
    "        #     nn.Softmax(dim = -1),\n",
    "        #     nn.Dropout(dropout)\n",
    "        # )\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(dim, dim, bias = False),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # relative positional bias\n",
    "        w1,w2,w3 = window_size\n",
    "        self.rel_pos_bias = nn.Embedding((2 * w1 - 1) *(2 * w2 - 1)*(2 * w3 - 1), self.heads)\n",
    "        pos1 = torch.arange(w1, dtype=torch.int32)\n",
    "        pos2 = torch.arange(w2, dtype=torch.int32)\n",
    "        pos3 = torch.arange(w3, dtype=torch.int32)\n",
    "        # First we use the torch.arange and torch.meshgrid functions to generate the corresponding coordinates, [3,H,W,D] \n",
    "        # and then stack them up and expand them into a two-dimensional vector to get the absolute position index.\n",
    "        grid = torch.stack(torch.meshgrid(pos1, pos2, pos3, indexing = 'ij'))\n",
    "        grid = rearrange(grid, 'c i j k -> (i j k) c')\n",
    "        # insert a dimension in the first dimension and the second dimension respectively, perform broadcast subtraction, and obtain the tensor of 3, whd*ww, whd*ww\n",
    "        rel_pos = rearrange(grid, 'i ... -> i 1 ...') - rearrange(grid, 'j ... -> 1 j ...') \n",
    "        rel_pos[...,0] += w1 - 1\n",
    "        rel_pos[...,1] += w2 - 1\n",
    "        rel_pos[...,2] += w3 - 1\n",
    "        # Do a multiplication operation to distinguish, sum up the last dimension, and expand it into a one-dimensional coordinate   a*x1 + b*x2 + c*x3  (a= hd b=d c =1) \n",
    "        rel_pos_indices = (rel_pos * torch.tensor([(2 *w2 - 1)*(2 *w3 - 1), (2 *w3 - 1), 1])).sum(dim = -1)\n",
    "        \n",
    "        # Register as a variable that does not participate in learning\n",
    "        self.register_buffer('rel_pos_indices', rel_pos_indices, persistent = False)\n",
    "        self.dropout_prob = dropout\n",
    "               \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, height, width, depth, window_height, window_width, window_depth ,_ , h = *x.shape, self.heads\n",
    "        # flatten\n",
    "        x = rearrange(x, 'b x y z w1 w2 w3 d -> (b x y z) (w1 w2 w3) d')\n",
    "        # project for queries, keys, values\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "\n",
    "        # split heads\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d ) -> b h n d', h = h), (q, k, v))\n",
    "        # scale\n",
    "        q = q * self.scale\n",
    "\n",
    "        # sim\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "\n",
    "        # add positional bias\n",
    "        bias = self.rel_pos_bias(self.rel_pos_indices)\n",
    "        sim = sim + rearrange(bias, 'i j h -> h i j')\n",
    "\n",
    "        # # attention\n",
    "        # attn = self.attend(sim)\n",
    "\n",
    "        context = F.scaled_dot_product_attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            dropout_p=self.dropout_prob,\n",
    "            is_causal=False,\n",
    "            scale=1.0,  # Already scaled the vectors\n",
    "        )\n",
    "        \n",
    "        # # aggregate\n",
    "        # out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "\n",
    "        # merge heads\n",
    "        out = rearrange(context, 'b h (w1 w2 w3) d -> b w1 w2 w3 (h d)', w1 = window_height, w2 = window_width, w3 = window_depth)\n",
    "\n",
    "        # combine heads out\n",
    "        out = self.to_out(out)\n",
    "        return rearrange(out, '(b x y z) ... -> b x y z ...', x = height, y = width, z = depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MaxViT3DStem0(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.conv_stem = nn.Sequential(\n",
    "            nn.Conv3d(self.config['in_channels'], self.config['hidden_dim'], kernel_size=3, stride = 2, padding = 1),\n",
    "            nn.Conv3d(self.config['hidden_dim'], self.config['out_channels'], 3, padding = 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 54, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_stage_config ={ \n",
    "    \"in_channels\": 54,           # dimension of first layer, doubles every layer\n",
    "    \"hidden_dim\": 98,            # dimension of attention heads, kept at 32 in paper`\n",
    "    \"out_channels\": 54           # window size for block and grids\n",
    "}\n",
    "\n",
    "maxvit_block = MaxViT3DStem0(test_stage_config)\n",
    "img = torch.randn(2, 54, 32, 32, 24)\n",
    "preds = maxvit_block(img) \n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MaxViT3DBlock(nn.Module):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stage_config = copy.deepcopy(stage_config)\n",
    "        \n",
    "        w1,w2,w3 = stage_config['window_size']\n",
    "        stem_dim_in = stage_config['stem_dim_in']       \n",
    "        dim = stage_config['dim']\n",
    "        dim_per_head = stage_config['dim_per_head']\n",
    "        dropout = stage_config['dropout']\n",
    "        window_size = stage_config['window_size']\n",
    "        expansion_rate = stage_config['expansion_rate']\n",
    "        shrinkage_rate = stage_config['shrinkage_rate']\n",
    "        is_first = stage_config['is_first']\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            MBConv3d(\n",
    "                in_channels = stem_dim_in if is_first else dim, \n",
    "                out_channels = dim,\n",
    "                kernel_size=3 ,  \n",
    "                padding=1 , \n",
    "                is_first=is_first,\n",
    "                expand_ratio = expansion_rate if expansion_rate is not None else 4, \n",
    "                reduction = shrinkage_rate if shrinkage_rate is not None else 4, #squeeze excitation 1/4 = 0.25\n",
    "                survival_prob = 1-dropout # for stocastic depth\n",
    "            ),\n",
    "            \n",
    "            Rearrange('b d (x w1) (y w2) (z w3) -> b x y z w1 w2 w3 d', w1 = w1, w2 = w2, w3 = w3),  # block-like attention\n",
    "            PreNormResidual(dim, MaxViT3DMHSA(dim = dim, dim_per_head = dim_per_head, dropout = dropout, window_size = window_size)),\n",
    "            PreNormResidual(dim, MaxViT3DMLP(dim = dim, dropout = dropout)),\n",
    "            Rearrange('b x y z w1 w2 w3 d -> b d (x w1) (y w2) (z w3)'),\n",
    "\n",
    "            Rearrange('b d (w1 x) (w2 y) (w3 z) -> b x y z w1 w2 w3 d', w1 = w1, w2 = w2, w3 = w3),  # grid-like attention\n",
    "            PreNormResidual(dim, MaxViT3DMHSA(dim = dim, dim_per_head = dim_per_head, dropout = dropout, window_size = window_size)),\n",
    "            PreNormResidual(dim, MaxViT3DMLP(dim = dim, dropout = dropout)),\n",
    "            Rearrange('b x y z w1 w2 w3 d -> b d (w1 x) (w2 y) (w3 z)'),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 32, 32, 24])\n",
      "torch.Size([2, 128, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_stage_config = {\n",
    "    \"stem_dim_in\": 64,             # used in first block to downsample z,x,y\n",
    "    \"dim\": 128,                    # dimension of whole layer, doubles every layer\n",
    "    \"is_first\": True,\n",
    "    \"dim_per_head\": 8,            # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4,4,4),        # window size for block and grids\n",
    "    \"dropout\": 0.1,               # dropout\n",
    "    \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,        # (squeeze) and excitation\n",
    "}\n",
    "\n",
    "maxvit_block = MaxViT3DBlock(test_stage_config)\n",
    "img = torch.randn(2, 64, 32, 32, 24)\n",
    "preds = maxvit_block(img) \n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 32, 32, 24])\n",
      "torch.Size([2, 128, 32, 32, 24])\n"
     ]
    }
   ],
   "source": [
    "test_stage_config = {\n",
    "    \"stem_dim_in\": 64,             # used in first block to downsample z,x,y\n",
    "    \"dim\": 128,                    # dimension of whole layer, doubles every layer\n",
    "    \"is_first\": False,\n",
    "    \"dim_per_head\": 8,            # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4,4,4),        # window size for block and grids\n",
    "    \"dropout\": 0.1,               # dropout\n",
    "    \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,        # (squeeze) and excitation\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    maxvit_block = MaxViT3DBlock(test_stage_config)\n",
    "    img = torch.randn(2, 128, 32, 32, 24)\n",
    "    preds = maxvit_block(img) \n",
    "    print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# # device='meta' -> no memory is consumed for visualization\n",
    "# model_graph = draw_graph(maxvit_block, input_size=(1, 256, 32, 32, 24), device='meta')\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class MaxViT3DStem(nn.Module):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = copy.deepcopy(stage_config)\n",
    "        self.first_block_config = copy.deepcopy(stage_config)\n",
    "\n",
    "        self.first_block_config[\"is_first\"] = True\n",
    "        self.config[\"is_first\"] = False\n",
    "         \n",
    "        self.stem = nn.ModuleList([])\n",
    "        self.stem.append(MaxViT3DBlock(self.first_block_config))\n",
    "        for i in range(self.config[\"num_maxvit_blocks\"]-1):\n",
    "            self.stem.append(MaxViT3DBlock(self.config))\n",
    "            \n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        for layer in self.stem:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 32, 32, 24])\n",
      "torch.Size([2, 64, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_stage_config = {\n",
    "    \"num_maxvit_blocks\": 1,\n",
    "    \"stem_dim_in\": 32,\n",
    "    \"dim\": 64,                     # dimension of stem, doubles every stem\n",
    "    \"dim_per_head\": 8,             # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4,4,4),        # window size for block and grids\n",
    "    \"dropout\": 0.1,                # dropout\n",
    "    \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,        # (squeeze) and excitation  \n",
    "}\n",
    "\n",
    "test = MaxViT3DStem(test_stage_config)\n",
    "img = torch.randn(2, 32, 32, 32, 24)\n",
    "preds = test(img)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MaxViT3DEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.stems = nn.ModuleList([])\n",
    "        \n",
    "        self.stems.append(MaxViT3DStem0(config[\"stem0\"]))\n",
    "        \n",
    "        for stage_config in config[\"stems\"]:\n",
    "            self.stems.append(MaxViT3DStem(stage_config))\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "\n",
    "        for stem in self.stems:\n",
    "            hidden_states = stem(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "test_encoder_config ={ \n",
    "    \"stem0\": {\n",
    "        \"in_channels\": 1,         \n",
    "        \"hidden_dim\": 32,         \n",
    "        \"out_channels\": 32        \n",
    "    },\n",
    "    \n",
    "    \"stems\": \n",
    "    [{\n",
    "        \"num_maxvit_blocks\": 2,\n",
    "        \"stem_dim_in\": 32,\n",
    "        \"dim\": 64,                     # dimension of first layer, doubles every layer\n",
    "        \"dim_per_head\": 8,             # dimension of attention heads, kept at 32 in paper`\n",
    "        \"window_size\": (4,4,4),        # window size for block and grids\n",
    "        \"dropout\": 0.1,                # dropout\n",
    "        \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "        \"shrinkage_rate\": None,        # (squeeze) and excitation \n",
    "    },\n",
    "    {\n",
    "        \"num_maxvit_blocks\": 2,\n",
    "        \"stem_dim_in\": 64,\n",
    "        \"dim\": 128,                     # dimension of first layer, doubles every layer\n",
    "        \"dim_per_head\": 8,             # dimension of attention heads, kept at 32 in paper`\n",
    "        \"window_size\": (4,4,4),        # window size for block and grids\n",
    "        \"dropout\": 0.1,                # dropout\n",
    "        \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "        \"shrinkage_rate\": None,        # (squeeze) and excitation },    \n",
    "    },\n",
    "    {        \n",
    "        \"num_maxvit_blocks\": 2,\n",
    "        \"stem_dim_in\": 128,\n",
    "        \"dim\": 256,                     # dimension of first layer, doubles every layer\n",
    "        \"dim_per_head\": 8,             # dimension of attention heads, kept at 32 in paper`\n",
    "        \"window_size\": (2,2,2),        # window size for block and grids\n",
    "        \"dropout\": 0.1,                # dropout\n",
    "        \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "        \"shrinkage_rate\": None,        # (squeeze) and excitation },}\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MaxViT3DEncoder(test_encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 16, 128, 128])\n",
      "torch.Size([2, 64, 8, 64, 64])\n",
      "torch.Size([2, 64, 8, 64, 64])\n",
      "torch.Size([2, 128, 4, 32, 32])\n",
      "torch.Size([2, 128, 4, 32, 32])\n",
      "torch.Size([2, 256, 2, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-2.2867e+00,  1.1482e-02,  6.1498e-02,  ..., -1.7228e+00,\n",
       "             8.2716e-01,  4.7021e-01],\n",
       "           [-1.1849e-01, -1.3532e+00,  5.1944e-01,  ..., -6.4874e-01,\n",
       "            -1.3123e+00,  1.0021e+00],\n",
       "           [-1.2210e+00,  3.9233e-01,  4.0349e+00,  ...,  1.9783e+00,\n",
       "            -9.1644e-01,  1.5266e+00],\n",
       "           ...,\n",
       "           [-7.1388e-01,  1.9040e+00, -2.7420e+00,  ..., -1.9899e+00,\n",
       "             1.9202e+00, -1.0261e+00],\n",
       "           [ 1.2139e+00,  3.5951e+00,  6.5220e-01,  ..., -7.0527e-01,\n",
       "             9.6935e-01, -2.2203e+00],\n",
       "           [-1.8401e+00, -7.3885e-01,  1.0114e+00,  ...,  1.6035e+00,\n",
       "            -2.6275e+00, -4.1875e-01]],\n",
       "\n",
       "          [[ 9.3094e-01, -1.0267e+00, -1.3423e+00,  ..., -4.0322e+00,\n",
       "            -1.7851e+00, -4.7488e-01],\n",
       "           [ 1.8620e-01,  2.3663e-01,  7.4904e-01,  ...,  5.9993e+00,\n",
       "             5.1404e-01, -8.9890e-01],\n",
       "           [ 4.4538e-01,  2.2139e+00, -2.0797e+00,  ...,  1.5005e+00,\n",
       "             2.2805e+00, -1.2135e+00],\n",
       "           ...,\n",
       "           [-1.2536e+00, -5.6823e-01,  1.7476e+00,  ...,  3.7043e-01,\n",
       "             1.1953e+00, -3.5082e+00],\n",
       "           [-1.1344e+00, -3.1606e+00, -6.3465e+00,  ...,  2.1488e+00,\n",
       "            -4.7945e-01, -2.6581e+00],\n",
       "           [-1.3155e+00,  3.3076e+00, -4.2048e-01,  ...,  1.0883e-01,\n",
       "            -4.2669e-01, -2.4076e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.0703e+00, -6.9323e-01, -6.2349e-02,  ...,  6.5222e-01,\n",
       "            -2.6835e+00, -8.7695e-01],\n",
       "           [ 1.2411e+00, -2.2814e-01, -1.3266e+00,  ...,  3.3813e-01,\n",
       "             1.9471e+00,  9.1133e-01],\n",
       "           [ 9.2264e-01,  1.7282e+00,  2.3270e+00,  ...,  1.4142e+00,\n",
       "             1.0260e+00,  5.0683e-01],\n",
       "           ...,\n",
       "           [ 1.8879e+00,  1.7796e+00,  1.9119e+00,  ...,  3.6201e+00,\n",
       "             3.1139e+00, -2.5291e+00],\n",
       "           [ 2.9937e+00, -4.1660e-01, -7.4098e-01,  ..., -2.4102e+00,\n",
       "             3.3167e-01, -3.8789e+00],\n",
       "           [ 2.0269e+00, -9.1060e-01,  2.0876e+00,  ..., -1.9523e+00,\n",
       "             2.6492e-01, -2.1725e+00]],\n",
       "\n",
       "          [[-7.5548e-03,  2.8814e+00, -1.0977e-01,  ...,  8.4724e-01,\n",
       "             2.1622e+00,  2.1645e+00],\n",
       "           [ 2.9360e+00,  1.9672e+00, -2.1836e+00,  ...,  4.1584e+00,\n",
       "             8.3708e-01, -1.9296e+00],\n",
       "           [ 1.7823e+00,  1.0143e+00, -1.1649e+00,  ...,  3.5478e+00,\n",
       "             3.9266e+00, -3.4149e+00],\n",
       "           ...,\n",
       "           [ 2.7609e+00,  3.1275e+00,  4.2128e+00,  ...,  4.6083e+00,\n",
       "             8.0305e-01, -3.4218e+00],\n",
       "           [ 2.9855e+00,  6.7477e-01,  6.9498e+00,  ..., -1.2219e+00,\n",
       "             1.9760e+00, -6.7735e-02],\n",
       "           [ 2.4293e+00,  8.6556e-01, -6.7204e-01,  ..., -1.4935e-01,\n",
       "            -3.0871e-01,  7.1837e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 2.1316e+00, -1.6300e+00,  1.2832e+00,  ..., -6.7965e-01,\n",
       "            -2.7421e-01, -2.4938e+00],\n",
       "           [ 5.0162e-01, -1.9942e+00, -9.8782e-02,  ...,  8.9476e-01,\n",
       "             1.2680e+00, -9.1726e-01],\n",
       "           [ 2.4219e+00,  9.5207e-02, -6.1233e-01,  ...,  1.3944e-01,\n",
       "             1.7217e+00, -9.9701e-01],\n",
       "           ...,\n",
       "           [ 4.9874e-01,  1.9515e+00,  1.1421e+00,  ...,  4.7349e+00,\n",
       "             2.7770e-01,  5.9924e-02],\n",
       "           [-2.7205e+00, -6.0879e-01,  9.1882e-02,  ...,  3.8365e-01,\n",
       "            -1.3826e+00,  3.3002e-01],\n",
       "           [ 9.0756e-01,  5.0839e-01,  2.9234e-01,  ...,  2.5169e-01,\n",
       "             2.9441e+00,  2.2132e+00]],\n",
       "\n",
       "          [[-1.2232e-01,  1.9225e+00, -1.1554e-01,  ...,  6.1877e-02,\n",
       "            -6.1488e-03, -3.5171e-01],\n",
       "           [-6.9637e-02, -1.4441e+00,  1.9693e+00,  ..., -2.3297e+00,\n",
       "             1.6598e-01, -7.2765e-01],\n",
       "           [ 8.4679e-01,  2.4881e-01, -1.2890e+00,  ..., -2.1847e+00,\n",
       "            -1.8047e-01, -8.9993e-02],\n",
       "           ...,\n",
       "           [-4.3157e+00, -6.9611e-02, -4.5044e+00,  ...,  3.7788e+00,\n",
       "            -4.9315e+00, -1.8025e+00],\n",
       "           [-3.0837e+00, -3.7073e+00,  6.6404e-01,  ..., -2.9997e+00,\n",
       "             5.5840e+00, -2.8050e+00],\n",
       "           [-9.9276e-02, -2.0369e+00, -3.5582e+00,  ..., -8.8544e-01,\n",
       "             8.4358e-01, -1.7840e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-4.2449e-01,  2.8255e-01, -1.5916e+00,  ...,  8.8557e-01,\n",
       "            -1.0454e+00, -2.7920e+00],\n",
       "           [-2.4033e+00, -3.6796e-01,  1.3179e+00,  ..., -3.2537e+00,\n",
       "             1.8830e+00,  2.1642e+00],\n",
       "           [-9.8880e-01,  3.4053e+00, -1.2357e+00,  ...,  2.4408e+00,\n",
       "             1.3064e+00, -1.3045e+00],\n",
       "           ...,\n",
       "           [-1.6429e+00,  3.6553e+00,  7.9744e-01,  ...,  1.6981e-01,\n",
       "            -1.7765e+00, -8.2410e-01],\n",
       "           [-3.1229e+00, -1.1031e+00,  2.3049e+00,  ...,  2.7316e+00,\n",
       "             4.1665e+00, -6.4481e-01],\n",
       "           [-1.5053e+00, -8.3761e-01, -1.0929e+00,  ..., -1.2236e+00,\n",
       "             6.3504e-01, -2.1532e+00]],\n",
       "\n",
       "          [[ 6.6018e-01,  4.1373e-01, -2.2952e-01,  ..., -2.7879e-01,\n",
       "             7.9485e-02,  2.5494e-01],\n",
       "           [-5.8372e-02, -3.9287e-01, -1.1648e+00,  ...,  6.8449e-01,\n",
       "             6.3353e-01,  6.9985e-02],\n",
       "           [ 2.8622e-01, -6.1177e-03,  2.1171e+00,  ..., -1.3625e-01,\n",
       "            -4.0741e-01, -3.1302e+00],\n",
       "           ...,\n",
       "           [ 1.8754e+00, -2.5888e-01,  3.1896e+00,  ...,  1.1178e-01,\n",
       "            -1.0787e+00,  2.2632e+00],\n",
       "           [ 1.7058e+00,  3.2322e+00, -3.5709e+00,  ...,  4.3214e+00,\n",
       "             2.1451e+00, -1.0195e+00],\n",
       "           [-1.4186e+00,  6.5298e-01,  8.3756e-01,  ...,  1.1610e-01,\n",
       "            -5.2691e+00, -6.3144e-01]]],\n",
       "\n",
       "\n",
       "         [[[-7.7736e-02,  1.6161e+00, -6.5878e-01,  ...,  1.5696e-01,\n",
       "            -1.6301e+00,  1.4259e+00],\n",
       "           [ 1.3053e+00,  8.5902e-01,  1.9288e+00,  ..., -1.5368e+00,\n",
       "             1.0274e+00,  2.0906e+00],\n",
       "           [-8.1321e-01,  1.1154e-03,  7.8546e-01,  ..., -3.5408e+00,\n",
       "             3.5492e-01,  5.2443e-01],\n",
       "           ...,\n",
       "           [ 1.2703e+00, -1.5010e+00, -1.0092e+00,  ..., -5.6572e-01,\n",
       "            -1.3205e+00,  8.9114e-01],\n",
       "           [-5.5721e-01,  1.3436e+00,  4.1636e-01,  ...,  1.7560e+00,\n",
       "             4.8978e-01, -3.1211e-01],\n",
       "           [ 5.9415e-01, -7.4399e-01,  6.8139e-01,  ..., -2.6731e+00,\n",
       "             2.1653e+00, -6.5678e-01]],\n",
       "\n",
       "          [[ 1.1270e+00,  2.1017e+00, -1.3166e-01,  ...,  3.6580e+00,\n",
       "             1.2882e+00,  2.0308e+00],\n",
       "           [ 5.4001e+00,  1.2063e+00,  5.3553e-02,  ..., -2.4303e+00,\n",
       "             1.4074e+00, -2.1382e-01],\n",
       "           [ 2.6461e+00,  1.0460e+00, -3.1693e+00,  ...,  2.1893e+00,\n",
       "             7.1713e+00,  1.4029e+00],\n",
       "           ...,\n",
       "           [-9.7216e-01,  2.5520e+00, -3.0845e+00,  ...,  1.2453e+00,\n",
       "             2.8549e-01,  4.0436e+00],\n",
       "           [ 1.9408e+00, -4.7013e-01,  3.1651e+00,  ..., -1.3952e+00,\n",
       "             1.8224e+00,  4.7770e+00],\n",
       "           [ 4.1158e-01,  6.0979e-01, -2.1411e+00,  ...,  9.1862e-01,\n",
       "             2.2932e+00,  2.3521e+00]]],\n",
       "\n",
       "\n",
       "         [[[-8.4852e-02,  2.9865e+00, -1.1765e+00,  ...,  1.6739e+00,\n",
       "             9.1976e-02, -3.2017e-01],\n",
       "           [-8.1085e-01, -2.3880e+00, -2.0328e+00,  ...,  2.4000e+00,\n",
       "            -2.5105e+00, -1.4804e-01],\n",
       "           [ 1.2254e+00,  1.9597e-02,  1.0795e+00,  ..., -1.7607e+00,\n",
       "            -3.8357e-01,  3.0434e+00],\n",
       "           ...,\n",
       "           [ 6.9043e-01,  3.0231e+00, -5.2165e+00,  ..., -1.5653e-01,\n",
       "            -2.7134e+00, -2.4807e-01],\n",
       "           [ 2.1541e+00, -9.0270e-01, -3.4498e+00,  ...,  4.0983e+00,\n",
       "             4.6699e+00,  1.7102e+00],\n",
       "           [-5.1652e-01,  2.9757e+00,  1.5628e+00,  ...,  4.2845e-01,\n",
       "             2.7561e+00, -1.8094e+00]],\n",
       "\n",
       "          [[ 7.3582e-02,  1.2515e+00, -1.4268e+00,  ..., -6.1643e-01,\n",
       "             1.3356e+00, -2.1574e+00],\n",
       "           [ 2.1847e+00, -3.1296e+00, -9.8680e-01,  ..., -3.3742e-01,\n",
       "            -2.5037e+00,  7.1569e-01],\n",
       "           [ 1.1441e+00, -3.6272e+00,  5.0961e-01,  ..., -1.3887e+00,\n",
       "            -2.1691e+00,  7.1953e-01],\n",
       "           ...,\n",
       "           [-1.6214e-01, -6.1971e-01, -2.2369e+00,  ..., -2.2471e+00,\n",
       "             9.4822e-01, -1.8141e+00],\n",
       "           [-2.2460e+00,  1.0446e+00,  5.2891e+00,  ..., -5.4322e+00,\n",
       "             1.1887e+00, -2.0874e-01],\n",
       "           [ 1.0162e+00,  3.2107e-01,  1.4387e+00,  ..., -1.5460e+00,\n",
       "            -2.4853e+00, -3.8236e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-2.2127e-02,  9.4257e-01, -5.9231e-02,  ..., -3.3917e-01,\n",
       "             1.1888e-01, -5.5372e-01],\n",
       "           [-5.0478e-01,  1.2013e-01, -1.6696e-01,  ...,  2.8122e-01,\n",
       "             1.0426e+00, -2.7053e-01],\n",
       "           [ 9.2962e-01,  4.0967e-02,  3.3485e-01,  ...,  3.1035e-01,\n",
       "             6.0973e-01,  1.3178e+00],\n",
       "           ...,\n",
       "           [ 9.6938e-01,  4.8549e-01, -4.1272e-01,  ...,  2.1943e-01,\n",
       "            -9.1941e-01, -6.8289e-01],\n",
       "           [-7.9189e-01, -2.0042e-01, -6.6076e-02,  ...,  3.4888e-01,\n",
       "            -7.4386e-01,  1.0018e-01],\n",
       "           [-6.7766e-01,  6.1255e-01, -1.4469e-01,  ...,  1.0767e-01,\n",
       "            -1.1565e-01,  3.7447e-01]],\n",
       "\n",
       "          [[ 1.0308e-01,  4.6204e-01,  3.0055e-01,  ...,  2.0210e-01,\n",
       "            -3.8269e-01, -1.2071e-01],\n",
       "           [ 3.5241e-01,  1.5903e-02,  4.5753e-01,  ..., -1.5901e+00,\n",
       "             2.7487e-01, -9.4484e-01],\n",
       "           [ 2.0988e+00,  9.5350e-02, -8.6708e-01,  ..., -5.6478e-01,\n",
       "            -1.1162e-02, -4.6849e-01],\n",
       "           ...,\n",
       "           [-2.1113e-01, -1.2266e-01, -7.3539e-01,  ...,  6.8506e-01,\n",
       "            -2.4912e-01, -1.0585e+00],\n",
       "           [-3.7779e-01, -4.8145e-01, -1.0909e+00,  ..., -3.3945e-01,\n",
       "             6.4244e-01, -1.2750e+00],\n",
       "           [ 4.4523e-01,  4.1044e-01,  3.6192e-02,  ..., -8.9284e-01,\n",
       "             6.4628e-01, -1.4640e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.6244e+00, -1.4544e+00, -1.3873e+00,  ..., -1.6503e+00,\n",
       "            -1.7223e+00, -9.6677e-01],\n",
       "           [-1.7435e+00, -1.0266e+00, -2.0505e+00,  ..., -9.2829e-01,\n",
       "            -2.4846e-01, -1.7543e+00],\n",
       "           [ 1.7546e-01, -1.9275e+00, -1.4333e+00,  ..., -7.2858e-01,\n",
       "            -6.6068e-01, -1.2140e+00],\n",
       "           ...,\n",
       "           [-7.9859e-01, -1.7119e+00, -1.9984e+00,  ..., -8.5327e-01,\n",
       "            -1.2958e+00, -5.1904e-01],\n",
       "           [-1.1778e+00, -1.8529e+00, -8.8596e-01,  ..., -1.5049e+00,\n",
       "            -8.2865e-01, -9.1019e-01],\n",
       "           [-1.4849e+00, -2.2784e+00, -4.9942e-01,  ..., -1.5599e+00,\n",
       "            -3.8851e-01, -1.3530e+00]],\n",
       "\n",
       "          [[-1.5280e+00, -1.9861e+00, -1.4980e+00,  ..., -2.3216e+00,\n",
       "            -1.2534e+00, -9.9716e-01],\n",
       "           [-1.2343e+00, -1.5752e+00, -1.6936e+00,  ..., -1.3066e+00,\n",
       "            -1.0110e+00, -1.5871e+00],\n",
       "           [-7.9314e-01, -1.5451e+00, -1.1745e+00,  ..., -1.5415e+00,\n",
       "            -2.0695e+00, -6.6024e-01],\n",
       "           ...,\n",
       "           [-1.2325e+00, -9.9458e-01, -1.5147e+00,  ...,  3.7172e-01,\n",
       "            -7.1804e-01, -1.7521e+00],\n",
       "           [-1.8365e+00, -1.1999e+00, -1.5282e+00,  ..., -2.5538e+00,\n",
       "            -5.8019e-01, -4.4524e-01],\n",
       "           [-1.8277e+00, -1.3959e+00, -6.6108e-01,  ..., -9.9041e-01,\n",
       "            -1.2202e+00, -4.7800e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 2.7346e-01, -9.7839e-01, -1.3781e+00,  ..., -1.4876e+00,\n",
       "            -9.4552e-01, -1.3026e+00],\n",
       "           [ 1.2232e+00, -1.1856e+00, -4.0144e-01,  ..., -7.2418e-01,\n",
       "            -4.4001e-01, -8.1476e-01],\n",
       "           [-1.1011e-01, -7.3494e-01, -5.0868e-01,  ...,  4.9216e-01,\n",
       "            -2.6215e-01, -1.3395e-01],\n",
       "           ...,\n",
       "           [-8.9784e-01,  4.4860e-01,  1.2671e+00,  ..., -1.7293e+00,\n",
       "            -1.2247e-01, -1.4346e-01],\n",
       "           [-5.1934e-01,  3.9293e-01, -1.1413e+00,  ..., -5.5712e-01,\n",
       "            -3.3234e-01, -5.1603e-01],\n",
       "           [-3.2839e-01, -3.1082e-01, -1.8231e-01,  ...,  3.6879e-02,\n",
       "            -3.4385e-01, -7.8345e-01]],\n",
       "\n",
       "          [[ 1.2076e-01, -2.1618e-01, -8.4153e-01,  ..., -7.0948e-01,\n",
       "            -7.1206e-01, -1.6125e-01],\n",
       "           [ 3.9983e-01, -1.3047e+00, -5.1150e-01,  ..., -8.5177e-01,\n",
       "            -9.7806e-01, -4.8674e-01],\n",
       "           [-1.3971e-01, -1.1108e+00, -6.5862e-01,  ..., -1.0735e+00,\n",
       "            -9.6819e-01, -1.0994e+00],\n",
       "           ...,\n",
       "           [-6.3041e-01, -1.0561e-01, -1.2015e+00,  ..., -8.1412e-01,\n",
       "             2.5803e-01,  4.7771e-01],\n",
       "           [ 7.4388e-01, -7.6710e-01,  4.3458e-02,  ..., -7.4588e-01,\n",
       "             7.2684e-01, -4.5591e-01],\n",
       "           [-9.3376e-01, -1.8449e+00, -5.2626e-01,  ...,  1.9887e-01,\n",
       "             5.1764e-01, -7.0805e-01]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 2.6831e-01,  1.9649e-02,  3.7829e-01,  ...,  6.7034e-02,\n",
       "             6.2260e-01, -1.7653e-01],\n",
       "           [ 6.9840e-01,  5.3347e-01, -2.1083e-01,  ...,  3.5245e-01,\n",
       "             7.5797e-01,  2.3250e-01],\n",
       "           [-4.8812e-01, -7.1987e-01, -3.3922e-01,  ...,  6.7759e-01,\n",
       "            -3.5608e-01,  4.3909e-01],\n",
       "           ...,\n",
       "           [-8.8383e-02, -1.7886e-01,  4.2748e-01,  ...,  1.0592e+00,\n",
       "             4.9943e-01,  9.1842e-01],\n",
       "           [ 3.0221e-01,  1.9392e-01,  1.9271e-01,  ...,  9.4995e-01,\n",
       "            -6.5526e-01,  7.9654e-01],\n",
       "           [ 1.6567e-01,  4.8772e-03,  1.1711e+00,  ...,  2.4263e-01,\n",
       "            -9.1545e-02, -1.4110e-01]],\n",
       "\n",
       "          [[-1.0242e-01,  7.8169e-01,  5.6017e-01,  ..., -1.1298e-01,\n",
       "            -4.9308e-02,  1.2610e-01],\n",
       "           [ 1.3838e-01,  4.2589e-03, -3.8184e-01,  ...,  5.0462e-01,\n",
       "            -1.6284e+00, -7.6670e-01],\n",
       "           [-5.3025e-01, -1.8707e-01,  3.5077e-01,  ..., -4.0426e-01,\n",
       "            -7.3514e-01,  6.1714e-01],\n",
       "           ...,\n",
       "           [ 2.6799e-01, -7.5239e-01,  1.2853e-01,  ...,  3.4494e-01,\n",
       "             6.9038e-01,  1.8435e-01],\n",
       "           [ 8.8794e-01,  6.4070e-02,  1.0151e+00,  ..., -9.8201e-01,\n",
       "             2.8533e-01, -2.1321e-01],\n",
       "           [-1.9103e-01,  8.1174e-01,  7.9602e-01,  ...,  4.2696e-01,\n",
       "            -9.2989e-01, -1.4549e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 3.4771e-01, -9.0628e-01, -3.1773e-01,  ..., -1.5206e-01,\n",
       "             5.7531e-01,  5.5755e-01],\n",
       "           [-1.0442e-01, -8.2121e-01,  3.3693e-01,  ..., -4.5605e-01,\n",
       "             1.2519e-01,  7.0611e-01],\n",
       "           [-2.5363e-02,  7.6828e-02, -8.0590e-01,  ..., -2.7482e-01,\n",
       "            -5.6313e-01, -4.2684e-01],\n",
       "           ...,\n",
       "           [ 4.1426e-01,  5.2572e-01, -1.6348e-01,  ..., -1.0545e-01,\n",
       "             3.3441e-01, -3.4872e-01],\n",
       "           [-3.6940e-01, -6.9073e-01,  3.0312e-01,  ...,  2.2825e-01,\n",
       "             4.1089e-01, -6.2172e-01],\n",
       "           [ 2.5370e-01,  8.5402e-01,  3.4001e-01,  ...,  9.5107e-01,\n",
       "             1.2395e+00, -1.2308e+00]],\n",
       "\n",
       "          [[ 1.0380e+00, -1.3350e-01,  1.5467e+00,  ..., -5.4630e-01,\n",
       "             3.5278e-01, -1.4554e-02],\n",
       "           [ 3.8635e-01,  2.0087e-01, -5.7378e-01,  ..., -1.2246e-01,\n",
       "            -1.0565e-01, -6.3054e-01],\n",
       "           [ 1.9691e-01, -5.8580e-02,  7.1857e-01,  ..., -4.2250e-01,\n",
       "            -1.1149e+00, -1.4454e+00],\n",
       "           ...,\n",
       "           [-5.1994e-01,  6.8559e-01,  2.6541e-01,  ...,  6.0279e-01,\n",
       "             1.0768e-01, -1.2363e+00],\n",
       "           [-1.8306e-02, -5.2104e-01, -6.5242e-01,  ..., -7.6142e-01,\n",
       "             6.6691e-01, -8.9204e-01],\n",
       "           [ 2.0199e-01,  3.8473e-01,  7.8952e-01,  ...,  5.2713e-01,\n",
       "             1.2510e+00,  3.5844e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 2.3897e-01,  1.1233e-01,  2.8409e-01,  ..., -2.0238e-01,\n",
       "             9.6286e-02, -4.9285e-01],\n",
       "           [-8.6747e-01,  1.9175e-01,  1.8313e-01,  ...,  5.3735e-01,\n",
       "             3.8587e-01, -1.3956e-01],\n",
       "           [-2.2482e-01, -6.9787e-02,  8.4421e-02,  ..., -8.1400e-01,\n",
       "            -4.1301e-01,  5.1661e-03],\n",
       "           ...,\n",
       "           [ 1.7250e-01, -1.4632e-02,  1.8098e-01,  ..., -1.4586e+00,\n",
       "             2.4902e-01,  3.0050e-01],\n",
       "           [ 2.8112e-01,  7.1655e-01, -2.1278e-01,  ..., -8.7345e-01,\n",
       "            -9.3558e-01,  1.2281e-01],\n",
       "           [-2.7444e-01,  5.7924e-01, -6.4992e-02,  ...,  7.5594e-01,\n",
       "            -4.4346e-01,  6.2641e-01]],\n",
       "\n",
       "          [[ 5.0591e-01, -5.1569e-01, -2.2313e-01,  ...,  7.7991e-02,\n",
       "             1.0457e+00, -1.2697e+00],\n",
       "           [-3.9983e-01,  2.3608e-01, -8.9198e-01,  ..., -1.4944e-01,\n",
       "            -3.0525e-01, -1.2567e-01],\n",
       "           [ 3.7552e-02, -5.4193e-01,  8.0745e-01,  ..., -4.4840e-01,\n",
       "            -1.6904e-01, -1.3957e-01],\n",
       "           ...,\n",
       "           [-4.6915e-01,  1.1788e+00, -1.3703e-01,  ..., -5.6678e-01,\n",
       "             6.0022e-01,  4.7297e-01],\n",
       "           [-1.0267e+00,  2.5054e-01,  5.0149e-02,  ..., -3.5507e-01,\n",
       "            -5.2434e-01, -1.0025e+00],\n",
       "           [ 5.1130e-01,  1.1119e+00, -8.2443e-02,  ..., -2.1920e-01,\n",
       "            -7.9446e-01,  4.6863e-01]]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.randn(2, 1, 32, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
