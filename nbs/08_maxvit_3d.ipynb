{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp maxvit_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class MaxViT3DMLP(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0., bias=False):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim, bias=bias),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim, bias=bias),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class MaxViTSqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super().__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),    # input C x H x W x D --> C x 1 X 1 x 1  ONE value of each channel\n",
    "            nn.Conv3d(in_channels, reduced_dim, kernel_size=1), # expansion\n",
    "            nn.SiLU(), # activation\n",
    "            nn.Conv3d(reduced_dim, in_channels, kernel_size=1), # brings it back\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.se(x)\n",
    "\n",
    "class MaxViTCNNBlock3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0 , groups=1, act=True, bn=True, bias=False):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias) #bias set to False as we are using BatchNorm\n",
    "\n",
    "        self.bn = nn.BatchNorm3d(out_channels) if bn else nn.Identity() \n",
    "        self.silu = nn.SiLU() if act else nn.Identity() ## SiLU <--> Swish same Thing\n",
    "        # 1 layer in MBConv doesn't have activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.silu(out)\n",
    "        return out\n",
    "\n",
    "#dropout\n",
    "class MaxViTStochasticDepth(nn.Module):\n",
    "    def __init__(self, survival_prob=0.8):\n",
    "        super().__init__()\n",
    "        self.survival_prob =survival_prob\n",
    "        \n",
    "    def forward(self, x): #form of dropout , randomly remove some layers not during testing\n",
    "        if not self.training:\n",
    "            return x\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, 1, device= x.device) < self.survival_prob # maybe add 1 more here\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "    \n",
    "class MaxViTMBConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, \n",
    "                 is_first = False,\n",
    "                 expand_ratio = 6, \n",
    "                 reduction = 4, #squeeze excitation 1/4 = 0.25\n",
    "                 survival_prob =0.8 # for stocastic depth\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        stride = 2 if is_first else 1\n",
    "        survival_prob = 0.8\n",
    "        self.use_residual = True if not is_first else False\n",
    "        hidden_dim = int(out_channels * expand_ratio)\n",
    "        reduced_dim = int(in_channels/reduction)\n",
    "        padding = padding\n",
    "        \n",
    "        ##expansion phase\n",
    "        self.expand = nn.Identity() if (expand_ratio == 1) else MaxViTCNNBlock3d(in_channels, hidden_dim, kernel_size = 1)\n",
    "        \n",
    "        ##Depthwise convolution phase\n",
    "        self.depthwise_conv = MaxViTCNNBlock3d(hidden_dim, hidden_dim,\n",
    "                                        kernel_size = kernel_size, stride = stride, \n",
    "                                        padding = padding, groups = hidden_dim\n",
    "                                       )\n",
    "        \n",
    "        # Squeeze Excitation phase\n",
    "        self.se = MaxViTSqueezeExcitation(hidden_dim, reduced_dim = reduced_dim)\n",
    "        \n",
    "        #output phase\n",
    "        self.pointwise_conv = MaxViTCNNBlock3d(hidden_dim, out_channels, kernel_size = 1, stride = 1, act = False, padding = 0)\n",
    "        # add Sigmoid Activation as mentioned in the paper\n",
    "        \n",
    "        # drop connect\n",
    "        self.drop_layers = MaxViTStochasticDepth(survival_prob = survival_prob)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Not 1st MBConv | 1st MBConv\n",
    "        residual = x\n",
    "        # b,d,x,y,z | b,d,x,y,z\n",
    "        x = self.expand(x)\n",
    "        # b,6d,x,y,z | b,6d,x,y,z\n",
    "        x = self.depthwise_conv(x)\n",
    "        # b,6d,x/2,y/2,z/2 | b,6d,x,y,z\n",
    "        x = self.se(x)\n",
    "        # b,6d,x/2,y/2,z/2 | b,6d,x,y,z\n",
    "        x = self.pointwise_conv(x)\n",
    "        # b, d,x,y,z | b,2d,x/2,y/2,z/2\n",
    "        if self.use_residual:\n",
    "            x = self.drop_layers(x)\n",
    "            x += residual\n",
    "        # b, d,x,y,z | b,2d,x/2,y/2,z/2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class MaxViT3DMHSA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_per_head = 32,\n",
    "        dropout = 0.,\n",
    "        window_size = (7,7,7),\n",
    "        bias = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % dim_per_head) == 0, 'dimension should be divisible by dimension per head'\n",
    "\n",
    "        self.heads = dim // dim_per_head\n",
    "        self.scale = dim_per_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias = bias)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(dim, dim, bias = bias),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # relative positional bias\n",
    "        w1,w2,w3 = window_size\n",
    "        self.rel_pos_bias = nn.Embedding((2 * w1 - 1) *(2 * w2 - 1)*(2 * w3 - 1), self.heads)\n",
    "        pos1 = torch.arange(w1, dtype=torch.int32)\n",
    "        pos2 = torch.arange(w2, dtype=torch.int32)\n",
    "        pos3 = torch.arange(w3, dtype=torch.int32)\n",
    "        # First we use the torch.arange and torch.meshgrid functions to generate the corresponding coordinates, [3,H,W,D] \n",
    "        # and then stack them up and expand them into a two-dimensional vector to get the absolute position index.\n",
    "        grid = torch.stack(torch.meshgrid(pos1, pos2, pos3, indexing = 'ij'))\n",
    "        grid = rearrange(grid, 'c i j k -> (i j k) c')\n",
    "        # insert a dimension in the first dimension and the second dimension respectively, perform broadcast subtraction, and obtain the tensor of 3, whd*ww, whd*ww\n",
    "        rel_pos = rearrange(grid, 'i ... -> i 1 ...') - rearrange(grid, 'j ... -> 1 j ...') \n",
    "        rel_pos[...,0] += w1 - 1\n",
    "        rel_pos[...,1] += w2 - 1\n",
    "        rel_pos[...,2] += w3 - 1\n",
    "        # Do a multiplication operation to distinguish, sum up the last dimension, and expand it into a one-dimensional coordinate   a*x1 + b*x2 + c*x3  (a= hd b=d c =1) \n",
    "        rel_pos_indices = (rel_pos * torch.tensor([(2 *w2 - 1)*(2 *w3 - 1), (2 *w3 - 1), 1])).sum(dim = -1)\n",
    "        \n",
    "        # Register as a variable that does not participate in learning\n",
    "        self.register_buffer('rel_pos_indices', rel_pos_indices, persistent = False)\n",
    "        self.dropout_prob = dropout\n",
    "               \n",
    "\n",
    "    def forward(self, x):\n",
    "        _, height, width, depth, window_height, window_width, window_depth ,_  = x.shape\n",
    "        h = self.heads\n",
    "\n",
    "        # b, x/w1, y/w2, z/w3, w1, w2, w3, d\n",
    "        x = rearrange(x, 'b x y z w1 w2 w3 d -> (b x y z) (w1 w2 w3) d')\n",
    "        # total_b, total_w, d \n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d ) -> b h n d', h = h), (q, k, v)) #split_heads\n",
    "        \n",
    "        # total_b, num_heads, total_w, d/num_heads\n",
    "        context = F.scaled_dot_product_attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            # attn_mask=self.rel_pos_bias(self.rel_pos_indices),  # Use this as a way to introduce relative position bias \n",
    "            #not working\n",
    "            dropout_p=self.dropout_prob,\n",
    "            is_causal=False,\n",
    "            scale=self.scale,  # Already scaled the vectors\n",
    "        )\n",
    "\n",
    "        # total_b, num_heads, total_w, d/num_heads\n",
    "        out = rearrange(context, 'b h (w1 w2 w3) d -> b w1 w2 w3 (h d)', w1 = window_height, w2 = window_width, w3 = window_depth) # merge heads\n",
    "\n",
    "        # total_b, w1, w2 ,w3, d\n",
    "        out = self.to_out(out) # combine heads out\n",
    "        out = rearrange(out, '(b x y z) ... -> b x y z ...', x = height, y = width, z = depth)\n",
    "        # b, x/w1, y/w2, z/w3, w1, w2, w3, d\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MaxViT3DStem0(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.conv_stem = nn.Sequential(\n",
    "            nn.Conv3d(self.config['in_channels'], self.config['hidden_dim'], kernel_size=3, stride = 2, padding = 1, bias=self.config['bias']),\n",
    "            nn.Conv3d(self.config['hidden_dim'], self.config['out_channels'], 3, padding = 1, bias=self.config['bias'])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 54, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_stem0_config ={ \n",
    "    \"in_channels\": 54,           # dimension of first layer, doubles every layer\n",
    "    \"hidden_dim\": 98,            # dimension of attention heads, kept at 32 in paper`\n",
    "    \"out_channels\": 54,           # window size for block and grids\n",
    "    \"bias\": True\n",
    "}\n",
    "\n",
    "maxvit_block = MaxViT3DStem0(test_stem0_config)\n",
    "img = torch.randn(2, 54, 32, 32, 24)\n",
    "preds = maxvit_block(img) \n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MaxViT3DBlock(nn.Module):\n",
    "    def __init__(self, block_config, is_first=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w1,self.w2,self.w3 = block_config['window_size']\n",
    "        stem_dim_in = block_config['stem_dim_in']       \n",
    "        dim = block_config['dim']\n",
    "        dim_per_head = block_config['dim_per_head']\n",
    "        dropout = block_config['dropout']\n",
    "        window_size = block_config['window_size']\n",
    "        expansion_rate = block_config['expansion_rate']\n",
    "        shrinkage_rate = block_config['shrinkage_rate']\n",
    "        bias = block_config['bias']\n",
    "        \n",
    "        self.MBConv =   MaxViTMBConv3d(\n",
    "                in_channels = stem_dim_in if is_first else dim, \n",
    "                out_channels = dim,\n",
    "                kernel_size=3 ,  \n",
    "                padding=1 , \n",
    "                is_first=is_first,\n",
    "                expand_ratio = expansion_rate if expansion_rate is not None else 4, \n",
    "                reduction = shrinkage_rate if shrinkage_rate is not None else 4, #squeeze excitation 1/4 = 0.25\n",
    "                survival_prob = 1-dropout # for stocastic depth\n",
    "            )\n",
    "            \n",
    "        self.layernorm1 = nn.LayerNorm(dim) \n",
    "        self.blockAttn = MaxViT3DMHSA(dim = dim, dim_per_head = dim_per_head, dropout = dropout, window_size = window_size, bias=bias)\n",
    "            \n",
    "        self.layernorm2 = nn.LayerNorm(dim)\n",
    "        self.FFN1 = MaxViT3DMLP(dim = dim, dropout = dropout, bias=bias)\n",
    "\n",
    "        self.layernorm3 = nn.LayerNorm(dim)\n",
    "        self.gridAttn = MaxViT3DMHSA(dim = dim, dim_per_head = dim_per_head, dropout = dropout, window_size = window_size,bias=bias)\n",
    "        self.layernorm4 = nn.LayerNorm(dim)\n",
    "        self.FFN2 = MaxViT3DMLP(dim = dim, dropout = dropout,bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # b,d,x,y,z | b,d/2,2x,2y,2z for first MBConv of stem\n",
    "        x = self.MBConv(x)\n",
    "        # b,d,x,y,z \n",
    "        x = rearrange(x, 'b d (x w1) (y w2) (z w3) -> b x y z w1 w2 w3 d', w1 = self.w1, w2 = self.w2, w3 = self.w3)  # block-like attention\n",
    "        # b,x/w1,y/w2,z/w3,w1,w2,w3,d\n",
    "        x = self.layernorm1(x)\n",
    "        x = x+self.blockAttn(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x+self.FFN1(x)\n",
    "        # b,x/w1,y/w2,z/w3,w1,w2,w3,d\n",
    "        x= rearrange(x, 'b x y z w1 w2 w3 d -> b d (x w1) (y w2) (z w3)')\n",
    "        # b,d,x,y,z\n",
    "        x = rearrange(x, 'b d (w1 x) (w2 y) (w3 z) -> b x y z w1 w2 w3 d', w1 = self.w1, w2 = self.w2, w3 = self.w3)  # grid-like attention\n",
    "        # b,x/w1,y/w2,z/w3,w1,w2,w3,d\n",
    "        x = self.layernorm3(x)\n",
    "        x = x+self.gridAttn(x)\n",
    "        x = self.layernorm4(x)\n",
    "        x = x+self.FFN2(x)\n",
    "        # b,x/w1,y/w2,z/w3,w1,w2,w3,d\n",
    "        x = rearrange(x, 'b x y z w1 w2 w3 d -> b d (w1 x) (w2 y) (w3 z)')\n",
    "        # b,d,x,y,z \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_block_config = {\n",
    "    \"stem_dim_in\": 64,             # used in first block to downsample z,x,y\n",
    "    \"dim\": 128,                    # dimension of whole layer, doubles every layer\n",
    "    \"dim_per_head\": 8,            # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4,4,4),        # window size for block and grids\n",
    "    \"dropout\": 0.1,               # dropout\n",
    "    \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,        # (squeeze) and excitation\n",
    "    \"bias\": True\n",
    "}\n",
    "\n",
    "maxvit_block = MaxViT3DBlock(test_block_config, is_first=True)\n",
    "img = torch.randn(2, 64, 32, 32, 24)\n",
    "preds = maxvit_block(img) \n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 32, 32, 24])\n"
     ]
    }
   ],
   "source": [
    "test_block_config = {\n",
    "    \"stem_dim_in\": 64,             # used in first block to downsample z,x,y\n",
    "    \"dim\": 128,                    # dimension of whole layer, doubles every layer\n",
    "    \"dim_per_head\": 8,            # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4,4,4),        # window size for block and grids\n",
    "    \"dropout\": 0.1,               # dropout\n",
    "    \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,        # (squeeze) and excitation\n",
    "    \"bias\":False\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    maxvit_block = MaxViT3DBlock(test_block_config, is_first=False)\n",
    "    img = torch.randn(2, 128, 32, 32, 24)\n",
    "    preds = maxvit_block(img) \n",
    "    print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class MaxViT3DStem(nn.Module):\n",
    "    def __init__(self, stem_config):\n",
    "        super().__init__()\n",
    "        self.stem = nn.ModuleList([MaxViT3DBlock(stem_config, is_first=(i == 0)) for i in range(stem_config[\"num_maxvit_blocks\"])])\n",
    "            \n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        for layer in self.stem:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "test_stem_config = {\n",
    "    \"num_maxvit_blocks\": 1,\n",
    "    \"stem_dim_in\": 32,\n",
    "    \"dim\": 64,                     # dimension of stem, doubles every stem\n",
    "    \"dim_per_head\": 8,             # dimension of attention heads, kept at 32 in paper`\n",
    "    \"window_size\": (4,4,4),        # window size for block and grids\n",
    "    \"dropout\": 0.1,                # dropout\n",
    "    \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "    \"shrinkage_rate\": None,        # (squeeze) and excitation  \n",
    "    \"bias\": True\n",
    "}\n",
    "\n",
    "test = MaxViT3DStem(test_stem_config)\n",
    "img = torch.randn(2, 32, 32, 32, 24)\n",
    "preds = test(img)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MaxViT3DEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.stems = nn.ModuleList([])\n",
    "        \n",
    "        self.stems.append(MaxViT3DStem0(config[\"stem0\"]))\n",
    "        \n",
    "        for stage_config in config[\"stems\"]:\n",
    "            self.stems.append(MaxViT3DStem(stage_config))\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "\n",
    "        for stem in self.stems:\n",
    "            hidden_states = stem(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "test_encoder_config ={ \n",
    "    \"stem0\": {\n",
    "        \"in_channels\": 1,         \n",
    "        \"hidden_dim\": 32,         \n",
    "        \"out_channels\": 32,\n",
    "        \"bias\": True        \n",
    "    },\n",
    "    \n",
    "    \"stems\": \n",
    "    [{\n",
    "        \"num_maxvit_blocks\": 2,\n",
    "        \"stem_dim_in\": 32,\n",
    "        \"dim\": 64,                     # dimension of first layer, doubles every layer\n",
    "        \"dim_per_head\": 8,             # dimension of attention heads, kept at 32 in paper`\n",
    "        \"window_size\": (4,4,4),        # window size for block and grids\n",
    "        \"dropout\": 0.1,                # dropout\n",
    "        \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "        \"shrinkage_rate\": None,        # (squeeze) and excitation \n",
    "        \"bias\": True\n",
    "    },\n",
    "    {\n",
    "        \"num_maxvit_blocks\": 2,\n",
    "        \"stem_dim_in\": 64,\n",
    "        \"dim\": 128,                     # dimension of first layer, doubles every layer\n",
    "        \"dim_per_head\": 8,             # dimension of attention heads, kept at 32 in paper`\n",
    "        \"window_size\": (4,4,4),        # window size for block and grids\n",
    "        \"dropout\": 0.1,                # dropout\n",
    "        \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "        \"shrinkage_rate\": None,        # (squeeze) and excitation }\n",
    "        \"bias\": True\n",
    "    },\n",
    "    {        \n",
    "        \"num_maxvit_blocks\": 2,\n",
    "        \"stem_dim_in\": 128,\n",
    "        \"dim\": 256,                     # dimension of first layer, doubles every layer\n",
    "        \"dim_per_head\": 8,             # dimension of attention heads, kept at 32 in paper`\n",
    "        \"window_size\": (2,2,2),        # window size for block and grids\n",
    "        \"dropout\": 0.1,                # dropout\n",
    "        \"expansion_rate\": None,        # squeeze and (excitation)\n",
    "        \"shrinkage_rate\": None,        # (squeeze) and excitation }\n",
    "        \"bias\": True\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MaxViT3DEncoder(test_encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-2.7354e-01, -2.8719e-01,  1.2851e+00,  ..., -8.1883e-01,\n",
       "             1.3938e+00, -1.3274e+00],\n",
       "           [ 1.5889e+00,  1.1704e+00, -1.2344e+00,  ...,  8.9436e-01,\n",
       "             1.8155e+00,  6.0398e-01],\n",
       "           [ 2.5968e-01, -4.8511e-01,  9.9510e-01,  ..., -2.1946e-01,\n",
       "             9.1342e-02,  3.6933e-01],\n",
       "           ...,\n",
       "           [ 7.6796e-01, -8.3130e-01,  6.2958e-01,  ..., -4.7292e-01,\n",
       "            -4.6050e-01,  3.0816e-01],\n",
       "           [-5.4006e-01, -9.9334e-01, -9.2556e-01,  ...,  8.5290e-01,\n",
       "             7.9385e-01,  6.0228e-01],\n",
       "           [ 3.8543e-02,  3.3222e-01, -1.0011e+00,  ...,  1.1563e+00,\n",
       "             1.3599e+00, -5.7324e-01]],\n",
       "\n",
       "          [[-8.4125e-01, -1.4427e+00, -4.8789e-01,  ..., -1.0948e+00,\n",
       "            -1.5865e+00, -1.4938e+00],\n",
       "           [-2.2214e+00,  9.6940e-02,  1.7580e+00,  ...,  1.2332e+00,\n",
       "            -2.0653e+00,  6.5060e-02],\n",
       "           [ 1.3293e-01, -4.4283e-01,  4.1952e-01,  ...,  9.0856e-01,\n",
       "            -2.4708e+00, -1.4429e+00],\n",
       "           ...,\n",
       "           [-5.6541e-01,  1.7919e-01,  1.0265e+00,  ..., -2.8955e-01,\n",
       "             7.5098e-01,  1.9988e+00],\n",
       "           [-1.8341e-01, -1.3086e+00,  8.7661e-01,  ..., -1.0681e+00,\n",
       "             1.6104e+00, -1.2642e+00],\n",
       "           [-2.7862e+00,  2.9061e-01,  2.7253e-01,  ..., -7.4379e-01,\n",
       "             3.9921e-01, -1.2267e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 7.3783e-01,  8.4985e-01,  1.2338e+00,  ..., -1.1142e+00,\n",
       "             1.8075e-01,  4.1171e-01],\n",
       "           [ 5.8950e-01, -4.4279e-01, -7.0465e-02,  ..., -1.4465e+00,\n",
       "            -6.9021e-01,  3.4124e-01],\n",
       "           [ 1.9348e+00, -5.3831e-01,  1.5442e+00,  ..., -1.2711e+00,\n",
       "             1.8260e-01,  1.5956e+00],\n",
       "           ...,\n",
       "           [-9.4521e-01,  3.0572e-02, -3.3768e-01,  ..., -7.2158e-01,\n",
       "             1.0870e+00, -9.5416e-01],\n",
       "           [ 5.1503e-02,  1.2926e+00, -4.8287e-02,  ...,  4.6119e-01,\n",
       "            -7.4241e-01,  4.1492e-02],\n",
       "           [ 1.9443e+00,  1.3196e-01, -4.5737e-02,  ..., -1.3344e+00,\n",
       "            -2.9237e-01, -1.7288e+00]],\n",
       "\n",
       "          [[ 8.4906e-01,  1.5950e+00,  9.1703e-03,  ...,  7.6275e-01,\n",
       "             1.8369e-03, -4.8282e-01],\n",
       "           [ 3.7695e-01,  2.6061e-02, -6.5936e-01,  ...,  7.7619e-02,\n",
       "             7.4856e-01,  2.1867e+00],\n",
       "           [-3.4342e-01,  1.8541e-01, -9.8025e-01,  ..., -1.6449e+00,\n",
       "             1.4244e+00, -1.3368e+00],\n",
       "           ...,\n",
       "           [-1.0449e-01,  7.6245e-01, -6.8245e-01,  ..., -5.9840e-01,\n",
       "             1.6565e+00, -1.5522e+00],\n",
       "           [-6.8456e-01,  7.2741e-01,  2.2746e-01,  ..., -5.9876e-01,\n",
       "            -9.4109e-01,  3.8484e-02],\n",
       "           [-8.8097e-01,  3.5384e-02, -1.0274e-01,  ..., -5.0547e-01,\n",
       "            -1.0651e+00,  3.9122e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 4.0791e-01, -1.6998e-01,  1.0002e-01,  ...,  1.0121e+00,\n",
       "             6.3889e-01, -7.8214e-01],\n",
       "           [-5.9348e-01,  2.3708e+00, -9.1975e-01,  ...,  1.4605e+00,\n",
       "             7.4182e-01, -2.0761e-01],\n",
       "           [-1.6148e+00,  4.1440e-01, -1.0230e+00,  ...,  2.1460e+00,\n",
       "             8.6743e-01, -1.3821e-02],\n",
       "           ...,\n",
       "           [-5.0028e-01, -1.1289e-01, -4.8279e-02,  ..., -3.1851e-01,\n",
       "            -2.4551e-01,  3.5241e-01],\n",
       "           [-8.0177e-02,  2.6178e-02,  1.2238e+00,  ..., -1.5350e+00,\n",
       "             1.0723e+00,  2.0259e-01],\n",
       "           [ 1.5533e+00, -8.0037e-01,  1.0249e+00,  ...,  7.2852e-01,\n",
       "            -4.7968e-01, -3.3442e-01]],\n",
       "\n",
       "          [[ 1.2955e-01, -1.0275e+00, -1.0244e+00,  ..., -8.1876e-01,\n",
       "             3.3847e-01, -1.8102e-01],\n",
       "           [ 2.7340e-01,  1.8472e-01,  7.4035e-02,  ..., -3.4013e-01,\n",
       "             9.8744e-01, -4.8636e-01],\n",
       "           [ 1.7429e+00,  9.7280e-01, -4.4185e-01,  ..., -4.6515e-01,\n",
       "            -1.4920e+00,  2.6756e-02],\n",
       "           ...,\n",
       "           [ 6.3980e-01,  2.0388e-01, -1.2474e+00,  ...,  1.2688e+00,\n",
       "            -7.5473e-01, -7.0315e-01],\n",
       "           [ 5.9489e-01, -2.7728e-01, -7.6532e-01,  ...,  7.3128e-01,\n",
       "            -5.2617e-01, -1.5254e+00],\n",
       "           [-2.0948e-01, -1.0163e+00,  9.4551e-01,  ..., -5.1957e-01,\n",
       "             1.4882e-01, -1.1582e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 2.6639e+00, -1.2316e-01,  5.2485e-01,  ...,  2.5817e-01,\n",
       "            -5.7493e-01,  3.3693e-02],\n",
       "           [ 9.2053e-01, -1.6307e+00, -8.9994e-01,  ...,  6.8193e-01,\n",
       "            -5.7201e-01, -1.9359e-01],\n",
       "           [ 2.1305e+00,  3.6040e-01,  7.1700e-01,  ..., -6.4909e-01,\n",
       "             9.0456e-01,  1.4809e+00],\n",
       "           ...,\n",
       "           [ 4.0787e-01, -1.9424e+00,  1.4055e+00,  ..., -6.0536e-02,\n",
       "            -3.1433e-01,  1.0842e-01],\n",
       "           [ 1.1631e+00,  1.5652e-01, -3.5992e-01,  ..., -1.3714e-01,\n",
       "             3.4907e-01, -1.9000e-01],\n",
       "           [-8.6602e-01, -5.4026e-01,  9.4009e-01,  ..., -1.4198e+00,\n",
       "            -7.6902e-01, -1.0206e+00]],\n",
       "\n",
       "          [[ 1.5112e+00,  2.0789e+00,  2.3087e-01,  ...,  1.0684e+00,\n",
       "            -1.8184e-01, -1.2117e+00],\n",
       "           [-8.7192e-01,  1.7721e+00,  1.4507e-01,  ...,  3.2812e-01,\n",
       "             2.4891e-01, -2.0309e-02],\n",
       "           [ 1.3740e+00, -5.7706e-01, -1.1661e+00,  ..., -6.9262e-02,\n",
       "             1.4798e+00, -9.3352e-01],\n",
       "           ...,\n",
       "           [-1.5009e+00, -9.2958e-01, -1.1677e+00,  ...,  2.0973e+00,\n",
       "            -2.0300e+00, -1.0804e+00],\n",
       "           [ 1.0355e-01,  1.4711e+00,  8.5803e-02,  ..., -1.2200e+00,\n",
       "            -2.0547e+00,  1.1828e+00],\n",
       "           [ 1.4569e-01,  1.2765e+00,  6.2102e-01,  ..., -8.8561e-02,\n",
       "            -6.3875e-01,  1.1471e+00]]],\n",
       "\n",
       "\n",
       "         [[[-9.4570e-02,  9.1639e-01,  1.0028e+00,  ..., -6.6046e-01,\n",
       "             1.9791e-01,  5.2122e-01],\n",
       "           [-2.1797e-01,  6.1316e-01,  3.7089e-02,  ..., -5.3197e-01,\n",
       "            -1.9307e+00,  4.2861e-01],\n",
       "           [-2.2585e+00,  2.4009e-01, -9.3238e-01,  ...,  1.3797e+00,\n",
       "            -2.0564e-01, -1.4779e-01],\n",
       "           ...,\n",
       "           [ 5.7582e-01,  6.5353e-01, -2.7171e-01,  ..., -7.2841e-01,\n",
       "            -2.2555e-01,  4.4884e-01],\n",
       "           [-1.1876e-01,  6.5848e-01,  1.4072e+00,  ..., -3.7019e-01,\n",
       "             1.4827e+00, -2.7549e-01],\n",
       "           [-1.8737e+00, -4.2516e-01, -1.5483e-01,  ...,  4.7824e-01,\n",
       "            -4.5948e-01,  1.3627e+00]],\n",
       "\n",
       "          [[-2.1014e-02,  1.7202e+00,  4.0426e-01,  ...,  8.7536e-01,\n",
       "             2.1093e+00,  8.8443e-01],\n",
       "           [ 6.0440e-03,  8.7731e-03, -1.2147e+00,  ...,  2.7748e-01,\n",
       "            -9.9831e-01,  1.6801e+00],\n",
       "           [-4.5930e-01,  4.2471e-01,  4.4006e-01,  ...,  1.0242e+00,\n",
       "            -2.5868e+00, -4.7702e-01],\n",
       "           ...,\n",
       "           [ 9.5891e-01,  9.1490e-01,  4.9460e-01,  ...,  1.2340e+00,\n",
       "             1.2932e+00,  2.1997e+00],\n",
       "           [-2.7256e+00,  2.5039e-01, -1.0707e+00,  ...,  1.3592e+00,\n",
       "             7.1992e-01, -3.0604e-01],\n",
       "           [-8.8630e-02,  9.8312e-01,  9.0182e-01,  ...,  2.1447e+00,\n",
       "             4.3981e-01,  8.8481e-01]]],\n",
       "\n",
       "\n",
       "         [[[-9.2813e-01,  3.3345e-01, -7.0849e-01,  ..., -1.1246e+00,\n",
       "            -5.7828e-02, -1.4576e+00],\n",
       "           [ 1.4039e-01, -5.8375e-01,  9.3475e-02,  ...,  1.8549e-01,\n",
       "             1.0340e+00, -1.5689e+00],\n",
       "           [ 1.2482e-01,  4.0513e-01, -1.1083e-01,  ..., -1.9192e-02,\n",
       "            -2.3924e+00, -3.5319e-01],\n",
       "           ...,\n",
       "           [-1.2722e-01,  4.5972e-01,  1.9431e-02,  ...,  1.2372e-01,\n",
       "            -1.6730e+00, -6.7185e-01],\n",
       "           [ 1.7342e-01,  6.4820e-01,  5.1669e-01,  ..., -5.3273e-02,\n",
       "             1.4615e+00,  3.5926e-01],\n",
       "           [-2.3854e-01, -2.0654e+00, -6.8096e-01,  ..., -1.9073e+00,\n",
       "            -7.0425e-01, -1.4147e+00]],\n",
       "\n",
       "          [[ 2.5534e-01, -4.8574e-01, -1.4393e+00,  ...,  5.3587e-01,\n",
       "             8.0770e-01,  5.8609e-01],\n",
       "           [-2.0404e+00, -1.2491e+00,  4.2548e-01,  ..., -1.1905e+00,\n",
       "             3.5772e-02,  2.0144e-01],\n",
       "           [ 4.7527e-01,  1.0227e+00,  5.0659e-01,  ..., -8.7638e-01,\n",
       "            -1.3243e+00,  1.1759e+00],\n",
       "           ...,\n",
       "           [ 6.0035e-01,  2.8383e+00, -1.4875e-01,  ..., -1.8134e+00,\n",
       "            -1.2565e+00, -1.3339e+00],\n",
       "           [ 1.0519e+00, -4.1971e-01, -1.0160e+00,  ..., -4.8321e-01,\n",
       "             8.0745e-01,  9.1628e-01],\n",
       "           [ 4.8964e-01,  2.6225e-01,  1.7059e-01,  ...,  2.1243e+00,\n",
       "            -8.4839e-01,  9.1428e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 5.5677e-01, -3.4855e-01,  8.8239e-01,  ..., -1.5256e+00,\n",
       "            -1.2395e+00,  7.2510e-01],\n",
       "           [ 1.6920e+00, -5.1579e-03, -7.2040e-01,  ...,  2.2010e+00,\n",
       "             1.9391e+00, -1.0660e-01],\n",
       "           [-3.3588e-01,  1.1955e+00,  4.1054e-01,  ..., -2.3699e+00,\n",
       "            -1.2824e+00, -2.0550e+00],\n",
       "           ...,\n",
       "           [ 5.3830e-01, -1.1865e+00,  1.1435e+00,  ..., -7.9219e-01,\n",
       "             1.2813e+00,  1.1608e+00],\n",
       "           [ 1.1943e+00, -2.0136e+00,  6.0277e-01,  ..., -1.3807e-01,\n",
       "            -1.7241e-01,  2.3994e+00],\n",
       "           [-5.4089e-01, -7.5338e-01, -7.0175e-01,  ...,  2.0384e+00,\n",
       "             1.3779e+00, -6.0921e-01]],\n",
       "\n",
       "          [[-1.1873e-02,  1.6145e+00, -6.3681e-01,  ..., -7.7541e-01,\n",
       "            -1.8738e+00,  9.2769e-02],\n",
       "           [-3.4955e-01,  1.7177e+00,  9.1189e-02,  ...,  6.9802e-01,\n",
       "            -2.6966e-01, -5.9386e-01],\n",
       "           [-8.2024e-01,  7.1933e-01, -3.2304e-01,  ..., -5.2789e-01,\n",
       "             1.2400e+00,  5.7878e-01],\n",
       "           ...,\n",
       "           [-1.0110e+00, -1.2958e+00, -2.9309e-01,  ..., -3.9408e-01,\n",
       "            -7.3904e-01, -9.7170e-01],\n",
       "           [-2.7481e+00, -6.9230e-01,  4.9113e-01,  ...,  1.1829e-01,\n",
       "            -2.6252e-01, -1.9718e+00],\n",
       "           [-1.8135e+00, -9.7049e-01,  1.1392e+00,  ..., -6.8346e-01,\n",
       "             1.1380e+00,  3.4699e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 1.2873e+00,  4.1142e-01, -2.8031e-01,  ...,  3.8674e-01,\n",
       "             8.7702e-01, -7.8232e-01],\n",
       "           [ 9.6235e-01, -6.4949e-01,  3.7742e-01,  ..., -5.4327e-01,\n",
       "            -4.9181e-01, -2.3218e-01],\n",
       "           [-5.1770e-01, -1.1087e+00,  1.7965e+00,  ..., -2.2368e-01,\n",
       "             5.7073e-01,  1.7433e+00],\n",
       "           ...,\n",
       "           [-6.4520e-01, -2.3461e-01, -1.6338e-01,  ...,  5.9305e-01,\n",
       "             2.4911e-01,  3.0328e-01],\n",
       "           [-8.7636e-01, -8.6871e-01,  2.0529e+00,  ...,  1.3551e+00,\n",
       "            -1.2625e+00,  5.1169e-01],\n",
       "           [ 1.2018e+00, -1.0523e+00,  4.5867e-01,  ..., -1.4350e-01,\n",
       "             9.7984e-01, -9.2117e-01]],\n",
       "\n",
       "          [[ 3.6118e-01,  9.6136e-01, -3.5292e-01,  ...,  6.9656e-02,\n",
       "            -5.2200e-01,  2.1655e-01],\n",
       "           [ 1.8480e+00,  2.1043e+00,  2.2799e-01,  ..., -2.3748e+00,\n",
       "            -8.0896e-01, -1.8590e-02],\n",
       "           [ 1.6034e+00,  8.0792e-01,  2.1708e-01,  ...,  1.5353e-01,\n",
       "             6.5576e-02,  1.1554e+00],\n",
       "           ...,\n",
       "           [ 6.7270e-01, -8.5563e-01, -1.6875e+00,  ...,  1.4170e+00,\n",
       "            -1.2873e+00, -6.6108e-01],\n",
       "           [-2.1489e+00,  9.9934e-01,  7.2740e-01,  ..., -1.2068e+00,\n",
       "            -1.1019e+00,  2.3142e-01],\n",
       "           [ 1.0325e+00, -1.1041e+00, -1.4713e+00,  ...,  4.4932e-01,\n",
       "            -3.0416e-01,  5.9208e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 6.1973e-01,  7.8426e-01, -1.9058e+00,  ...,  1.1905e-01,\n",
       "             2.2710e-01, -1.1938e+00],\n",
       "           [-2.3530e-01, -4.4103e-01, -1.6760e+00,  ..., -5.0935e-02,\n",
       "             9.4981e-01, -4.8274e-01],\n",
       "           [-1.7400e+00, -1.6764e-01,  1.5950e+00,  ..., -9.8478e-01,\n",
       "            -2.5099e-01, -2.0212e+00],\n",
       "           ...,\n",
       "           [-1.0008e-01, -2.0722e-01,  7.9215e-01,  ...,  1.4174e+00,\n",
       "             4.0380e-01,  5.8675e-01],\n",
       "           [-5.2765e-01, -4.8494e-01,  9.0739e-02,  ...,  1.1010e-01,\n",
       "             6.4583e-01, -1.8000e-01],\n",
       "           [-1.0858e+00, -1.9145e+00, -8.9431e-01,  ...,  1.2562e+00,\n",
       "            -2.3851e+00,  4.4043e-01]],\n",
       "\n",
       "          [[-2.1742e-01, -7.9653e-01,  3.1053e-01,  ..., -2.4090e-01,\n",
       "            -3.0930e-01, -9.8543e-02],\n",
       "           [-1.8363e-01,  2.8920e-01, -6.5019e-01,  ...,  5.3380e-01,\n",
       "             2.7955e-01, -1.0422e+00],\n",
       "           [ 4.6050e-01, -1.8971e+00, -1.6695e+00,  ..., -4.2407e-01,\n",
       "            -7.1739e-01, -1.9593e+00],\n",
       "           ...,\n",
       "           [ 1.0066e+00, -4.7113e-01,  3.5184e-01,  ..., -1.4459e+00,\n",
       "             1.4026e+00, -1.9970e+00],\n",
       "           [ 8.9341e-01,  1.1805e+00, -8.2811e-01,  ..., -7.0824e-01,\n",
       "            -4.9486e-01, -5.3437e-01],\n",
       "           [ 1.7635e+00,  9.9610e-01, -1.8126e+00,  ...,  3.5850e-01,\n",
       "             4.6082e-01, -8.4134e-01]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-8.2660e-01,  6.7395e-01,  6.6932e-01,  ..., -7.4662e-01,\n",
       "            -7.7729e-01,  8.2323e-01],\n",
       "           [-1.1680e+00,  1.1680e+00, -1.2956e+00,  ..., -9.4615e-01,\n",
       "             4.8385e-01, -1.8452e-01],\n",
       "           [-1.2670e+00, -1.4522e+00, -1.8426e+00,  ..., -1.3795e-01,\n",
       "             1.2932e+00,  9.0116e-01],\n",
       "           ...,\n",
       "           [ 1.2208e+00,  1.0547e+00,  2.4625e+00,  ..., -5.5835e-01,\n",
       "            -7.3974e-01,  9.4795e-01],\n",
       "           [ 1.8199e+00, -5.6075e-01, -2.1640e-01,  ...,  1.9223e-01,\n",
       "            -4.9530e-01, -7.9015e-01],\n",
       "           [ 1.1381e+00,  3.6983e-01,  1.5024e+00,  ...,  9.1683e-01,\n",
       "            -3.3644e-02, -1.2401e+00]],\n",
       "\n",
       "          [[-3.5522e-01, -3.2785e-01, -2.4285e-01,  ...,  1.8462e+00,\n",
       "             9.3579e-01,  1.4623e+00],\n",
       "           [ 1.1506e+00, -4.8707e-01, -6.1025e-01,  ...,  2.6659e-02,\n",
       "             1.4745e+00,  6.1720e-01],\n",
       "           [ 1.7348e+00,  1.0196e+00, -4.4462e-01,  ..., -1.8341e+00,\n",
       "             1.2382e+00,  2.0823e+00],\n",
       "           ...,\n",
       "           [-4.0788e-01, -7.7804e-02, -1.6970e-01,  ..., -8.1648e-01,\n",
       "             9.2890e-01,  2.1595e-01],\n",
       "           [ 1.4331e+00, -2.3822e-01,  1.6917e+00,  ...,  3.2709e-01,\n",
       "             1.4121e-01, -4.8025e-01],\n",
       "           [-4.5555e-01,  2.5061e+00,  1.0025e+00,  ...,  3.1414e-01,\n",
       "            -1.9149e+00, -1.3474e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 6.6103e-01,  1.1847e-01,  1.0968e+00,  ...,  9.0503e-01,\n",
       "            -5.8479e-01,  5.0445e-02],\n",
       "           [-9.4356e-01,  1.9087e-01,  1.1434e+00,  ..., -2.5715e-01,\n",
       "             1.5372e-01, -2.7874e+00],\n",
       "           [ 2.3289e+00, -2.6206e-02, -1.0104e+00,  ..., -6.9342e-02,\n",
       "             5.1103e-01, -1.0449e+00],\n",
       "           ...,\n",
       "           [-5.4556e-01, -3.5044e-01,  8.3832e-01,  ...,  1.9916e+00,\n",
       "             4.0357e-01,  2.2329e+00],\n",
       "           [-7.8863e-01, -3.8985e-01, -9.1813e-01,  ..., -5.6715e-01,\n",
       "            -4.4612e-01,  6.5203e-01],\n",
       "           [-4.9521e-01, -1.2534e+00,  5.2532e-01,  ...,  8.2803e-01,\n",
       "            -3.4568e-01, -5.2747e-01]],\n",
       "\n",
       "          [[-1.8007e-01, -1.0454e-01,  2.5746e+00,  ..., -4.1873e-01,\n",
       "            -6.0350e-01, -3.1634e-01],\n",
       "           [-3.7582e-01,  8.2102e-01,  4.9900e-01,  ..., -1.2940e+00,\n",
       "            -1.1544e+00, -1.9440e-01],\n",
       "           [-2.5894e-01,  7.2821e-01,  1.4248e+00,  ...,  6.1225e-02,\n",
       "             7.7483e-02,  1.0157e-01],\n",
       "           ...,\n",
       "           [-1.4974e+00,  4.7599e-01, -2.0216e+00,  ..., -9.0664e-01,\n",
       "             7.6063e-01,  1.3968e+00],\n",
       "           [-7.1307e-01,  4.7927e-01,  1.6239e-01,  ..., -2.0083e-01,\n",
       "             6.1180e-01,  2.5797e+00],\n",
       "           [ 3.2842e-02, -3.9889e-01, -5.5187e-01,  ...,  2.3036e+00,\n",
       "            -7.5500e-01, -5.4855e-02]]],\n",
       "\n",
       "\n",
       "         [[[-2.8782e+00, -9.2047e-01,  3.2289e-01,  ..., -4.3195e-01,\n",
       "            -1.4026e+00, -1.8024e+00],\n",
       "           [-1.6032e+00, -3.9996e-01, -7.3938e-01,  ..., -6.7030e-01,\n",
       "            -4.7427e-01,  2.1686e-01],\n",
       "           [-1.9812e-01,  3.4012e-01, -1.5534e+00,  ...,  8.0596e-01,\n",
       "            -1.8760e-01,  7.5953e-02],\n",
       "           ...,\n",
       "           [-2.2415e+00, -1.7743e+00, -4.0059e-02,  ...,  6.2537e-01,\n",
       "            -3.8851e-01, -5.1431e-01],\n",
       "           [-1.0173e+00, -1.3498e+00, -1.3341e+00,  ...,  2.4385e-01,\n",
       "             1.6487e+00,  5.1924e-01],\n",
       "           [-1.2856e+00, -1.0659e+00,  1.2131e+00,  ..., -6.7723e-01,\n",
       "            -1.0666e+00,  2.4772e-01]],\n",
       "\n",
       "          [[-9.8263e-01,  8.7506e-03, -1.3469e+00,  ...,  2.1554e+00,\n",
       "            -2.5989e-01, -1.5242e+00],\n",
       "           [ 9.9095e-02, -2.0182e-02,  1.2946e+00,  ...,  1.1519e+00,\n",
       "            -5.4924e-01,  5.1613e-01],\n",
       "           [-6.5076e-01, -6.9712e-02,  7.0583e-02,  ...,  1.9206e+00,\n",
       "            -2.7518e-01, -4.9893e-01],\n",
       "           ...,\n",
       "           [-1.4979e+00, -1.3000e+00,  1.3288e+00,  ...,  4.0101e-01,\n",
       "            -5.9545e-01, -1.0880e+00],\n",
       "           [ 2.0171e-01,  5.1201e-01,  1.8226e+00,  ..., -4.2418e-01,\n",
       "             1.2346e+00, -1.7079e-01],\n",
       "           [-9.2704e-01, -4.5021e-01,  9.6445e-01,  ...,  4.4263e-01,\n",
       "             9.7600e-01,  1.5819e+00]]]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.randn(2, 1, 32, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
