{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp transforms/resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3453a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import torch\n",
    "from monai.data import MetaTensor\n",
    "from monai.transforms.spatial.array import Resize, TraceKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bdd976",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ResizeWithSpacing(Resize):  # Turns out that meta_tensor.pixdim already tracks this and so this class is unhelpful\n",
    "    def __call__(\n",
    "        self,\n",
    "        img: MetaTensor,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> MetaTensor:\n",
    "        old_shape = torch.tensor(img.shape[1:])  # Channel is first dim\n",
    "        old_spacing = torch.tensor(img.meta[\"spacing\"])\n",
    "        new_shape = torch.tensor(self.spatial_size)\n",
    "        new_spacing = old_spacing * old_shape / new_shape\n",
    "        img.meta[\"spacing\"] = new_spacing\n",
    "\n",
    "        return super().__call__(img, *args, **kwargs)\n",
    "\n",
    "    def inverse_transform(self, data: MetaTensor, transform) -> MetaTensor:\n",
    "        new_shape = data.shape[1:]\n",
    "        new_spacing = data.meta[\"spacing\"]\n",
    "        old_shape = transform[TraceKeys.ORIG_SIZE]\n",
    "        old_spacing = new_spacing * new_shape / old_shape\n",
    "        data.meta[\"spacing\"] = old_spacing\n",
    "        return super().inverse_transform(data, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74903f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'spacing'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    affine: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float64\u001b[1m)\u001b[0m,\n",
       "    space: RAS\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2844928/2466326269.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  old_spacing = torch.tensor(img.meta[\"spacing\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m30\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'spacing'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3.3333\u001b[0m, \u001b[1;36m2.0000\u001b[0m, \u001b[1;36m1.0000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    affine: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.6667\u001b[0m,  \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m0.3333\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m1.0000\u001b[0m,  \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m0.0000\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m0.5000\u001b[0m, \u001b[1;36m-0.2500\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m0.0000\u001b[0m,  \u001b[1;36m1.0000\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float64\u001b[1m)\u001b[0m,\n",
       "    space: RAS\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.randn(1, 5, 10, 15)\n",
    "a = MetaTensor(a, meta={\"spacing\": torch.tensor([2, 2, 2])})\n",
    "display(a.shape, a.meta)\n",
    "\n",
    "transform = ResizeWithSpacing(spatial_size=(3, 10, 30))\n",
    "b = transform(a)\n",
    "display(b.shape, b.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ced83",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54db661",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bc963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
