{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp transforms/clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9869ac14",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from collections.abc import Hashable\n",
    "\n",
    "import torch\n",
    "from monai.config import KeysCollection\n",
    "from monai.transforms import Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c3888",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Clip(Transform):\n",
    "    def __init__(self, min_value: float, max_value: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def __call__(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Clip the input tensor to the specified range.\n",
    "\n",
    "        Args:\n",
    "            data: Input tensor to be clipped.\n",
    "\n",
    "        Returns:\n",
    "            Clipped tensor.\n",
    "        \"\"\"\n",
    "        return torch.clamp(data, self.min_value, self.max_value)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Clip(min_value={self.min_value}, max_value={self.max_value})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95debd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mClip\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_value\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mmax_value\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-22.1990) tensor(22.3186)\n",
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(3, 4, 5) * 10\n",
    "transform = Clip(0, 1)\n",
    "display(transform)\n",
    "\n",
    "print(data.min(), data.max())\n",
    "\n",
    "clipped_data = transform(data)\n",
    "\n",
    "print(clipped_data.min(), clipped_data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbfb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Clipd(Transform):\n",
    "    def __init__(self, keys: KeysCollection, min_value: float, max_value: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.keys = keys\n",
    "        self.transform = Clip(min_value, max_value)\n",
    "\n",
    "    def __call__(self, data: dict[Hashable, torch.Tensor]) -> dict[Hashable, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Clip the input tensor to the specified range.\n",
    "\n",
    "        Args:\n",
    "            data: Input tensor to be clipped.\n",
    "\n",
    "        Returns:\n",
    "            Clipped tensor.\n",
    "        \"\"\"\n",
    "        for key in self.keys:\n",
    "            if key in data:\n",
    "                data[key] = self.transform(data[key])\n",
    "            else:\n",
    "                raise KeyError(f\"Key {key} not found in input data.\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Clipd(keys={self.keys}, min_value={self.transform.min_value}, max_value={self.transform.max_value})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a2baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mClipd\u001b[0m\u001b[1m(\u001b[0m\u001b[33mkeys\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'images'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mmin_value\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mmax_value\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-19.7777) tensor(18.1412)\n",
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "data = {\"images\": torch.randn(3, 4, 5) * 10}\n",
    "transform = Clipd([\"images\"], 0, 1)\n",
    "display(transform)\n",
    "\n",
    "print(data[\"images\"].min(), data[\"images\"].max())\n",
    "\n",
    "clipped_data = transform(data)\n",
    "\n",
    "print(clipped_data[\"images\"].min(), clipped_data[\"images\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066b186",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18359071",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42442162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
