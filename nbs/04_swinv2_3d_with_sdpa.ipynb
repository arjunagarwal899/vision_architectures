{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp swinv2_3d_with_sdpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from vision_architectures.swinv2_3d import (\n",
    "    populate_and_validate_config,\n",
    "    get_coords_grid,\n",
    "    SwinV23DMHSA as SwinV23DMHSAWithoutSDPA,\n",
    "    SwinV23DLayerMLP,\n",
    "    SwinV23DLayer as SwinV23DLayerWithoutSDPA,\n",
    "    SwinV23DBlock as SwinV23DBlockWithoutSDPA,\n",
    "    SwinV23DPatchMerging,\n",
    "    SwinV23DStage as SwinV23DStageWithoutSDPA,\n",
    "    SwinV23DEncoder as SwinV23DEncoderWithoutSDPA,\n",
    "    SwinV23DPatchEmbeddings,\n",
    "    get_3d_position_embeddings,\n",
    "    embed_spacings_in_position_embeddings,\n",
    "    SwinV23DEmbeddings,\n",
    "    SwinV23DModel as SwinV23DModelWithoutSDPA,\n",
    "    SwinV23DReconstructionDecoder,\n",
    "    SwinV23DMIM as SwinV23DMIMWithoutSDPA,\n",
    "    SwinV23DSimMIM as SwinV23DSimMIMWithoutSDPA,\n",
    "    SwinV23DVAEMIM as SwinV23DVAEMIMWithoutSDPA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DMHSA(SwinV23DMHSAWithoutSDPA):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        use_relative_position_bias,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__(dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob)\n",
    "\n",
    "        # Remove attention dropout layer as that is handled automatically, but store the dropout for later\n",
    "        del self.attn_drop\n",
    "        self.attn_drop_prob = attn_drop_prob\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        # hidden_states: (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "        _, num_patches_z, num_patches_y, num_patches_x, _ = hidden_states.shape\n",
    "\n",
    "        query, key, value = rearrange(\n",
    "            self.W_qkv(hidden_states),\n",
    "            \"b nz ny nx (n num_heads d) -> n b num_heads (nz ny nx) d\",\n",
    "            n=3,\n",
    "            num_heads=self.num_heads,\n",
    "        )\n",
    "        # num_patches = window_size_z * window_size_y * window_size_x\n",
    "        # Each is (windowed_b, num_heads, num_patches, per_head_dim)\n",
    "\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=np.log(1.0 / 0.01)).exp()\n",
    "\n",
    "        query_normalized = F.normalize(query, dim=-1)\n",
    "        key_normalized = F.normalize(key, dim=-1)\n",
    "\n",
    "        query_normalized_and_scaled = query_normalized * logit_scale  # Scale the query beforehand\n",
    "\n",
    "        relative_position_bias = None\n",
    "        if self.use_relative_position_bias:\n",
    "            relative_position_bias = self.calculate_relative_position_bias()\n",
    "\n",
    "        context = F.scaled_dot_product_attention(\n",
    "            query_normalized_and_scaled,\n",
    "            key_normalized,\n",
    "            value,\n",
    "            attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias\n",
    "            dropout_p=self.attn_drop_prob,\n",
    "            is_causal=False,\n",
    "            scale=1.0,  # Already scaled the vectors\n",
    "        )\n",
    "        # (windowed_b, num_heads, num_patches, per_head_dim)\n",
    "\n",
    "        context = rearrange(\n",
    "            context,\n",
    "            \"b num_heads (num_patches_z num_patches_y num_patches_x) d -> \"\n",
    "            \"b num_patches_z num_patches_y num_patches_x (num_heads d)\",\n",
    "            num_patches_z=num_patches_z,\n",
    "            num_patches_y=num_patches_y,\n",
    "            num_patches_x=num_patches_x,\n",
    "        )\n",
    "        # (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "\n",
    "        context = self.proj(context)\n",
    "        context = self.proj_drop(context)\n",
    "        # (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mSwinV23DMHSA\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_qkv\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m162\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcpb_mlp\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33minplace\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = SwinV23DMHSA(54, 6, (4, 4, 4), True)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 4, 4, 4, 54)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for use_relative_position_bias in [True, False]:\n",
    "    m1 = SwinV23DMHSA(54, 6, (4, 4, 4), use_relative_position_bias)\n",
    "    m2 = SwinV23DMHSAWithoutSDPA(54, 6, (4, 4, 4), use_relative_position_bias)\n",
    "\n",
    "    m1.load_state_dict(m2.state_dict())\n",
    "    m1.eval(), m2.eval()\n",
    "\n",
    "    example_input = torch.randn(2, 4, 4, 4, 54)\n",
    "    o1 = m1(example_input)\n",
    "    o2 = m2(example_input)\n",
    "\n",
    "    assert torch.allclose(o1, o2, atol=1e-6), (o1 - o2).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify other classes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DLayer(SwinV23DLayerWithoutSDPA):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        intermediate_ratio,\n",
    "        layer_norm_eps,\n",
    "        window_size,\n",
    "        use_relative_position_bias,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "        mlp_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim,\n",
    "            num_heads,\n",
    "            intermediate_ratio,\n",
    "            layer_norm_eps,\n",
    "            window_size,\n",
    "            use_relative_position_bias,\n",
    "            attn_drop_prob,\n",
    "            proj_drop_prob,\n",
    "            mlp_drop_prob,\n",
    "        )\n",
    "\n",
    "        self.mhsa = SwinV23DMHSA(\n",
    "            dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DBlock(SwinV23DBlockWithoutSDPA):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__(stage_config)\n",
    "\n",
    "        self.stage_config = stage_config\n",
    "        self.w_layer = SwinV23DLayer(\n",
    "            stage_config[\"_out_dim\"],\n",
    "            stage_config[\"num_heads\"],\n",
    "            stage_config[\"intermediate_ratio\"],\n",
    "            stage_config[\"layer_norm_eps\"],\n",
    "            stage_config[\"window_size\"],\n",
    "            stage_config[\"use_relative_position_bias\"],\n",
    "            stage_config.get(\"attn_drop_prob\", 0.0),\n",
    "            stage_config.get(\"proj_drop_prob\", 0.0),\n",
    "            stage_config.get(\"mlp_drop_prob\", 0.0),\n",
    "        )\n",
    "        self.sw_layer = SwinV23DLayer(\n",
    "            stage_config[\"_out_dim\"],\n",
    "            stage_config[\"num_heads\"],\n",
    "            stage_config[\"intermediate_ratio\"],\n",
    "            stage_config[\"layer_norm_eps\"],\n",
    "            stage_config[\"window_size\"],\n",
    "            stage_config[\"use_relative_position_bias\"],\n",
    "            stage_config.get(\"attn_drop_prob\", 0.0),\n",
    "            stage_config.get(\"proj_drop_prob\", 0.0),\n",
    "            stage_config.get(\"mlp_drop_prob\", 0.0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DStage(SwinV23DStageWithoutSDPA):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__(stage_config)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [SwinV23DBlock(stage_config) for _ in range(stage_config[\"depth\"])],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DEncoder(SwinV23DEncoderWithoutSDPA):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.stages = nn.ModuleList([SwinV23DStage(stage_config) for stage_config in config[\"stages\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DModel(SwinV23DModelWithoutSDPA):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.encoder = SwinV23DEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DMIM(SwinV23DMIMWithoutSDPA):\n",
    "    def __init__(self, swin_config, decoder_config, mim_config):\n",
    "        super().__init__(swin_config, decoder_config, mim_config)\n",
    "\n",
    "        self.swin = SwinV23DModel(swin_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DSimMIM(SwinV23DSimMIMWithoutSDPA, SwinV23DMIM):\n",
    "    def __init__(self, swin_config, decoder_config, mim_config):\n",
    "        super().__init__(swin_config, decoder_config, mim_config)  # This calls all inits in order written above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DVAEMIM(SwinV23DVAEMIMWithoutSDPA, SwinV23DMIM):\n",
    "    def __init__(self, swin_config, decoder_config, mim_config):\n",
    "        super().__init__(swin_config, decoder_config, mim_config)  # This calls all inits in order written above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1183892\u001b[0m, \u001b[1;36m197632\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "sample_spacings = torch.tensor([[1, 0.1, 0.1], [2, 0.2, 0.2], [3, 0.3, 0.3], [4, 0.4, 0.4], [5, 0.5, 0.5]])\n",
    "sample_batch = torch.rand(3, 1, 16, 128, 128)\n",
    "sample_config = {\n",
    "    \"swin\": populate_and_validate_config(\n",
    "        {\n",
    "            \"patch_size\": (1, 4, 4),\n",
    "            \"dim\": 12,\n",
    "            \"in_channels\": 1,\n",
    "            \"use_absolute_position_embeddings\": True,\n",
    "            \"learnable_absolute_position_embeddings\": False,\n",
    "            \"embed_spacing_info\": False,\n",
    "            \"image_size\": (16, 128, 128),\n",
    "            \"drop_prob\": 0.2,\n",
    "            \"stages\": [\n",
    "                {\n",
    "                    \"patch_merging\": None,\n",
    "                    \"depth\": 1,\n",
    "                    \"num_heads\": 4,\n",
    "                    \"intermediate_ratio\": 4,\n",
    "                    \"layer_norm_eps\": 1e-6,\n",
    "                    \"window_size\": (4, 4, 4),\n",
    "                    \"use_relative_position_bias\": True,\n",
    "                    \"attn_drop_prob\": 0.2,\n",
    "                    \"proj_drop_prob\": 0.2,\n",
    "                    \"mlp_drop_prob\": 0.2,\n",
    "                },\n",
    "                {\n",
    "                    \"patch_merging\": {\n",
    "                        \"merge_window_size\": (2, 2, 2),\n",
    "                        \"out_dim_ratio\": 4,\n",
    "                    },\n",
    "                    \"depth\": 3,\n",
    "                    \"num_heads\": 4,\n",
    "                    \"intermediate_ratio\": 4,\n",
    "                    \"layer_norm_eps\": 1e-6,\n",
    "                    \"window_size\": (4, 4, 4),\n",
    "                    \"use_relative_position_bias\": True,\n",
    "                },\n",
    "                {\n",
    "                    \"patch_merging\": {\n",
    "                        \"merge_window_size\": (2, 2, 2),\n",
    "                        \"out_dim_ratio\": 4,\n",
    "                    },\n",
    "                    \"depth\": 1,\n",
    "                    \"num_heads\": 4,\n",
    "                    \"intermediate_ratio\": 4,\n",
    "                    \"layer_norm_eps\": 1e-6,\n",
    "                    \"window_size\": (4, 4, 4),\n",
    "                    \"use_relative_position_bias\": True,\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ),\n",
    "    \"mim\": {\n",
    "        \"mask_ratio\": 0.7,\n",
    "        \"mask_grid_size\": (8, 8, 8),\n",
    "    },\n",
    "}\n",
    "sample_config[\"decoder\"] = {\n",
    "    \"dim\": sample_config[\"swin\"][\"stages\"][-1][\"_out_dim\"],\n",
    "    \"patch_size\": sample_config[\"swin\"][\"stages\"][-1][\"_out_patch_size\"],\n",
    "    \"in_channels\": sample_config[\"swin\"][\"in_channels\"],\n",
    "}\n",
    "\n",
    "model = SwinV23DSimMIM(sample_config['swin'], sample_config['decoder'], sample_config['mim'])\n",
    "\n",
    "sum(x.numel() for x in model.swin.parameters()), sum(x.numel() for x in model.decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1,381,536\n",
      "+---------------------------------------------------------------+------------+\n",
      "|                             Module                            | Parameters |\n",
      "+---------------------------------------------------------------+------------+\n",
      "|                           mask_token                          |     12     |\n",
      "|    swin.embeddings.patch_embeddings.patch_embeddings.weight   |    192     |\n",
      "|     swin.embeddings.patch_embeddings.patch_embeddings.bias    |     12     |\n",
      "|               swin.embeddings.layer_norm.weight               |     12     |\n",
      "|                swin.embeddings.layer_norm.bias                |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.W_qkv.weight   |    432     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mhsa.W_qkv.bias    |     36     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.proj.weight    |    144     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mhsa.proj.bias     |     12     |\n",
      "|  swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.layernorm1.weight   |     12     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.layernorm1.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mlp.dense1.weight   |    576     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mlp.dense1.bias    |     48     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mlp.dense2.weight   |    576     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mlp.dense2.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.layernorm2.weight   |     12     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.layernorm2.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mhsa.W_qkv.weight   |    432     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.W_qkv.bias    |     36     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.proj.weight   |    144     |\n",
      "|     swin.encoder.stages.0.blocks.0.sw_layer.mhsa.proj.bias    |     12     |\n",
      "| swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.layernorm1.weight   |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.layernorm1.bias    |     12     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense1.weight   |    576     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense1.bias    |     48     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense2.weight   |    576     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense2.bias    |     12     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.layernorm2.weight   |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.layernorm2.bias    |     12     |\n",
      "|     swin.encoder.stages.1.patch_merging.layer_norm.weight     |     96     |\n",
      "|      swin.encoder.stages.1.patch_merging.layer_norm.bias      |     96     |\n",
      "|        swin.encoder.stages.1.patch_merging.proj.weight        |   4,608    |\n",
      "|         swin.encoder.stages.1.patch_merging.proj.bias         |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.0.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.1.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.2.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.layernorm2.bias    |     48     |\n",
      "|     swin.encoder.stages.2.patch_merging.layer_norm.weight     |    384     |\n",
      "|      swin.encoder.stages.2.patch_merging.layer_norm.bias      |    384     |\n",
      "|        swin.encoder.stages.2.patch_merging.proj.weight        |   73,728   |\n",
      "|         swin.encoder.stages.2.patch_merging.proj.bias         |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.W_qkv.weight   |  110,592   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mhsa.W_qkv.bias    |    576     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.proj.weight    |   36,864   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mhsa.proj.bias     |    192     |\n",
      "|  swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.layernorm1.weight   |    192     |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.layernorm1.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mlp.dense1.weight   |  147,456   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mlp.dense1.bias    |    768     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mlp.dense2.weight   |  147,456   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mlp.dense2.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.layernorm2.weight   |    192     |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.layernorm2.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mhsa.W_qkv.weight   |  110,592   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.W_qkv.bias    |    576     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.proj.weight   |   36,864   |\n",
      "|     swin.encoder.stages.2.blocks.0.sw_layer.mhsa.proj.bias    |    192     |\n",
      "| swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.layernorm1.weight   |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.layernorm1.bias    |    192     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense1.weight   |  147,456   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense1.bias    |    768     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense2.weight   |  147,456   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense2.bias    |    192     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.layernorm2.weight   |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.layernorm2.bias    |    192     |\n",
      "|                     decoder.decoder.weight                    |  196,608   |\n",
      "|                      decoder.decoder.bias                     |   1,024    |\n",
      "+---------------------------------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from neuro_utils.describe import describe_model\n",
    "\n",
    "describe_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "sample_spacings = sample_spacings.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f5d8e7948b496db6c5bacc031a43bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.356076\tLR: 0.500000\n",
      "Loss: 3.334477\tLR: 0.500000\n",
      "Loss: 5.702736\tLR: 0.500000\n",
      "Loss: 4.145500\tLR: 0.500000\n",
      "Loss: 3.306134\tLR: 0.500000\n",
      "Loss: 2.851023\tLR: 0.450000\n",
      "Loss: 3.064271\tLR: 0.450000\n",
      "Loss: 1.868436\tLR: 0.450000\n",
      "Loss: 2.064336\tLR: 0.450000\n",
      "Loss: 1.785206\tLR: 0.450000\n",
      "Loss: 1.773572\tLR: 0.405000\n",
      "Loss: 1.483809\tLR: 0.405000\n",
      "Loss: 1.629214\tLR: 0.405000\n",
      "Loss: 1.603664\tLR: 0.405000\n",
      "Loss: 1.426439\tLR: 0.405000\n",
      "Loss: 1.708982\tLR: 0.364500\n",
      "Loss: 1.414643\tLR: 0.364500\n",
      "Loss: 1.447917\tLR: 0.364500\n",
      "Loss: 1.408905\tLR: 0.364500\n",
      "Loss: 1.305879\tLR: 0.364500\n",
      "Loss: 1.424265\tLR: 0.328050\n",
      "Loss: 1.184872\tLR: 0.328050\n",
      "Loss: 1.148759\tLR: 0.328050\n",
      "Loss: 1.217010\tLR: 0.328050\n",
      "Loss: 1.154983\tLR: 0.328050\n",
      "Loss: 1.349384\tLR: 0.295245\n",
      "Loss: 1.119373\tLR: 0.295245\n",
      "Loss: 1.185996\tLR: 0.295245\n",
      "Loss: 1.079443\tLR: 0.295245\n",
      "Loss: 1.110415\tLR: 0.295245\n",
      "Loss: 1.102653\tLR: 0.265721\n",
      "Loss: 1.015544\tLR: 0.265721\n",
      "Loss: 0.958608\tLR: 0.265721\n",
      "Loss: 0.970079\tLR: 0.265721\n",
      "Loss: 0.975689\tLR: 0.265721\n",
      "Loss: 1.048073\tLR: 0.239148\n",
      "Loss: 0.930088\tLR: 0.239148\n",
      "Loss: 0.930919\tLR: 0.239148\n",
      "Loss: 0.900346\tLR: 0.239148\n",
      "Loss: 0.911739\tLR: 0.239148\n",
      "Loss: 0.898669\tLR: 0.215234\n",
      "Loss: 0.865282\tLR: 0.215234\n",
      "Loss: 0.842023\tLR: 0.215234\n",
      "Loss: 0.839973\tLR: 0.215234\n",
      "Loss: 0.833077\tLR: 0.215234\n",
      "Loss: 0.832388\tLR: 0.193710\n",
      "Loss: 0.794777\tLR: 0.193710\n",
      "Loss: 0.775840\tLR: 0.193710\n",
      "Loss: 0.763727\tLR: 0.193710\n",
      "Loss: 0.758299\tLR: 0.193710\n",
      "Loss: 0.754972\tLR: 0.174339\n",
      "Loss: 0.752186\tLR: 0.174339\n",
      "Loss: 0.750992\tLR: 0.174339\n",
      "Loss: 0.750076\tLR: 0.174339\n",
      "Loss: 0.749669\tLR: 0.174339\n",
      "Loss: 0.749921\tLR: 0.156905\n",
      "Loss: 0.749953\tLR: 0.156905\n",
      "Loss: 0.749624\tLR: 0.156905\n",
      "Loss: 0.749436\tLR: 0.156905\n",
      "Loss: 0.749742\tLR: 0.156905\n",
      "Loss: 0.749282\tLR: 0.141215\n",
      "Loss: 0.749605\tLR: 0.141215\n",
      "Loss: 0.749523\tLR: 0.141215\n",
      "Loss: 0.749186\tLR: 0.141215\n",
      "Loss: 0.749381\tLR: 0.141215\n",
      "Loss: 0.749549\tLR: 0.127093\n",
      "Loss: 0.749374\tLR: 0.127093\n",
      "Loss: 0.749426\tLR: 0.127093\n",
      "Loss: 0.749691\tLR: 0.127093\n",
      "Loss: 0.749458\tLR: 0.127093\n",
      "Loss: 0.749358\tLR: 0.114384\n",
      "Loss: 0.749474\tLR: 0.114384\n",
      "Loss: 0.749314\tLR: 0.114384\n",
      "Loss: 0.749365\tLR: 0.114384\n",
      "Loss: 0.749621\tLR: 0.114384\n",
      "Loss: 0.749604\tLR: 0.102946\n",
      "Loss: 0.749433\tLR: 0.102946\n",
      "Loss: 0.749556\tLR: 0.102946\n",
      "Loss: 0.749169\tLR: 0.102946\n",
      "Loss: 0.749330\tLR: 0.102946\n",
      "Loss: 0.749436\tLR: 0.092651\n",
      "Loss: 0.749571\tLR: 0.092651\n",
      "Loss: 0.749502\tLR: 0.092651\n",
      "Loss: 0.749551\tLR: 0.092651\n",
      "Loss: 0.749657\tLR: 0.092651\n",
      "Loss: 0.749639\tLR: 0.083386\n",
      "Loss: 0.749372\tLR: 0.083386\n",
      "Loss: 0.749442\tLR: 0.083386\n",
      "Loss: 0.749422\tLR: 0.083386\n",
      "Loss: 0.749438\tLR: 0.083386\n",
      "Loss: 0.749281\tLR: 0.075047\n",
      "Loss: 0.749570\tLR: 0.075047\n",
      "Loss: 0.749679\tLR: 0.075047\n",
      "Loss: 0.749246\tLR: 0.075047\n",
      "Loss: 0.749531\tLR: 0.075047\n",
      "Loss: 0.749282\tLR: 0.067543\n",
      "Loss: 0.749282\tLR: 0.067543\n",
      "Loss: 0.749506\tLR: 0.067543\n",
      "Loss: 0.749470\tLR: 0.067543\n",
      "Loss: 0.749464\tLR: 0.067543\n",
      "Loss: 0.749523\tLR: 0.060788\n",
      "Loss: 0.749621\tLR: 0.060788\n",
      "Loss: 0.749312\tLR: 0.060788\n",
      "Loss: 0.749703\tLR: 0.060788\n",
      "Loss: 0.749636\tLR: 0.060788\n",
      "Loss: 0.749811\tLR: 0.054709\n",
      "Loss: 0.749669\tLR: 0.054709\n",
      "Loss: 0.749379\tLR: 0.054709\n",
      "Loss: 0.749556\tLR: 0.054709\n",
      "Loss: 0.749410\tLR: 0.054709\n",
      "Loss: 0.749221\tLR: 0.049239\n",
      "Loss: 0.749609\tLR: 0.049239\n",
      "Loss: 0.749420\tLR: 0.049239\n",
      "Loss: 0.749258\tLR: 0.049239\n",
      "Loss: 0.749609\tLR: 0.049239\n",
      "Loss: 0.749463\tLR: 0.044315\n",
      "Loss: 0.749258\tLR: 0.044315\n",
      "Loss: 0.749620\tLR: 0.044315\n",
      "Loss: 0.749534\tLR: 0.044315\n",
      "Loss: 0.749479\tLR: 0.044315\n",
      "Loss: 0.749522\tLR: 0.039883\n",
      "Loss: 0.749511\tLR: 0.039883\n",
      "Loss: 0.749230\tLR: 0.039883\n",
      "Loss: 0.749535\tLR: 0.039883\n",
      "Loss: 0.749362\tLR: 0.039883\n",
      "Loss: 0.749793\tLR: 0.035895\n",
      "Loss: 0.749011\tLR: 0.035895\n",
      "Loss: 0.749460\tLR: 0.035895\n",
      "Loss: 0.749397\tLR: 0.035895\n",
      "Loss: 0.749225\tLR: 0.035895\n",
      "Loss: 0.749377\tLR: 0.032305\n",
      "Loss: 0.749218\tLR: 0.032305\n",
      "Loss: 0.749606\tLR: 0.032305\n",
      "Loss: 0.749025\tLR: 0.032305\n",
      "Loss: 0.749408\tLR: 0.032305\n",
      "Loss: 0.749429\tLR: 0.029075\n",
      "Loss: 0.749404\tLR: 0.029075\n",
      "Loss: 0.749700\tLR: 0.029075\n",
      "Loss: 0.749391\tLR: 0.029075\n",
      "Loss: 0.749022\tLR: 0.029075\n",
      "Loss: 0.749196\tLR: 0.026167\n",
      "Loss: 0.749822\tLR: 0.026167\n",
      "Loss: 0.749463\tLR: 0.026167\n",
      "Loss: 0.749503\tLR: 0.026167\n",
      "Loss: 0.749288\tLR: 0.026167\n",
      "Loss: 0.749465\tLR: 0.023551\n",
      "Loss: 0.749727\tLR: 0.023551\n",
      "Loss: 0.749276\tLR: 0.023551\n",
      "Loss: 0.749610\tLR: 0.023551\n",
      "Loss: 0.749441\tLR: 0.023551\n",
      "Loss: 0.749587\tLR: 0.021196\n",
      "Loss: 0.749267\tLR: 0.021196\n",
      "Loss: 0.749665\tLR: 0.021196\n",
      "Loss: 0.749346\tLR: 0.021196\n",
      "Loss: 0.749049\tLR: 0.021196\n",
      "Loss: 0.749517\tLR: 0.019076\n",
      "Loss: 0.749558\tLR: 0.019076\n",
      "Loss: 0.749312\tLR: 0.019076\n",
      "Loss: 0.749718\tLR: 0.019076\n",
      "Loss: 0.749344\tLR: 0.019076\n",
      "Loss: 0.749560\tLR: 0.017168\n",
      "Loss: 0.749375\tLR: 0.017168\n",
      "Loss: 0.749204\tLR: 0.017168\n",
      "Loss: 0.749390\tLR: 0.017168\n",
      "Loss: 0.749396\tLR: 0.017168\n",
      "Loss: 0.749610\tLR: 0.015452\n",
      "Loss: 0.749511\tLR: 0.015452\n",
      "Loss: 0.749478\tLR: 0.015452\n",
      "Loss: 0.749167\tLR: 0.015452\n",
      "Loss: 0.749410\tLR: 0.015452\n",
      "Loss: 0.749565\tLR: 0.013906\n",
      "Loss: 0.749460\tLR: 0.013906\n",
      "Loss: 0.749963\tLR: 0.013906\n",
      "Loss: 0.749491\tLR: 0.013906\n",
      "Loss: 0.749236\tLR: 0.013906\n",
      "Loss: 0.749164\tLR: 0.012516\n",
      "Loss: 0.749665\tLR: 0.012516\n",
      "Loss: 0.749397\tLR: 0.012516\n",
      "Loss: 0.749137\tLR: 0.012516\n",
      "Loss: 0.749837\tLR: 0.012516\n",
      "Loss: 0.749554\tLR: 0.011264\n",
      "Loss: 0.749602\tLR: 0.011264\n",
      "Loss: 0.749223\tLR: 0.011264\n",
      "Loss: 0.749713\tLR: 0.011264\n",
      "Loss: 0.749240\tLR: 0.011264\n",
      "Loss: 0.749191\tLR: 0.010138\n",
      "Loss: 0.749643\tLR: 0.010138\n",
      "Loss: 0.749410\tLR: 0.010138\n",
      "Loss: 0.749574\tLR: 0.010138\n",
      "Loss: 0.749385\tLR: 0.010138\n",
      "Loss: 0.749505\tLR: 0.009124\n",
      "Loss: 0.749475\tLR: 0.009124\n",
      "Loss: 0.749520\tLR: 0.009124\n",
      "Loss: 0.749432\tLR: 0.009124\n",
      "Loss: 0.749119\tLR: 0.009124\n",
      "Loss: 0.749246\tLR: 0.008212\n",
      "Loss: 0.749530\tLR: 0.008212\n",
      "Loss: 0.749307\tLR: 0.008212\n",
      "Loss: 0.749339\tLR: 0.008212\n",
      "Loss: 0.749681\tLR: 0.008212\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch, sample_spacings)\n",
    "    print(f\"Loss: {output[1]:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    output[1].backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1183892\u001b[0m, \u001b[1;36m197632\u001b[0m, \u001b[1;36m74112\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_config['decoder']['beta'] = None\n",
    "sample_config['decoder']['beta_schedule'] = (100, 0, 2)\n",
    "\n",
    "model = SwinV23DVAEMIM(sample_config['swin'], sample_config['decoder'], sample_config['mim'])\n",
    "\n",
    "encoder_params = sum(x.numel() for x in model.swin.parameters())\n",
    "decoder_params = sum(x.numel() for x in model.decoder.parameters())\n",
    "sampling_params = sum(x.numel() for x in model.mu_layer.parameters()) + sum(\n",
    "    x.numel() for x in model.logvar_layer.parameters()\n",
    ")\n",
    "encoder_params, decoder_params, sampling_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "sample_spacings = sample_spacings.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cf50cd161b476db234f8df8f043909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.494753\tLR: 0.001000\tBeta: 0.000000\n",
      "Loss: 4.444311\tLR: 0.001000\tBeta: 0.020000\n",
      "Loss: 6.283701\tLR: 0.001000\tBeta: 0.040000\n",
      "Loss: 16.800419\tLR: 0.001000\tBeta: 0.060000\n",
      "Loss: 9.140267\tLR: 0.001000\tBeta: 0.080000\n",
      "Loss: 7.460201\tLR: 0.000900\tBeta: 0.100000\n",
      "Loss: 8.268412\tLR: 0.000900\tBeta: 0.120000\n",
      "Loss: 9.869583\tLR: 0.000900\tBeta: 0.140000\n",
      "Loss: 12.585202\tLR: 0.000900\tBeta: 0.160000\n",
      "Loss: 15.908322\tLR: 0.000900\tBeta: 0.180000\n",
      "Loss: 13.942308\tLR: 0.000810\tBeta: 0.200000\n",
      "Loss: 6.442122\tLR: 0.000810\tBeta: 0.220000\n",
      "Loss: 6.089732\tLR: 0.000810\tBeta: 0.240000\n",
      "Loss: 6.108383\tLR: 0.000810\tBeta: 0.260000\n",
      "Loss: 8.575150\tLR: 0.000810\tBeta: 0.280000\n",
      "Loss: 10.641992\tLR: 0.000729\tBeta: 0.300000\n",
      "Loss: 9.281338\tLR: 0.000729\tBeta: 0.320000\n",
      "Loss: 7.760039\tLR: 0.000729\tBeta: 0.340000\n",
      "Loss: 7.441273\tLR: 0.000729\tBeta: 0.360000\n",
      "Loss: 9.329135\tLR: 0.000729\tBeta: 0.380000\n",
      "Loss: 10.819846\tLR: 0.000656\tBeta: 0.400000\n",
      "Loss: 9.125205\tLR: 0.000656\tBeta: 0.420000\n",
      "Loss: 5.875267\tLR: 0.000656\tBeta: 0.440000\n",
      "Loss: 4.806513\tLR: 0.000656\tBeta: 0.460000\n",
      "Loss: 4.617932\tLR: 0.000656\tBeta: 0.480000\n",
      "Loss: 5.808108\tLR: 0.000590\tBeta: 0.500000\n",
      "Loss: 6.663777\tLR: 0.000590\tBeta: 0.520000\n",
      "Loss: 8.094163\tLR: 0.000590\tBeta: 0.540000\n",
      "Loss: 9.491042\tLR: 0.000590\tBeta: 0.560000\n",
      "Loss: 9.292027\tLR: 0.000590\tBeta: 0.580000\n",
      "Loss: 8.307156\tLR: 0.000531\tBeta: 0.600000\n",
      "Loss: 4.730088\tLR: 0.000531\tBeta: 0.620000\n",
      "Loss: 3.497044\tLR: 0.000531\tBeta: 0.640000\n",
      "Loss: 3.140891\tLR: 0.000531\tBeta: 0.660000\n",
      "Loss: 3.356820\tLR: 0.000531\tBeta: 0.680000\n",
      "Loss: 4.071903\tLR: 0.000478\tBeta: 0.700000\n",
      "Loss: 4.255055\tLR: 0.000478\tBeta: 0.720000\n",
      "Loss: 4.740982\tLR: 0.000478\tBeta: 0.740000\n",
      "Loss: 5.511296\tLR: 0.000478\tBeta: 0.760000\n",
      "Loss: 6.474718\tLR: 0.000478\tBeta: 0.780000\n",
      "Loss: 7.367472\tLR: 0.000430\tBeta: 0.800000\n",
      "Loss: 5.747357\tLR: 0.000430\tBeta: 0.820000\n",
      "Loss: 4.356796\tLR: 0.000430\tBeta: 0.840000\n",
      "Loss: 3.532301\tLR: 0.000430\tBeta: 0.860000\n",
      "Loss: 3.031710\tLR: 0.000430\tBeta: 0.880000\n",
      "Loss: 2.807204\tLR: 0.000387\tBeta: 0.900000\n",
      "Loss: 2.317821\tLR: 0.000387\tBeta: 0.920000\n",
      "Loss: 2.112914\tLR: 0.000387\tBeta: 0.940000\n",
      "Loss: 2.040892\tLR: 0.000387\tBeta: 0.960000\n",
      "Loss: 2.076647\tLR: 0.000387\tBeta: 0.980000\n",
      "Loss: 2.211030\tLR: 0.000349\tBeta: 1.000000\n",
      "Loss: 2.140365\tLR: 0.000349\tBeta: 1.020000\n",
      "Loss: 2.159584\tLR: 0.000349\tBeta: 1.040000\n",
      "Loss: 2.254931\tLR: 0.000349\tBeta: 1.060000\n",
      "Loss: 2.429906\tLR: 0.000349\tBeta: 1.080000\n",
      "Loss: 2.657706\tLR: 0.000314\tBeta: 1.100000\n",
      "Loss: 2.537779\tLR: 0.000314\tBeta: 1.120000\n",
      "Loss: 2.455308\tLR: 0.000314\tBeta: 1.140000\n",
      "Loss: 2.432484\tLR: 0.000314\tBeta: 1.160000\n",
      "Loss: 2.458261\tLR: 0.000314\tBeta: 1.180000\n",
      "Loss: 2.535715\tLR: 0.000282\tBeta: 1.200000\n",
      "Loss: 2.217612\tLR: 0.000282\tBeta: 1.220000\n",
      "Loss: 2.015187\tLR: 0.000282\tBeta: 1.240000\n",
      "Loss: 1.922818\tLR: 0.000282\tBeta: 1.260000\n",
      "Loss: 1.844895\tLR: 0.000282\tBeta: 1.280000\n",
      "Loss: 1.824036\tLR: 0.000254\tBeta: 1.300000\n",
      "Loss: 1.691846\tLR: 0.000254\tBeta: 1.320000\n",
      "Loss: 1.633452\tLR: 0.000254\tBeta: 1.340000\n",
      "Loss: 1.582836\tLR: 0.000254\tBeta: 1.360000\n",
      "Loss: 1.554804\tLR: 0.000254\tBeta: 1.380000\n",
      "Loss: 1.520308\tLR: 0.000229\tBeta: 1.400000\n",
      "Loss: 1.513356\tLR: 0.000229\tBeta: 1.420000\n",
      "Loss: 1.488985\tLR: 0.000229\tBeta: 1.440000\n",
      "Loss: 1.494623\tLR: 0.000229\tBeta: 1.460000\n",
      "Loss: 1.495261\tLR: 0.000229\tBeta: 1.480000\n",
      "Loss: 1.486380\tLR: 0.000206\tBeta: 1.500000\n",
      "Loss: 1.491729\tLR: 0.000206\tBeta: 1.520000\n",
      "Loss: 1.493920\tLR: 0.000206\tBeta: 1.540000\n",
      "Loss: 1.495512\tLR: 0.000206\tBeta: 1.560000\n",
      "Loss: 1.499758\tLR: 0.000206\tBeta: 1.580000\n",
      "Loss: 1.487227\tLR: 0.000185\tBeta: 1.600000\n",
      "Loss: 1.482359\tLR: 0.000185\tBeta: 1.620000\n",
      "Loss: 1.500050\tLR: 0.000185\tBeta: 1.640000\n",
      "Loss: 1.497453\tLR: 0.000185\tBeta: 1.660000\n",
      "Loss: 1.500440\tLR: 0.000185\tBeta: 1.680000\n",
      "Loss: 1.509466\tLR: 0.000167\tBeta: 1.700000\n",
      "Loss: 1.501283\tLR: 0.000167\tBeta: 1.720000\n",
      "Loss: 1.517751\tLR: 0.000167\tBeta: 1.740000\n",
      "Loss: 1.516009\tLR: 0.000167\tBeta: 1.760000\n",
      "Loss: 1.526785\tLR: 0.000167\tBeta: 1.780000\n",
      "Loss: 1.531080\tLR: 0.000150\tBeta: 1.800000\n",
      "Loss: 1.536500\tLR: 0.000150\tBeta: 1.820000\n",
      "Loss: 1.528550\tLR: 0.000150\tBeta: 1.840000\n",
      "Loss: 1.534507\tLR: 0.000150\tBeta: 1.860000\n",
      "Loss: 1.550107\tLR: 0.000150\tBeta: 1.880000\n",
      "Loss: 1.545322\tLR: 0.000135\tBeta: 1.900000\n",
      "Loss: 1.550282\tLR: 0.000135\tBeta: 1.920000\n",
      "Loss: 1.554406\tLR: 0.000135\tBeta: 1.940000\n",
      "Loss: 1.559388\tLR: 0.000135\tBeta: 1.960000\n",
      "Loss: 1.563088\tLR: 0.000135\tBeta: 1.980000\n",
      "Loss: 1.564775\tLR: 0.000122\tBeta: 2.000000\n",
      "Loss: 1.556687\tLR: 0.000122\tBeta: 2.000000\n",
      "Loss: 1.554421\tLR: 0.000122\tBeta: 2.000000\n",
      "Loss: 1.541538\tLR: 0.000122\tBeta: 2.000000\n",
      "Loss: 1.542512\tLR: 0.000122\tBeta: 2.000000\n",
      "Loss: 1.550429\tLR: 0.000109\tBeta: 2.000000\n",
      "Loss: 1.538294\tLR: 0.000109\tBeta: 2.000000\n",
      "Loss: 1.533406\tLR: 0.000109\tBeta: 2.000000\n",
      "Loss: 1.535998\tLR: 0.000109\tBeta: 2.000000\n",
      "Loss: 1.515030\tLR: 0.000109\tBeta: 2.000000\n",
      "Loss: 1.521267\tLR: 0.000098\tBeta: 2.000000\n",
      "Loss: 1.511556\tLR: 0.000098\tBeta: 2.000000\n",
      "Loss: 1.518951\tLR: 0.000098\tBeta: 2.000000\n",
      "Loss: 1.516504\tLR: 0.000098\tBeta: 2.000000\n",
      "Loss: 1.496266\tLR: 0.000098\tBeta: 2.000000\n",
      "Loss: 1.502874\tLR: 0.000089\tBeta: 2.000000\n",
      "Loss: 1.499985\tLR: 0.000089\tBeta: 2.000000\n",
      "Loss: 1.515819\tLR: 0.000089\tBeta: 2.000000\n",
      "Loss: 1.495569\tLR: 0.000089\tBeta: 2.000000\n",
      "Loss: 1.491960\tLR: 0.000089\tBeta: 2.000000\n",
      "Loss: 1.492050\tLR: 0.000080\tBeta: 2.000000\n",
      "Loss: 1.490896\tLR: 0.000080\tBeta: 2.000000\n",
      "Loss: 1.485948\tLR: 0.000080\tBeta: 2.000000\n",
      "Loss: 1.488424\tLR: 0.000080\tBeta: 2.000000\n",
      "Loss: 1.489927\tLR: 0.000080\tBeta: 2.000000\n",
      "Loss: 1.473956\tLR: 0.000072\tBeta: 2.000000\n",
      "Loss: 1.469484\tLR: 0.000072\tBeta: 2.000000\n",
      "Loss: 1.480910\tLR: 0.000072\tBeta: 2.000000\n",
      "Loss: 1.485661\tLR: 0.000072\tBeta: 2.000000\n",
      "Loss: 1.474638\tLR: 0.000072\tBeta: 2.000000\n",
      "Loss: 1.480119\tLR: 0.000065\tBeta: 2.000000\n",
      "Loss: 1.485809\tLR: 0.000065\tBeta: 2.000000\n",
      "Loss: 1.472945\tLR: 0.000065\tBeta: 2.000000\n",
      "Loss: 1.465080\tLR: 0.000065\tBeta: 2.000000\n",
      "Loss: 1.471198\tLR: 0.000065\tBeta: 2.000000\n",
      "Loss: 1.463759\tLR: 0.000058\tBeta: 2.000000\n",
      "Loss: 1.469596\tLR: 0.000058\tBeta: 2.000000\n",
      "Loss: 1.449734\tLR: 0.000058\tBeta: 2.000000\n",
      "Loss: 1.453392\tLR: 0.000058\tBeta: 2.000000\n",
      "Loss: 1.458339\tLR: 0.000058\tBeta: 2.000000\n",
      "Loss: 1.455877\tLR: 0.000052\tBeta: 2.000000\n",
      "Loss: 1.449702\tLR: 0.000052\tBeta: 2.000000\n",
      "Loss: 1.460032\tLR: 0.000052\tBeta: 2.000000\n",
      "Loss: 1.449164\tLR: 0.000052\tBeta: 2.000000\n",
      "Loss: 1.463161\tLR: 0.000052\tBeta: 2.000000\n",
      "Loss: 1.455352\tLR: 0.000047\tBeta: 2.000000\n",
      "Loss: 1.452486\tLR: 0.000047\tBeta: 2.000000\n",
      "Loss: 1.440969\tLR: 0.000047\tBeta: 2.000000\n",
      "Loss: 1.444197\tLR: 0.000047\tBeta: 2.000000\n",
      "Loss: 1.444685\tLR: 0.000047\tBeta: 2.000000\n",
      "Loss: 1.445744\tLR: 0.000042\tBeta: 2.000000\n",
      "Loss: 1.443839\tLR: 0.000042\tBeta: 2.000000\n",
      "Loss: 1.433542\tLR: 0.000042\tBeta: 2.000000\n",
      "Loss: 1.443351\tLR: 0.000042\tBeta: 2.000000\n",
      "Loss: 1.444486\tLR: 0.000042\tBeta: 2.000000\n",
      "Loss: 1.445268\tLR: 0.000038\tBeta: 2.000000\n",
      "Loss: 1.435627\tLR: 0.000038\tBeta: 2.000000\n",
      "Loss: 1.446416\tLR: 0.000038\tBeta: 2.000000\n",
      "Loss: 1.446167\tLR: 0.000038\tBeta: 2.000000\n",
      "Loss: 1.439757\tLR: 0.000038\tBeta: 2.000000\n",
      "Loss: 1.436230\tLR: 0.000034\tBeta: 2.000000\n",
      "Loss: 1.432221\tLR: 0.000034\tBeta: 2.000000\n",
      "Loss: 1.431714\tLR: 0.000034\tBeta: 2.000000\n",
      "Loss: 1.437523\tLR: 0.000034\tBeta: 2.000000\n",
      "Loss: 1.432269\tLR: 0.000034\tBeta: 2.000000\n",
      "Loss: 1.433507\tLR: 0.000031\tBeta: 2.000000\n",
      "Loss: 1.420419\tLR: 0.000031\tBeta: 2.000000\n",
      "Loss: 1.436862\tLR: 0.000031\tBeta: 2.000000\n",
      "Loss: 1.427465\tLR: 0.000031\tBeta: 2.000000\n",
      "Loss: 1.434209\tLR: 0.000031\tBeta: 2.000000\n",
      "Loss: 1.439345\tLR: 0.000028\tBeta: 2.000000\n",
      "Loss: 1.427285\tLR: 0.000028\tBeta: 2.000000\n",
      "Loss: 1.434585\tLR: 0.000028\tBeta: 2.000000\n",
      "Loss: 1.425735\tLR: 0.000028\tBeta: 2.000000\n",
      "Loss: 1.413599\tLR: 0.000028\tBeta: 2.000000\n",
      "Loss: 1.429448\tLR: 0.000025\tBeta: 2.000000\n",
      "Loss: 1.420674\tLR: 0.000025\tBeta: 2.000000\n",
      "Loss: 1.411784\tLR: 0.000025\tBeta: 2.000000\n",
      "Loss: 1.416606\tLR: 0.000025\tBeta: 2.000000\n",
      "Loss: 1.420864\tLR: 0.000025\tBeta: 2.000000\n",
      "Loss: 1.407161\tLR: 0.000023\tBeta: 2.000000\n",
      "Loss: 1.414664\tLR: 0.000023\tBeta: 2.000000\n",
      "Loss: 1.422229\tLR: 0.000023\tBeta: 2.000000\n",
      "Loss: 1.413551\tLR: 0.000023\tBeta: 2.000000\n",
      "Loss: 1.420661\tLR: 0.000023\tBeta: 2.000000\n",
      "Loss: 1.417667\tLR: 0.000020\tBeta: 2.000000\n",
      "Loss: 1.411732\tLR: 0.000020\tBeta: 2.000000\n",
      "Loss: 1.423517\tLR: 0.000020\tBeta: 2.000000\n",
      "Loss: 1.419387\tLR: 0.000020\tBeta: 2.000000\n",
      "Loss: 1.411038\tLR: 0.000020\tBeta: 2.000000\n",
      "Loss: 1.411221\tLR: 0.000018\tBeta: 2.000000\n",
      "Loss: 1.411029\tLR: 0.000018\tBeta: 2.000000\n",
      "Loss: 1.416566\tLR: 0.000018\tBeta: 2.000000\n",
      "Loss: 1.410593\tLR: 0.000018\tBeta: 2.000000\n",
      "Loss: 1.414047\tLR: 0.000018\tBeta: 2.000000\n",
      "Loss: 1.418036\tLR: 0.000016\tBeta: 2.000000\n",
      "Loss: 1.404949\tLR: 0.000016\tBeta: 2.000000\n",
      "Loss: 1.405240\tLR: 0.000016\tBeta: 2.000000\n",
      "Loss: 1.413098\tLR: 0.000016\tBeta: 2.000000\n",
      "Loss: 1.414999\tLR: 0.000016\tBeta: 2.000000\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch, sample_spacings)\n",
    "    print(f\"Loss: {output[1]:f}\\tLR: {scheduler.get_last_lr()[0]:f}\\tBeta: {output[3][2]:f}\")\n",
    "    # print(output[-1])\n",
    "    output[1].backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "coords_h = torch.arange(4)\n",
    "coords_w = torch.arange(4)\n",
    "coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "coords_flatten = torch.flatten(coords, 1)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += 4 - 1\n",
    "relative_coords[:, :, 1] += 4 - 1\n",
    "relative_coords[:, :, 0] *= 2 * 4 - 1\n",
    "relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "window_size = (4, 4, 4)\n",
    "relative_limits = (7, 7, 7)\n",
    "\n",
    "coords = get_coords_grid(window_size)\n",
    "coords_flatten = rearrange(coords, \"three_dimensional d h w -> three_dimensional (d h w)\", three_dimensional=3)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += window_size[0] - 1\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "relative_coords[:, :, 2] += window_size[2] - 1\n",
    "relative_position_index: torch.Tensor = (\n",
    "    relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 2]\n",
    ")\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "window_size = (2, 2, 2)\n",
    "relative_limits = (2 * window_size[0] - 1, 2 * window_size[1] - 1, 2 * window_size[2] - 1)\n",
    "\n",
    "relative_coords_table = get_coords_grid(relative_limits)\n",
    "relative_coords_table[0] -= window_size[0] - 1\n",
    "relative_coords_table[1] -= window_size[1] - 1\n",
    "relative_coords_table[2] -= window_size[2] - 1\n",
    "relative_coords_table = relative_coords_table.permute(1, 2, 3, 0).contiguous()\n",
    "relative_coords_table[0, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
