{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp swinv2_3d_with_sdpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from vision_architectures.swinv2_3d import (\n",
    "    populate_and_validate_config,\n",
    "    get_coords_grid,\n",
    "    SwinV23DMHSA as SwinV23DMHSAWithoutSDPA,\n",
    "    SwinV23DLayerMLP,\n",
    "    SwinV23DLayer as SwinV23DLayerWithoutSDPA,\n",
    "    SwinV23DBlock as SwinV23DBlockWithoutSDPA,\n",
    "    SwinV23DPatchMerging,\n",
    "    SwinV23DStage as SwinV23DStageWithoutSDPA,\n",
    "    SwinV23DEncoder as SwinV23DEncoderWithoutSDPA,\n",
    "    SwinV23DPatchEmbeddings,\n",
    "    get_3d_position_embeddings,\n",
    "    embed_spacings_in_position_embeddings,\n",
    "    SwinV23DEmbeddings,\n",
    "    SwinV23DModel as SwinV23DModelWithoutSDPA,\n",
    "    SwinV23DMIMDecoder,\n",
    "    SwinV23DMIM as SwinV23DMIMWithoutSDPA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DMHSA(SwinV23DMHSAWithoutSDPA):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        use_relative_position_bias,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__(dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob)\n",
    "\n",
    "        # Remove attention dropout layer as that is handled automatically, but store the dropout for later\n",
    "        del self.attn_drop\n",
    "        self.attn_drop_prob = attn_drop_prob\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        # hidden_states: (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "        _, num_patches_z, num_patches_y, num_patches_x, _ = hidden_states.shape\n",
    "\n",
    "        query, key, value = rearrange(\n",
    "            self.W_qkv(hidden_states),\n",
    "            \"b nz ny nx (n num_heads d) -> n b num_heads (nz ny nx) d\",\n",
    "            n=3,\n",
    "            num_heads=self.num_heads,\n",
    "        )\n",
    "        # num_patches = window_size_z * window_size_y * window_size_x\n",
    "        # Each is (windowed_b, num_heads, num_patches, per_head_dim)\n",
    "\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=np.log(1.0 / 0.01)).exp()\n",
    "\n",
    "        query_normalized = F.normalize(query, dim=-1)\n",
    "        key_normalized = F.normalize(key, dim=-1)\n",
    "\n",
    "        query_normalized_and_scaled = query_normalized * logit_scale  # Scale the query beforehand\n",
    "\n",
    "        relative_position_bias = None\n",
    "        if self.use_relative_position_bias:\n",
    "            relative_position_bias = self.calculate_relative_position_bias()\n",
    "\n",
    "        context = F.scaled_dot_product_attention(\n",
    "            query_normalized_and_scaled,\n",
    "            key_normalized,\n",
    "            value,\n",
    "            attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias\n",
    "            dropout_p=self.attn_drop_prob,\n",
    "            is_causal=False,\n",
    "            scale=1.0,  # Already scaled the vectors\n",
    "        )\n",
    "        # (windowed_b, num_heads, num_patches, per_head_dim)\n",
    "        \n",
    "        context = rearrange(\n",
    "            context,\n",
    "            \"b num_heads (num_patches_z num_patches_y num_patches_x) d -> \"\n",
    "            \"b num_patches_z num_patches_y num_patches_x (num_heads d)\",\n",
    "            num_patches_z=num_patches_z,\n",
    "            num_patches_y=num_patches_y,\n",
    "            num_patches_x=num_patches_x,\n",
    "        )\n",
    "        # (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "\n",
    "        context = self.proj(context)\n",
    "        context = self.proj_drop(context)\n",
    "        # (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mSwinV23DMHSA\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_qkv\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m162\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcpb_mlp\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33minplace\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = SwinV23DMHSA(54, 6, (4, 4, 4), True)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 4, 4, 4, 54)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for use_relative_position_bias in [True, False]:\n",
    "    m1 = SwinV23DMHSA(54, 6, (4, 4, 4), use_relative_position_bias)\n",
    "    m2 = SwinV23DMHSAWithoutSDPA(54, 6, (4, 4, 4), use_relative_position_bias)\n",
    "\n",
    "    m1.load_state_dict(m2.state_dict())\n",
    "    m1.eval(), m2.eval()\n",
    "\n",
    "    example_input = torch.randn(2, 4, 4, 4, 54)\n",
    "    o1 = m1(example_input)\n",
    "    o2 = m2(example_input)\n",
    "\n",
    "    assert torch.allclose(o1, o2, atol=1e-6), (o1 - o2).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify other classes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DLayer(SwinV23DLayerWithoutSDPA):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        intermediate_ratio,\n",
    "        layer_norm_eps,\n",
    "        window_size,\n",
    "        use_relative_position_bias,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "        mlp_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim,\n",
    "            num_heads,\n",
    "            intermediate_ratio,\n",
    "            layer_norm_eps,\n",
    "            window_size,\n",
    "            use_relative_position_bias,\n",
    "            attn_drop_prob,\n",
    "            proj_drop_prob,\n",
    "            mlp_drop_prob,\n",
    "        )\n",
    "\n",
    "        self.mhsa = SwinV23DMHSA(\n",
    "            dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DBlock(SwinV23DBlockWithoutSDPA):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__(stage_config)\n",
    "\n",
    "        self.stage_config = stage_config\n",
    "        self.w_layer = SwinV23DLayer(\n",
    "            stage_config[\"_out_dim\"],\n",
    "            stage_config[\"num_heads\"],\n",
    "            stage_config[\"intermediate_ratio\"],\n",
    "            stage_config[\"layer_norm_eps\"],\n",
    "            stage_config[\"window_size\"],\n",
    "            stage_config[\"use_relative_position_bias\"],\n",
    "            stage_config.get(\"attn_drop_prob\", 0.0),\n",
    "            stage_config.get(\"proj_drop_prob\", 0.0),\n",
    "            stage_config.get(\"mlp_drop_prob\", 0.0),\n",
    "        )\n",
    "        self.sw_layer = SwinV23DLayer(\n",
    "            stage_config[\"_out_dim\"],\n",
    "            stage_config[\"num_heads\"],\n",
    "            stage_config[\"intermediate_ratio\"],\n",
    "            stage_config[\"layer_norm_eps\"],\n",
    "            stage_config[\"window_size\"],\n",
    "            stage_config[\"use_relative_position_bias\"],\n",
    "            stage_config.get(\"attn_drop_prob\", 0.0),\n",
    "            stage_config.get(\"proj_drop_prob\", 0.0),\n",
    "            stage_config.get(\"mlp_drop_prob\", 0.0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DStage(SwinV23DStageWithoutSDPA):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__(stage_config)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [SwinV23DBlock(stage_config) for _ in range(stage_config[\"depth\"])],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DEncoder(SwinV23DEncoderWithoutSDPA):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.stages = nn.ModuleList([SwinV23DStage(stage_config) for stage_config in config[\"stages\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DModel(SwinV23DModelWithoutSDPA):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.encoder = SwinV23DEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DMIM(SwinV23DMIMWithoutSDPA):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.swin = SwinV23DModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1183892\u001b[0m, \u001b[1;36m197632\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "sample_spacings = torch.tensor([[1, 0.1, 0.1], [2, 0.2, 0.2], [3, 0.3, 0.3], [4, 0.4, 0.4], [5, 0.5, 0.5]])\n",
    "sample_batch = torch.rand(3, 1, 16, 128, 128)\n",
    "sample_config = populate_and_validate_config(\n",
    "    {\n",
    "        \"patch_size\": (1, 4, 4),\n",
    "        \"dim\": 12,\n",
    "        \"in_channels\": 1,\n",
    "        \"use_absolute_position_embeddings\": True,\n",
    "        \"learnable_absolute_position_embeddings\": False,\n",
    "        \"embed_spacing_info\": False,\n",
    "        \"image_size\": (16, 128, 128),\n",
    "        \"drop_prob\": 0.2,\n",
    "        \"stages\": [\n",
    "            {\n",
    "                \"patch_merging\": None,\n",
    "                \"depth\": 1,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "                \"attn_drop_prob\": 0.2,\n",
    "                \"proj_drop_prob\": 0.2,\n",
    "                \"mlp_drop_prob\": 0.2,\n",
    "            },\n",
    "            {\n",
    "                \"patch_merging\": {\n",
    "                    \"merge_window_size\": (2, 2, 2),\n",
    "                    \"out_dim_ratio\": 4,\n",
    "                },\n",
    "                \"depth\": 3,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "            },\n",
    "            {\n",
    "                \"patch_merging\": {\n",
    "                    \"merge_window_size\": (2, 2, 2),\n",
    "                    \"out_dim_ratio\": 4,\n",
    "                },\n",
    "                \"depth\": 1,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "            },\n",
    "        ],\n",
    "        \"mim\": {\n",
    "            \"mask_ratio\": 0.7,\n",
    "            \"mask_grid_size\": (8, 8, 8),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "model = SwinV23DMIM(sample_config)\n",
    "\n",
    "sum(x.numel() for x in model.swin.parameters()), sum(x.numel() for x in model.decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1,381,536\n",
      "+---------------------------------------------------------------+------------+\n",
      "|                             Module                            | Parameters |\n",
      "+---------------------------------------------------------------+------------+\n",
      "|                           mask_token                          |     12     |\n",
      "|    swin.embeddings.patch_embeddings.patch_embeddings.weight   |    192     |\n",
      "|     swin.embeddings.patch_embeddings.patch_embeddings.bias    |     12     |\n",
      "|               swin.embeddings.layer_norm.weight               |     12     |\n",
      "|                swin.embeddings.layer_norm.bias                |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.W_qkv.weight   |    432     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mhsa.W_qkv.bias    |     36     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.proj.weight    |    144     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mhsa.proj.bias     |     12     |\n",
      "|  swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.layernorm1.weight   |     12     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.layernorm1.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mlp.dense1.weight   |    576     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mlp.dense1.bias    |     48     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mlp.dense2.weight   |    576     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mlp.dense2.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.layernorm2.weight   |     12     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.layernorm2.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mhsa.W_qkv.weight   |    432     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.W_qkv.bias    |     36     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.proj.weight   |    144     |\n",
      "|     swin.encoder.stages.0.blocks.0.sw_layer.mhsa.proj.bias    |     12     |\n",
      "| swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.layernorm1.weight   |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.layernorm1.bias    |     12     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense1.weight   |    576     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense1.bias    |     48     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense2.weight   |    576     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense2.bias    |     12     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.layernorm2.weight   |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.layernorm2.bias    |     12     |\n",
      "|     swin.encoder.stages.1.patch_merging.layer_norm.weight     |     96     |\n",
      "|      swin.encoder.stages.1.patch_merging.layer_norm.bias      |     96     |\n",
      "|        swin.encoder.stages.1.patch_merging.proj.weight        |   4,608    |\n",
      "|         swin.encoder.stages.1.patch_merging.proj.bias         |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.0.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.1.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.2.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.layernorm2.bias    |     48     |\n",
      "|     swin.encoder.stages.2.patch_merging.layer_norm.weight     |    384     |\n",
      "|      swin.encoder.stages.2.patch_merging.layer_norm.bias      |    384     |\n",
      "|        swin.encoder.stages.2.patch_merging.proj.weight        |   73,728   |\n",
      "|         swin.encoder.stages.2.patch_merging.proj.bias         |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.W_qkv.weight   |  110,592   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mhsa.W_qkv.bias    |    576     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.proj.weight    |   36,864   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mhsa.proj.bias     |    192     |\n",
      "|  swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.layernorm1.weight   |    192     |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.layernorm1.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mlp.dense1.weight   |  147,456   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mlp.dense1.bias    |    768     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mlp.dense2.weight   |  147,456   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mlp.dense2.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.layernorm2.weight   |    192     |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.layernorm2.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mhsa.W_qkv.weight   |  110,592   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.W_qkv.bias    |    576     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.proj.weight   |   36,864   |\n",
      "|     swin.encoder.stages.2.blocks.0.sw_layer.mhsa.proj.bias    |    192     |\n",
      "| swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.layernorm1.weight   |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.layernorm1.bias    |    192     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense1.weight   |  147,456   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense1.bias    |    768     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense2.weight   |  147,456   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense2.bias    |    192     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.layernorm2.weight   |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.layernorm2.bias    |    192     |\n",
      "|                     decoder.decoder.weight                    |  196,608   |\n",
      "|                      decoder.decoder.bias                     |   1,024    |\n",
      "+---------------------------------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from neuro_utils.describe import describe_model\n",
    "\n",
    "describe_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "sample_spacings = sample_spacings.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b20032745c34142b4c027b31c5aca51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.286826\tLR: 0.500000\n",
      "Loss: 3.551189\tLR: 0.500000\n",
      "Loss: 5.618439\tLR: 0.500000\n",
      "Loss: 3.360113\tLR: 0.500000\n",
      "Loss: 3.476582\tLR: 0.500000\n",
      "Loss: 2.366527\tLR: 0.450000\n",
      "Loss: 1.915956\tLR: 0.450000\n",
      "Loss: 1.969440\tLR: 0.450000\n",
      "Loss: 1.796858\tLR: 0.450000\n",
      "Loss: 1.840450\tLR: 0.450000\n",
      "Loss: 1.803444\tLR: 0.405000\n",
      "Loss: 1.675246\tLR: 0.405000\n",
      "Loss: 1.577672\tLR: 0.405000\n",
      "Loss: 1.529282\tLR: 0.405000\n",
      "Loss: 1.477986\tLR: 0.405000\n",
      "Loss: 1.581834\tLR: 0.364500\n",
      "Loss: 1.315247\tLR: 0.364500\n",
      "Loss: 1.345085\tLR: 0.364500\n",
      "Loss: 1.300409\tLR: 0.364500\n",
      "Loss: 1.340105\tLR: 0.364500\n",
      "Loss: 1.328799\tLR: 0.328050\n",
      "Loss: 1.138287\tLR: 0.328050\n",
      "Loss: 1.161520\tLR: 0.328050\n",
      "Loss: 1.210390\tLR: 0.328050\n",
      "Loss: 1.176165\tLR: 0.328050\n",
      "Loss: 1.152816\tLR: 0.295245\n",
      "Loss: 1.039268\tLR: 0.295245\n",
      "Loss: 1.030901\tLR: 0.295245\n",
      "Loss: 1.058415\tLR: 0.295245\n",
      "Loss: 1.032585\tLR: 0.295245\n",
      "Loss: 1.005308\tLR: 0.265721\n",
      "Loss: 0.917888\tLR: 0.265721\n",
      "Loss: 0.892791\tLR: 0.265721\n",
      "Loss: 0.905334\tLR: 0.265721\n",
      "Loss: 0.891857\tLR: 0.265721\n",
      "Loss: 0.905171\tLR: 0.239148\n",
      "Loss: 0.845704\tLR: 0.239148\n",
      "Loss: 0.836304\tLR: 0.239148\n",
      "Loss: 0.831451\tLR: 0.239148\n",
      "Loss: 0.843820\tLR: 0.239148\n",
      "Loss: 0.850482\tLR: 0.215234\n",
      "Loss: 0.829496\tLR: 0.215234\n",
      "Loss: 0.805963\tLR: 0.215234\n",
      "Loss: 0.795041\tLR: 0.215234\n",
      "Loss: 0.783814\tLR: 0.215234\n",
      "Loss: 0.776657\tLR: 0.193710\n",
      "Loss: 0.762175\tLR: 0.193710\n",
      "Loss: 0.756411\tLR: 0.193710\n",
      "Loss: 0.753086\tLR: 0.193710\n",
      "Loss: 0.751086\tLR: 0.193710\n",
      "Loss: 0.750685\tLR: 0.174339\n",
      "Loss: 0.750006\tLR: 0.174339\n",
      "Loss: 0.749481\tLR: 0.174339\n",
      "Loss: 0.749348\tLR: 0.174339\n",
      "Loss: 0.748933\tLR: 0.174339\n",
      "Loss: 0.749656\tLR: 0.156905\n",
      "Loss: 0.749256\tLR: 0.156905\n",
      "Loss: 0.749313\tLR: 0.156905\n",
      "Loss: 0.749217\tLR: 0.156905\n",
      "Loss: 0.749649\tLR: 0.156905\n",
      "Loss: 0.749586\tLR: 0.141215\n",
      "Loss: 0.749374\tLR: 0.141215\n",
      "Loss: 0.749227\tLR: 0.141215\n",
      "Loss: 0.749276\tLR: 0.141215\n",
      "Loss: 0.749200\tLR: 0.141215\n",
      "Loss: 0.749239\tLR: 0.127093\n",
      "Loss: 0.749129\tLR: 0.127093\n",
      "Loss: 0.749243\tLR: 0.127093\n",
      "Loss: 0.749694\tLR: 0.127093\n",
      "Loss: 0.749392\tLR: 0.127093\n",
      "Loss: 0.749418\tLR: 0.114384\n",
      "Loss: 0.749039\tLR: 0.114384\n",
      "Loss: 0.749181\tLR: 0.114384\n",
      "Loss: 0.749315\tLR: 0.114384\n",
      "Loss: 0.749353\tLR: 0.114384\n",
      "Loss: 0.748980\tLR: 0.102946\n",
      "Loss: 0.749663\tLR: 0.102946\n",
      "Loss: 0.749459\tLR: 0.102946\n",
      "Loss: 0.749344\tLR: 0.102946\n",
      "Loss: 0.749494\tLR: 0.102946\n",
      "Loss: 0.749132\tLR: 0.092651\n",
      "Loss: 0.749489\tLR: 0.092651\n",
      "Loss: 0.749285\tLR: 0.092651\n",
      "Loss: 0.749381\tLR: 0.092651\n",
      "Loss: 0.749324\tLR: 0.092651\n",
      "Loss: 0.749343\tLR: 0.083386\n",
      "Loss: 0.749344\tLR: 0.083386\n",
      "Loss: 0.749651\tLR: 0.083386\n",
      "Loss: 0.749254\tLR: 0.083386\n",
      "Loss: 0.749355\tLR: 0.083386\n",
      "Loss: 0.749379\tLR: 0.075047\n",
      "Loss: 0.749402\tLR: 0.075047\n",
      "Loss: 0.749210\tLR: 0.075047\n",
      "Loss: 0.749299\tLR: 0.075047\n",
      "Loss: 0.749177\tLR: 0.075047\n",
      "Loss: 0.749160\tLR: 0.067543\n",
      "Loss: 0.749178\tLR: 0.067543\n",
      "Loss: 0.749290\tLR: 0.067543\n",
      "Loss: 0.749059\tLR: 0.067543\n",
      "Loss: 0.749407\tLR: 0.067543\n",
      "Loss: 0.749113\tLR: 0.060788\n",
      "Loss: 0.749249\tLR: 0.060788\n",
      "Loss: 0.749033\tLR: 0.060788\n",
      "Loss: 0.749727\tLR: 0.060788\n",
      "Loss: 0.749478\tLR: 0.060788\n",
      "Loss: 0.749460\tLR: 0.054709\n",
      "Loss: 0.749386\tLR: 0.054709\n",
      "Loss: 0.749574\tLR: 0.054709\n",
      "Loss: 0.749144\tLR: 0.054709\n",
      "Loss: 0.749024\tLR: 0.054709\n",
      "Loss: 0.749218\tLR: 0.049239\n",
      "Loss: 0.749340\tLR: 0.049239\n",
      "Loss: 0.749210\tLR: 0.049239\n",
      "Loss: 0.749317\tLR: 0.049239\n",
      "Loss: 0.749501\tLR: 0.049239\n",
      "Loss: 0.749102\tLR: 0.044315\n",
      "Loss: 0.749360\tLR: 0.044315\n",
      "Loss: 0.749404\tLR: 0.044315\n",
      "Loss: 0.749043\tLR: 0.044315\n",
      "Loss: 0.749359\tLR: 0.044315\n",
      "Loss: 0.749229\tLR: 0.039883\n",
      "Loss: 0.749317\tLR: 0.039883\n",
      "Loss: 0.749259\tLR: 0.039883\n",
      "Loss: 0.749107\tLR: 0.039883\n",
      "Loss: 0.749454\tLR: 0.039883\n",
      "Loss: 0.749794\tLR: 0.035895\n",
      "Loss: 0.749224\tLR: 0.035895\n",
      "Loss: 0.749128\tLR: 0.035895\n",
      "Loss: 0.749606\tLR: 0.035895\n",
      "Loss: 0.749363\tLR: 0.035895\n",
      "Loss: 0.749663\tLR: 0.032305\n",
      "Loss: 0.749407\tLR: 0.032305\n",
      "Loss: 0.749678\tLR: 0.032305\n",
      "Loss: 0.749177\tLR: 0.032305\n",
      "Loss: 0.749251\tLR: 0.032305\n",
      "Loss: 0.749418\tLR: 0.029075\n",
      "Loss: 0.749401\tLR: 0.029075\n",
      "Loss: 0.749204\tLR: 0.029075\n",
      "Loss: 0.749090\tLR: 0.029075\n",
      "Loss: 0.749332\tLR: 0.029075\n",
      "Loss: 0.748977\tLR: 0.026167\n",
      "Loss: 0.749006\tLR: 0.026167\n",
      "Loss: 0.749079\tLR: 0.026167\n",
      "Loss: 0.749267\tLR: 0.026167\n",
      "Loss: 0.749273\tLR: 0.026167\n",
      "Loss: 0.749228\tLR: 0.023551\n",
      "Loss: 0.749236\tLR: 0.023551\n",
      "Loss: 0.749070\tLR: 0.023551\n",
      "Loss: 0.749172\tLR: 0.023551\n",
      "Loss: 0.748868\tLR: 0.023551\n",
      "Loss: 0.749507\tLR: 0.021196\n",
      "Loss: 0.749228\tLR: 0.021196\n",
      "Loss: 0.749165\tLR: 0.021196\n",
      "Loss: 0.749337\tLR: 0.021196\n",
      "Loss: 0.748768\tLR: 0.021196\n",
      "Loss: 0.749190\tLR: 0.019076\n",
      "Loss: 0.749159\tLR: 0.019076\n",
      "Loss: 0.749286\tLR: 0.019076\n",
      "Loss: 0.749318\tLR: 0.019076\n",
      "Loss: 0.749565\tLR: 0.019076\n",
      "Loss: 0.749179\tLR: 0.017168\n",
      "Loss: 0.749656\tLR: 0.017168\n",
      "Loss: 0.749787\tLR: 0.017168\n",
      "Loss: 0.749396\tLR: 0.017168\n",
      "Loss: 0.749107\tLR: 0.017168\n",
      "Loss: 0.749070\tLR: 0.015452\n",
      "Loss: 0.749491\tLR: 0.015452\n",
      "Loss: 0.749270\tLR: 0.015452\n",
      "Loss: 0.749086\tLR: 0.015452\n",
      "Loss: 0.749210\tLR: 0.015452\n",
      "Loss: 0.749572\tLR: 0.013906\n",
      "Loss: 0.749425\tLR: 0.013906\n",
      "Loss: 0.749389\tLR: 0.013906\n",
      "Loss: 0.749452\tLR: 0.013906\n",
      "Loss: 0.749400\tLR: 0.013906\n",
      "Loss: 0.749269\tLR: 0.012516\n",
      "Loss: 0.749040\tLR: 0.012516\n",
      "Loss: 0.749028\tLR: 0.012516\n",
      "Loss: 0.749222\tLR: 0.012516\n",
      "Loss: 0.749059\tLR: 0.012516\n",
      "Loss: 0.749362\tLR: 0.011264\n",
      "Loss: 0.749390\tLR: 0.011264\n",
      "Loss: 0.749266\tLR: 0.011264\n",
      "Loss: 0.749132\tLR: 0.011264\n",
      "Loss: 0.749103\tLR: 0.011264\n",
      "Loss: 0.749210\tLR: 0.010138\n",
      "Loss: 0.749237\tLR: 0.010138\n",
      "Loss: 0.749315\tLR: 0.010138\n",
      "Loss: 0.749563\tLR: 0.010138\n",
      "Loss: 0.749221\tLR: 0.010138\n",
      "Loss: 0.748880\tLR: 0.009124\n",
      "Loss: 0.749371\tLR: 0.009124\n",
      "Loss: 0.749782\tLR: 0.009124\n",
      "Loss: 0.749005\tLR: 0.009124\n",
      "Loss: 0.749403\tLR: 0.009124\n",
      "Loss: 0.749197\tLR: 0.008212\n",
      "Loss: 0.749614\tLR: 0.008212\n",
      "Loss: 0.748914\tLR: 0.008212\n",
      "Loss: 0.749281\tLR: 0.008212\n",
      "Loss: 0.749263\tLR: 0.008212\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch, sample_spacings)\n",
    "    print(f\"Loss: {output[1]:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    output[1].backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "coords_h = torch.arange(4)\n",
    "coords_w = torch.arange(4)\n",
    "coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "coords_flatten = torch.flatten(coords, 1)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += 4 - 1\n",
    "relative_coords[:, :, 1] += 4 - 1\n",
    "relative_coords[:, :, 0] *= 2 * 4 - 1\n",
    "relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "window_size = (4, 4, 4)\n",
    "relative_limits = (7, 7, 7)\n",
    "\n",
    "coords = get_coords_grid(window_size)\n",
    "coords_flatten = rearrange(coords, \"three_dimensional d h w -> three_dimensional (d h w)\", three_dimensional=3)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += window_size[0] - 1\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "relative_coords[:, :, 2] += window_size[2] - 1\n",
    "relative_position_index: torch.Tensor = (\n",
    "    relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 2]\n",
    ")\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "window_size = (2, 2, 2)\n",
    "relative_limits = (2 * window_size[0] - 1, 2 * window_size[1] - 1, 2 * window_size[2] - 1)\n",
    "\n",
    "relative_coords_table = get_coords_grid(relative_limits)\n",
    "relative_coords_table[0] -= window_size[0] - 1\n",
    "relative_coords_table[1] -= window_size[1] - 1\n",
    "relative_coords_table[2] -= window_size[2] - 1\n",
    "relative_coords_table = relative_coords_table.permute(1, 2, 3, 0).contiguous()\n",
    "relative_coords_table[0, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
