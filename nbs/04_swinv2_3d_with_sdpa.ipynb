{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp swinv2_3d_with_sdpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from vision_architectures.swinv2_3d import (\n",
    "    populate_and_validate_config,\n",
    "    get_coords_grid,\n",
    "    SwinV23DMHSA as SwinV23DMHSAWithoutSDPA,\n",
    "    SwinV23DLayerMLP,\n",
    "    SwinV23DLayer as SwinV23DLayerWithoutSDPA,\n",
    "    SwinV23DBlock as SwinV23DBlockWithoutSDPA,\n",
    "    SwinV23DPatchMerging,\n",
    "    SwinV23DStage as SwinV23DStageWithoutSDPA,\n",
    "    SwinV23DEncoder as SwinV23DEncoderWithoutSDPA,\n",
    "    SwinV23DPatchEmbeddings,\n",
    "    get_3d_position_embeddings,\n",
    "    embed_spacings_in_position_embeddings,\n",
    "    SwinV23DEmbeddings,\n",
    "    SwinV23DModel as SwinV23DModelWithoutSDPA,\n",
    "    SwinV23DMIMDecoder,\n",
    "    SwinV23DMIM as SwinV23DMIMWithoutSDPA,\n",
    "    SwinV23DSimMIM as SwinV23DSimMIMWithoutSDPA,\n",
    "    SwinV23DVAEMIM as SwinV23DVAEMIMWithoutSDPA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DMHSA(SwinV23DMHSAWithoutSDPA):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        use_relative_position_bias,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__(dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob)\n",
    "\n",
    "        # Remove attention dropout layer as that is handled automatically, but store the dropout for later\n",
    "        del self.attn_drop\n",
    "        self.attn_drop_prob = attn_drop_prob\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        # hidden_states: (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "        _, num_patches_z, num_patches_y, num_patches_x, _ = hidden_states.shape\n",
    "\n",
    "        query, key, value = rearrange(\n",
    "            self.W_qkv(hidden_states),\n",
    "            \"b nz ny nx (n num_heads d) -> n b num_heads (nz ny nx) d\",\n",
    "            n=3,\n",
    "            num_heads=self.num_heads,\n",
    "        )\n",
    "        # num_patches = window_size_z * window_size_y * window_size_x\n",
    "        # Each is (windowed_b, num_heads, num_patches, per_head_dim)\n",
    "\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=np.log(1.0 / 0.01)).exp()\n",
    "\n",
    "        query_normalized = F.normalize(query, dim=-1)\n",
    "        key_normalized = F.normalize(key, dim=-1)\n",
    "\n",
    "        query_normalized_and_scaled = query_normalized * logit_scale  # Scale the query beforehand\n",
    "\n",
    "        relative_position_bias = None\n",
    "        if self.use_relative_position_bias:\n",
    "            relative_position_bias = self.calculate_relative_position_bias()\n",
    "\n",
    "        context = F.scaled_dot_product_attention(\n",
    "            query_normalized_and_scaled,\n",
    "            key_normalized,\n",
    "            value,\n",
    "            attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias\n",
    "            dropout_p=self.attn_drop_prob,\n",
    "            is_causal=False,\n",
    "            scale=1.0,  # Already scaled the vectors\n",
    "        )\n",
    "        # (windowed_b, num_heads, num_patches, per_head_dim)\n",
    "        \n",
    "        context = rearrange(\n",
    "            context,\n",
    "            \"b num_heads (num_patches_z num_patches_y num_patches_x) d -> \"\n",
    "            \"b num_patches_z num_patches_y num_patches_x (num_heads d)\",\n",
    "            num_patches_z=num_patches_z,\n",
    "            num_patches_y=num_patches_y,\n",
    "            num_patches_x=num_patches_x,\n",
    "        )\n",
    "        # (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "\n",
    "        context = self.proj(context)\n",
    "        context = self.proj_drop(context)\n",
    "        # (windowed_b, window_size_z window_size_y window_size_x, dim)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mSwinV23DMHSA\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_qkv\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m162\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m54\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mproj_drop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mcpb_mlp\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[33minplace\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m6\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m54\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = SwinV23DMHSA(54, 6, (4, 4, 4), True)\n",
    "display(test)\n",
    "display(test(torch.randn(2, 4, 4, 4, 54)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for use_relative_position_bias in [True, False]:\n",
    "    m1 = SwinV23DMHSA(54, 6, (4, 4, 4), use_relative_position_bias)\n",
    "    m2 = SwinV23DMHSAWithoutSDPA(54, 6, (4, 4, 4), use_relative_position_bias)\n",
    "\n",
    "    m1.load_state_dict(m2.state_dict())\n",
    "    m1.eval(), m2.eval()\n",
    "\n",
    "    example_input = torch.randn(2, 4, 4, 4, 54)\n",
    "    o1 = m1(example_input)\n",
    "    o2 = m2(example_input)\n",
    "\n",
    "    assert torch.allclose(o1, o2, atol=1e-6), (o1 - o2).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify other classes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DLayer(SwinV23DLayerWithoutSDPA):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        intermediate_ratio,\n",
    "        layer_norm_eps,\n",
    "        window_size,\n",
    "        use_relative_position_bias,\n",
    "        attn_drop_prob=0.0,\n",
    "        proj_drop_prob=0.0,\n",
    "        mlp_drop_prob=0.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim,\n",
    "            num_heads,\n",
    "            intermediate_ratio,\n",
    "            layer_norm_eps,\n",
    "            window_size,\n",
    "            use_relative_position_bias,\n",
    "            attn_drop_prob,\n",
    "            proj_drop_prob,\n",
    "            mlp_drop_prob,\n",
    "        )\n",
    "\n",
    "        self.mhsa = SwinV23DMHSA(\n",
    "            dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DBlock(SwinV23DBlockWithoutSDPA):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__(stage_config)\n",
    "\n",
    "        self.stage_config = stage_config\n",
    "        self.w_layer = SwinV23DLayer(\n",
    "            stage_config[\"_out_dim\"],\n",
    "            stage_config[\"num_heads\"],\n",
    "            stage_config[\"intermediate_ratio\"],\n",
    "            stage_config[\"layer_norm_eps\"],\n",
    "            stage_config[\"window_size\"],\n",
    "            stage_config[\"use_relative_position_bias\"],\n",
    "            stage_config.get(\"attn_drop_prob\", 0.0),\n",
    "            stage_config.get(\"proj_drop_prob\", 0.0),\n",
    "            stage_config.get(\"mlp_drop_prob\", 0.0),\n",
    "        )\n",
    "        self.sw_layer = SwinV23DLayer(\n",
    "            stage_config[\"_out_dim\"],\n",
    "            stage_config[\"num_heads\"],\n",
    "            stage_config[\"intermediate_ratio\"],\n",
    "            stage_config[\"layer_norm_eps\"],\n",
    "            stage_config[\"window_size\"],\n",
    "            stage_config[\"use_relative_position_bias\"],\n",
    "            stage_config.get(\"attn_drop_prob\", 0.0),\n",
    "            stage_config.get(\"proj_drop_prob\", 0.0),\n",
    "            stage_config.get(\"mlp_drop_prob\", 0.0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DStage(SwinV23DStageWithoutSDPA):\n",
    "    def __init__(self, stage_config):\n",
    "        super().__init__(stage_config)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [SwinV23DBlock(stage_config) for _ in range(stage_config[\"depth\"])],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DEncoder(SwinV23DEncoderWithoutSDPA):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.stages = nn.ModuleList([SwinV23DStage(stage_config) for stage_config in config[\"stages\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DModel(SwinV23DModelWithoutSDPA):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.encoder = SwinV23DEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DMIM(SwinV23DMIMWithoutSDPA):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.swin = SwinV23DModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DSimMIM(SwinV23DSimMIMWithoutSDPA, SwinV23DMIM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)  # This calls all inits in order written above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SwinV23DVAEMIM(SwinV23DVAEMIMWithoutSDPA, SwinV23DMIM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)  # This calls all inits in order written above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "this is the new one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1183892\u001b[0m, \u001b[1;36m197632\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "sample_spacings = torch.tensor([[1, 0.1, 0.1], [2, 0.2, 0.2], [3, 0.3, 0.3], [4, 0.4, 0.4], [5, 0.5, 0.5]])\n",
    "sample_batch = torch.rand(3, 1, 16, 128, 128)\n",
    "sample_config = populate_and_validate_config(\n",
    "    {\n",
    "        \"patch_size\": (1, 4, 4),\n",
    "        \"dim\": 12,\n",
    "        \"in_channels\": 1,\n",
    "        \"use_absolute_position_embeddings\": True,\n",
    "        \"learnable_absolute_position_embeddings\": False,\n",
    "        \"embed_spacing_info\": False,\n",
    "        \"image_size\": (16, 128, 128),\n",
    "        \"drop_prob\": 0.2,\n",
    "        \"stages\": [\n",
    "            {\n",
    "                \"patch_merging\": None,\n",
    "                \"depth\": 1,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "                \"attn_drop_prob\": 0.2,\n",
    "                \"proj_drop_prob\": 0.2,\n",
    "                \"mlp_drop_prob\": 0.2,\n",
    "            },\n",
    "            {\n",
    "                \"patch_merging\": {\n",
    "                    \"merge_window_size\": (2, 2, 2),\n",
    "                    \"out_dim_ratio\": 4,\n",
    "                },\n",
    "                \"depth\": 3,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "            },\n",
    "            {\n",
    "                \"patch_merging\": {\n",
    "                    \"merge_window_size\": (2, 2, 2),\n",
    "                    \"out_dim_ratio\": 4,\n",
    "                },\n",
    "                \"depth\": 1,\n",
    "                \"num_heads\": 4,\n",
    "                \"intermediate_ratio\": 4,\n",
    "                \"layer_norm_eps\": 1e-6,\n",
    "                \"window_size\": (4, 4, 4),\n",
    "                \"use_relative_position_bias\": True,\n",
    "            },\n",
    "        ],\n",
    "        \"mim\": {\n",
    "            \"mask_ratio\": 0.7,\n",
    "            \"mask_grid_size\": (8, 8, 8),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "model = SwinV23DSimMIM(sample_config)\n",
    "\n",
    "sum(x.numel() for x in model.swin.parameters()), sum(x.numel() for x in model.decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1,381,536\n",
      "+---------------------------------------------------------------+------------+\n",
      "|                             Module                            | Parameters |\n",
      "+---------------------------------------------------------------+------------+\n",
      "|                           mask_token                          |     12     |\n",
      "|    swin.embeddings.patch_embeddings.patch_embeddings.weight   |    192     |\n",
      "|     swin.embeddings.patch_embeddings.patch_embeddings.bias    |     12     |\n",
      "|               swin.embeddings.layer_norm.weight               |     12     |\n",
      "|                swin.embeddings.layer_norm.bias                |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.W_qkv.weight   |    432     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mhsa.W_qkv.bias    |     36     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mhsa.proj.weight    |    144     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mhsa.proj.bias     |     12     |\n",
      "|  swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.0.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.layernorm1.weight   |     12     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.layernorm1.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mlp.dense1.weight   |    576     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mlp.dense1.bias    |     48     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.mlp.dense2.weight   |    576     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.mlp.dense2.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.w_layer.layernorm2.weight   |     12     |\n",
      "|     swin.encoder.stages.0.blocks.0.w_layer.layernorm2.bias    |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mhsa.W_qkv.weight   |    432     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.W_qkv.bias    |     36     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mhsa.proj.weight   |    144     |\n",
      "|     swin.encoder.stages.0.blocks.0.sw_layer.mhsa.proj.bias    |     12     |\n",
      "| swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.0.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.layernorm1.weight   |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.layernorm1.bias    |     12     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense1.weight   |    576     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense1.bias    |     48     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense2.weight   |    576     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.mlp.dense2.bias    |     12     |\n",
      "|   swin.encoder.stages.0.blocks.0.sw_layer.layernorm2.weight   |     12     |\n",
      "|    swin.encoder.stages.0.blocks.0.sw_layer.layernorm2.bias    |     12     |\n",
      "|     swin.encoder.stages.1.patch_merging.layer_norm.weight     |     96     |\n",
      "|      swin.encoder.stages.1.patch_merging.layer_norm.bias      |     96     |\n",
      "|        swin.encoder.stages.1.patch_merging.proj.weight        |   4,608    |\n",
      "|         swin.encoder.stages.1.patch_merging.proj.bias         |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.0.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.0.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.0.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.0.sw_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.1.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.1.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.1.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.1.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.1.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.1.sw_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mhsa.proj.weight    |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mhsa.proj.bias     |     48     |\n",
      "|  swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.1.blocks.2.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.layernorm1.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.layernorm1.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mlp.dense1.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mlp.dense1.bias    |    192     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.mlp.dense2.weight   |   9,216    |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.mlp.dense2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.w_layer.layernorm2.weight   |     48     |\n",
      "|     swin.encoder.stages.1.blocks.2.w_layer.layernorm2.bias    |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mhsa.W_qkv.weight   |   6,912    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.W_qkv.bias    |    144     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mhsa.proj.weight   |   2,304    |\n",
      "|     swin.encoder.stages.1.blocks.2.sw_layer.mhsa.proj.bias    |     48     |\n",
      "| swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.1.blocks.2.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.layernorm1.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.layernorm1.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense1.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense1.bias    |    192     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense2.weight   |   9,216    |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.mlp.dense2.bias    |     48     |\n",
      "|   swin.encoder.stages.1.blocks.2.sw_layer.layernorm2.weight   |     48     |\n",
      "|    swin.encoder.stages.1.blocks.2.sw_layer.layernorm2.bias    |     48     |\n",
      "|     swin.encoder.stages.2.patch_merging.layer_norm.weight     |    384     |\n",
      "|      swin.encoder.stages.2.patch_merging.layer_norm.bias      |    384     |\n",
      "|        swin.encoder.stages.2.patch_merging.proj.weight        |   73,728   |\n",
      "|         swin.encoder.stages.2.patch_merging.proj.bias         |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.logit_scale    |     4      |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.W_qkv.weight   |  110,592   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mhsa.W_qkv.bias    |    576     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mhsa.proj.weight    |   36,864   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mhsa.proj.bias     |    192     |\n",
      "|  swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|   swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "|  swin.encoder.stages.2.blocks.0.w_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.layernorm1.weight   |    192     |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.layernorm1.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mlp.dense1.weight   |  147,456   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mlp.dense1.bias    |    768     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.mlp.dense2.weight   |  147,456   |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.mlp.dense2.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.w_layer.layernorm2.weight   |    192     |\n",
      "|     swin.encoder.stages.2.blocks.0.w_layer.layernorm2.bias    |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.logit_scale   |     4      |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mhsa.W_qkv.weight   |  110,592   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.W_qkv.bias    |    576     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mhsa.proj.weight   |   36,864   |\n",
      "|     swin.encoder.stages.2.blocks.0.sw_layer.mhsa.proj.bias    |    192     |\n",
      "| swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.0.weight |   1,536    |\n",
      "|  swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.0.bias  |    512     |\n",
      "| swin.encoder.stages.2.blocks.0.sw_layer.mhsa.cpb_mlp.2.weight |   2,048    |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.layernorm1.weight   |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.layernorm1.bias    |    192     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense1.weight   |  147,456   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense1.bias    |    768     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense2.weight   |  147,456   |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.mlp.dense2.bias    |    192     |\n",
      "|   swin.encoder.stages.2.blocks.0.sw_layer.layernorm2.weight   |    192     |\n",
      "|    swin.encoder.stages.2.blocks.0.sw_layer.layernorm2.bias    |    192     |\n",
      "|                     decoder.decoder.weight                    |  196,608   |\n",
      "|                      decoder.decoder.bias                     |   1,024    |\n",
      "+---------------------------------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from neuro_utils.describe import describe_model\n",
    "\n",
    "describe_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "sample_spacings = sample_spacings.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276d1d1638d945f1931f8f60b4d2ea78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.083878\tLR: 0.500000\n",
      "Loss: 3.319292\tLR: 0.500000\n",
      "Loss: 5.392017\tLR: 0.500000\n",
      "Loss: 4.929338\tLR: 0.500000\n",
      "Loss: 3.861884\tLR: 0.500000\n",
      "Loss: 3.151842\tLR: 0.450000\n",
      "Loss: 2.418938\tLR: 0.450000\n",
      "Loss: 2.444153\tLR: 0.450000\n",
      "Loss: 2.341598\tLR: 0.450000\n",
      "Loss: 2.043196\tLR: 0.450000\n",
      "Loss: 1.876006\tLR: 0.405000\n",
      "Loss: 1.658004\tLR: 0.405000\n",
      "Loss: 1.518335\tLR: 0.405000\n",
      "Loss: 1.575046\tLR: 0.405000\n",
      "Loss: 1.500601\tLR: 0.405000\n",
      "Loss: 1.505771\tLR: 0.364500\n",
      "Loss: 1.416240\tLR: 0.364500\n",
      "Loss: 1.290093\tLR: 0.364500\n",
      "Loss: 1.305964\tLR: 0.364500\n",
      "Loss: 1.200005\tLR: 0.364500\n",
      "Loss: 1.412019\tLR: 0.328050\n",
      "Loss: 1.140101\tLR: 0.328050\n",
      "Loss: 1.242553\tLR: 0.328050\n",
      "Loss: 1.129826\tLR: 0.328050\n",
      "Loss: 1.184893\tLR: 0.328050\n",
      "Loss: 1.221602\tLR: 0.295245\n",
      "Loss: 1.139555\tLR: 0.295245\n",
      "Loss: 1.119853\tLR: 0.295245\n",
      "Loss: 1.129400\tLR: 0.295245\n",
      "Loss: 1.026167\tLR: 0.295245\n",
      "Loss: 1.012522\tLR: 0.265721\n",
      "Loss: 0.907528\tLR: 0.265721\n",
      "Loss: 0.928666\tLR: 0.265721\n",
      "Loss: 0.991159\tLR: 0.265721\n",
      "Loss: 1.017249\tLR: 0.265721\n",
      "Loss: 1.009063\tLR: 0.239148\n",
      "Loss: 0.868451\tLR: 0.239148\n",
      "Loss: 0.836559\tLR: 0.239148\n",
      "Loss: 0.827999\tLR: 0.239148\n",
      "Loss: 0.831820\tLR: 0.239148\n",
      "Loss: 0.845520\tLR: 0.215234\n",
      "Loss: 0.820772\tLR: 0.215234\n",
      "Loss: 0.809016\tLR: 0.215234\n",
      "Loss: 0.797011\tLR: 0.215234\n",
      "Loss: 0.792685\tLR: 0.215234\n",
      "Loss: 0.785076\tLR: 0.193710\n",
      "Loss: 0.771678\tLR: 0.193710\n",
      "Loss: 0.763738\tLR: 0.193710\n",
      "Loss: 0.760088\tLR: 0.193710\n",
      "Loss: 0.757464\tLR: 0.193710\n",
      "Loss: 0.756654\tLR: 0.174339\n",
      "Loss: 0.754990\tLR: 0.174339\n",
      "Loss: 0.754018\tLR: 0.174339\n",
      "Loss: 0.754244\tLR: 0.174339\n",
      "Loss: 0.754011\tLR: 0.174339\n",
      "Loss: 0.753900\tLR: 0.156905\n",
      "Loss: 0.753909\tLR: 0.156905\n",
      "Loss: 0.753926\tLR: 0.156905\n",
      "Loss: 0.753322\tLR: 0.156905\n",
      "Loss: 0.753330\tLR: 0.156905\n",
      "Loss: 0.753576\tLR: 0.141215\n",
      "Loss: 0.753422\tLR: 0.141215\n",
      "Loss: 0.753124\tLR: 0.141215\n",
      "Loss: 0.753330\tLR: 0.141215\n",
      "Loss: 0.753331\tLR: 0.141215\n",
      "Loss: 0.753199\tLR: 0.127093\n",
      "Loss: 0.752880\tLR: 0.127093\n",
      "Loss: 0.752886\tLR: 0.127093\n",
      "Loss: 0.752515\tLR: 0.127093\n",
      "Loss: 0.752979\tLR: 0.127093\n",
      "Loss: 0.752849\tLR: 0.114384\n",
      "Loss: 0.752697\tLR: 0.114384\n",
      "Loss: 0.752993\tLR: 0.114384\n",
      "Loss: 0.752903\tLR: 0.114384\n",
      "Loss: 0.752665\tLR: 0.114384\n",
      "Loss: 0.752499\tLR: 0.102946\n",
      "Loss: 0.752744\tLR: 0.102946\n",
      "Loss: 0.752960\tLR: 0.102946\n",
      "Loss: 0.752644\tLR: 0.102946\n",
      "Loss: 0.752615\tLR: 0.102946\n",
      "Loss: 0.752613\tLR: 0.092651\n",
      "Loss: 0.752724\tLR: 0.092651\n",
      "Loss: 0.752412\tLR: 0.092651\n",
      "Loss: 0.752602\tLR: 0.092651\n",
      "Loss: 0.752387\tLR: 0.092651\n",
      "Loss: 0.752038\tLR: 0.083386\n",
      "Loss: 0.752370\tLR: 0.083386\n",
      "Loss: 0.752203\tLR: 0.083386\n",
      "Loss: 0.752425\tLR: 0.083386\n",
      "Loss: 0.752335\tLR: 0.083386\n",
      "Loss: 0.751981\tLR: 0.075047\n",
      "Loss: 0.752244\tLR: 0.075047\n",
      "Loss: 0.752165\tLR: 0.075047\n",
      "Loss: 0.752334\tLR: 0.075047\n",
      "Loss: 0.752033\tLR: 0.075047\n",
      "Loss: 0.752414\tLR: 0.067543\n",
      "Loss: 0.752383\tLR: 0.067543\n",
      "Loss: 0.751925\tLR: 0.067543\n",
      "Loss: 0.751853\tLR: 0.067543\n",
      "Loss: 0.752013\tLR: 0.067543\n",
      "Loss: 0.752233\tLR: 0.060788\n",
      "Loss: 0.752558\tLR: 0.060788\n",
      "Loss: 0.752178\tLR: 0.060788\n",
      "Loss: 0.752298\tLR: 0.060788\n",
      "Loss: 0.752079\tLR: 0.060788\n",
      "Loss: 0.752188\tLR: 0.054709\n",
      "Loss: 0.752416\tLR: 0.054709\n",
      "Loss: 0.752287\tLR: 0.054709\n",
      "Loss: 0.752038\tLR: 0.054709\n",
      "Loss: 0.752033\tLR: 0.054709\n",
      "Loss: 0.752092\tLR: 0.049239\n",
      "Loss: 0.752014\tLR: 0.049239\n",
      "Loss: 0.751980\tLR: 0.049239\n",
      "Loss: 0.751752\tLR: 0.049239\n",
      "Loss: 0.751617\tLR: 0.049239\n",
      "Loss: 0.752063\tLR: 0.044315\n",
      "Loss: 0.751952\tLR: 0.044315\n",
      "Loss: 0.752444\tLR: 0.044315\n",
      "Loss: 0.752180\tLR: 0.044315\n",
      "Loss: 0.751451\tLR: 0.044315\n",
      "Loss: 0.752239\tLR: 0.039883\n",
      "Loss: 0.751810\tLR: 0.039883\n",
      "Loss: 0.751917\tLR: 0.039883\n",
      "Loss: 0.751965\tLR: 0.039883\n",
      "Loss: 0.751851\tLR: 0.039883\n",
      "Loss: 0.751938\tLR: 0.035895\n",
      "Loss: 0.751967\tLR: 0.035895\n",
      "Loss: 0.752325\tLR: 0.035895\n",
      "Loss: 0.751838\tLR: 0.035895\n",
      "Loss: 0.751709\tLR: 0.035895\n",
      "Loss: 0.751897\tLR: 0.032305\n",
      "Loss: 0.752248\tLR: 0.032305\n",
      "Loss: 0.751860\tLR: 0.032305\n",
      "Loss: 0.751934\tLR: 0.032305\n",
      "Loss: 0.752225\tLR: 0.032305\n",
      "Loss: 0.752022\tLR: 0.029075\n",
      "Loss: 0.751808\tLR: 0.029075\n",
      "Loss: 0.751449\tLR: 0.029075\n",
      "Loss: 0.751603\tLR: 0.029075\n",
      "Loss: 0.751622\tLR: 0.029075\n",
      "Loss: 0.751922\tLR: 0.026167\n",
      "Loss: 0.751792\tLR: 0.026167\n",
      "Loss: 0.751484\tLR: 0.026167\n",
      "Loss: 0.752204\tLR: 0.026167\n",
      "Loss: 0.751878\tLR: 0.026167\n",
      "Loss: 0.751756\tLR: 0.023551\n",
      "Loss: 0.751711\tLR: 0.023551\n",
      "Loss: 0.751827\tLR: 0.023551\n",
      "Loss: 0.751824\tLR: 0.023551\n",
      "Loss: 0.751971\tLR: 0.023551\n",
      "Loss: 0.751673\tLR: 0.021196\n",
      "Loss: 0.751964\tLR: 0.021196\n",
      "Loss: 0.751393\tLR: 0.021196\n",
      "Loss: 0.751720\tLR: 0.021196\n",
      "Loss: 0.752004\tLR: 0.021196\n",
      "Loss: 0.751876\tLR: 0.019076\n",
      "Loss: 0.752283\tLR: 0.019076\n",
      "Loss: 0.751640\tLR: 0.019076\n",
      "Loss: 0.752059\tLR: 0.019076\n",
      "Loss: 0.751616\tLR: 0.019076\n",
      "Loss: 0.751906\tLR: 0.017168\n",
      "Loss: 0.751703\tLR: 0.017168\n",
      "Loss: 0.751897\tLR: 0.017168\n",
      "Loss: 0.751850\tLR: 0.017168\n",
      "Loss: 0.751430\tLR: 0.017168\n",
      "Loss: 0.751754\tLR: 0.015452\n",
      "Loss: 0.751589\tLR: 0.015452\n",
      "Loss: 0.751840\tLR: 0.015452\n",
      "Loss: 0.751663\tLR: 0.015452\n",
      "Loss: 0.751587\tLR: 0.015452\n",
      "Loss: 0.751962\tLR: 0.013906\n",
      "Loss: 0.751615\tLR: 0.013906\n",
      "Loss: 0.751433\tLR: 0.013906\n",
      "Loss: 0.751441\tLR: 0.013906\n",
      "Loss: 0.751842\tLR: 0.013906\n",
      "Loss: 0.751848\tLR: 0.012516\n",
      "Loss: 0.751543\tLR: 0.012516\n",
      "Loss: 0.751875\tLR: 0.012516\n",
      "Loss: 0.751915\tLR: 0.012516\n",
      "Loss: 0.751905\tLR: 0.012516\n",
      "Loss: 0.751606\tLR: 0.011264\n",
      "Loss: 0.751924\tLR: 0.011264\n",
      "Loss: 0.751556\tLR: 0.011264\n",
      "Loss: 0.751716\tLR: 0.011264\n",
      "Loss: 0.751513\tLR: 0.011264\n",
      "Loss: 0.751150\tLR: 0.010138\n",
      "Loss: 0.751912\tLR: 0.010138\n",
      "Loss: 0.751963\tLR: 0.010138\n",
      "Loss: 0.751618\tLR: 0.010138\n",
      "Loss: 0.751806\tLR: 0.010138\n",
      "Loss: 0.751858\tLR: 0.009124\n",
      "Loss: 0.751498\tLR: 0.009124\n",
      "Loss: 0.751357\tLR: 0.009124\n",
      "Loss: 0.751657\tLR: 0.009124\n",
      "Loss: 0.751421\tLR: 0.009124\n",
      "Loss: 0.751465\tLR: 0.008212\n",
      "Loss: 0.751691\tLR: 0.008212\n",
      "Loss: 0.751506\tLR: 0.008212\n",
      "Loss: 0.751861\tLR: 0.008212\n",
      "Loss: 0.751415\tLR: 0.008212\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch, sample_spacings)\n",
    "    print(f\"Loss: {output[1]:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    output[1].backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "this is the new one\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m1183892\u001b[0m, \u001b[1;36m197632\u001b[0m, \u001b[1;36m74112\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwinV23DVAEMIM(sample_config)\n",
    "\n",
    "encoder_params = sum(x.numel() for x in model.swin.parameters())\n",
    "decoder_params = sum(x.numel() for x in model.decoder.parameters())\n",
    "sampling_params = sum(x.numel() for x in model.mu_layer.parameters()) + sum(x.numel() for x in model.logvar_layer.parameters())\n",
    "encoder_params, decoder_params, sampling_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "sample_spacings = sample_spacings.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9157f5d28644f2b5f6d0a756a35726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 229.825439\tLR: 0.000100\n",
      "Loss: 266.294525\tLR: 0.000100\n",
      "Loss: 211.772339\tLR: 0.000100\n",
      "Loss: 128.313461\tLR: 0.000100\n",
      "Loss: 45.630985\tLR: 0.000100\n",
      "Loss: 29.337191\tLR: 0.000090\n",
      "Loss: 22.004816\tLR: 0.000090\n",
      "Loss: 17.929337\tLR: 0.000090\n",
      "Loss: 15.220288\tLR: 0.000090\n",
      "Loss: 13.296724\tLR: 0.000090\n",
      "Loss: 11.891582\tLR: 0.000081\n",
      "Loss: 10.755937\tLR: 0.000081\n",
      "Loss: 9.921865\tLR: 0.000081\n",
      "Loss: 9.178457\tLR: 0.000081\n",
      "Loss: 8.562808\tLR: 0.000081\n",
      "Loss: 8.041553\tLR: 0.000073\n",
      "Loss: 7.621122\tLR: 0.000073\n",
      "Loss: 7.297593\tLR: 0.000073\n",
      "Loss: 6.964215\tLR: 0.000073\n",
      "Loss: 6.675024\tLR: 0.000073\n",
      "Loss: 6.440987\tLR: 0.000066\n",
      "Loss: 6.235725\tLR: 0.000066\n",
      "Loss: 6.011176\tLR: 0.000066\n",
      "Loss: 5.872846\tLR: 0.000066\n",
      "Loss: 5.721246\tLR: 0.000066\n",
      "Loss: 5.515887\tLR: 0.000059\n",
      "Loss: 5.422292\tLR: 0.000059\n",
      "Loss: 5.314825\tLR: 0.000059\n",
      "Loss: 5.229799\tLR: 0.000059\n",
      "Loss: 5.096042\tLR: 0.000059\n",
      "Loss: 5.030997\tLR: 0.000053\n",
      "Loss: 4.951133\tLR: 0.000053\n",
      "Loss: 4.853930\tLR: 0.000053\n",
      "Loss: 4.755425\tLR: 0.000053\n",
      "Loss: 4.697513\tLR: 0.000053\n",
      "Loss: 4.655155\tLR: 0.000048\n",
      "Loss: 4.585632\tLR: 0.000048\n",
      "Loss: 4.543992\tLR: 0.000048\n",
      "Loss: 4.498172\tLR: 0.000048\n",
      "Loss: 4.398968\tLR: 0.000048\n",
      "Loss: 4.414746\tLR: 0.000043\n",
      "Loss: 4.331257\tLR: 0.000043\n",
      "Loss: 4.302761\tLR: 0.000043\n",
      "Loss: 4.258042\tLR: 0.000043\n",
      "Loss: 4.214169\tLR: 0.000043\n",
      "Loss: 4.222549\tLR: 0.000039\n",
      "Loss: 4.168661\tLR: 0.000039\n",
      "Loss: 4.151212\tLR: 0.000039\n",
      "Loss: 4.097654\tLR: 0.000039\n",
      "Loss: 4.083145\tLR: 0.000039\n",
      "Loss: 4.049427\tLR: 0.000035\n",
      "Loss: 4.003036\tLR: 0.000035\n",
      "Loss: 4.020994\tLR: 0.000035\n",
      "Loss: 4.001678\tLR: 0.000035\n",
      "Loss: 3.958652\tLR: 0.000035\n",
      "Loss: 3.948031\tLR: 0.000031\n",
      "Loss: 3.913857\tLR: 0.000031\n",
      "Loss: 3.900446\tLR: 0.000031\n",
      "Loss: 3.871281\tLR: 0.000031\n",
      "Loss: 3.882369\tLR: 0.000031\n",
      "Loss: 3.859942\tLR: 0.000028\n",
      "Loss: 3.818576\tLR: 0.000028\n",
      "Loss: 3.839758\tLR: 0.000028\n",
      "Loss: 3.798193\tLR: 0.000028\n",
      "Loss: 3.796963\tLR: 0.000028\n",
      "Loss: 3.771493\tLR: 0.000025\n",
      "Loss: 3.755910\tLR: 0.000025\n",
      "Loss: 3.723812\tLR: 0.000025\n",
      "Loss: 3.737274\tLR: 0.000025\n",
      "Loss: 3.747642\tLR: 0.000025\n",
      "Loss: 3.724979\tLR: 0.000023\n",
      "Loss: 3.731329\tLR: 0.000023\n",
      "Loss: 3.723603\tLR: 0.000023\n",
      "Loss: 3.677242\tLR: 0.000023\n",
      "Loss: 3.696579\tLR: 0.000023\n",
      "Loss: 3.676951\tLR: 0.000021\n",
      "Loss: 3.687791\tLR: 0.000021\n",
      "Loss: 3.655093\tLR: 0.000021\n",
      "Loss: 3.626177\tLR: 0.000021\n",
      "Loss: 3.625601\tLR: 0.000021\n",
      "Loss: 3.646304\tLR: 0.000019\n",
      "Loss: 3.620301\tLR: 0.000019\n",
      "Loss: 3.626426\tLR: 0.000019\n",
      "Loss: 3.578183\tLR: 0.000019\n",
      "Loss: 3.621894\tLR: 0.000019\n",
      "Loss: 3.576928\tLR: 0.000017\n",
      "Loss: 3.579092\tLR: 0.000017\n",
      "Loss: 3.576733\tLR: 0.000017\n",
      "Loss: 3.576123\tLR: 0.000017\n",
      "Loss: 3.587022\tLR: 0.000017\n",
      "Loss: 3.554913\tLR: 0.000015\n",
      "Loss: 3.591639\tLR: 0.000015\n",
      "Loss: 3.558323\tLR: 0.000015\n",
      "Loss: 3.537654\tLR: 0.000015\n",
      "Loss: 3.513718\tLR: 0.000015\n",
      "Loss: 3.535796\tLR: 0.000014\n",
      "Loss: 3.510078\tLR: 0.000014\n",
      "Loss: 3.523817\tLR: 0.000014\n",
      "Loss: 3.500267\tLR: 0.000014\n",
      "Loss: 3.501859\tLR: 0.000014\n",
      "Loss: 3.500761\tLR: 0.000012\n",
      "Loss: 3.516524\tLR: 0.000012\n",
      "Loss: 3.510353\tLR: 0.000012\n",
      "Loss: 3.477823\tLR: 0.000012\n",
      "Loss: 3.505070\tLR: 0.000012\n",
      "Loss: 3.484880\tLR: 0.000011\n",
      "Loss: 3.480259\tLR: 0.000011\n",
      "Loss: 3.506196\tLR: 0.000011\n",
      "Loss: 3.481089\tLR: 0.000011\n",
      "Loss: 3.472924\tLR: 0.000011\n",
      "Loss: 3.459353\tLR: 0.000010\n",
      "Loss: 3.492070\tLR: 0.000010\n",
      "Loss: 3.460005\tLR: 0.000010\n",
      "Loss: 3.467032\tLR: 0.000010\n",
      "Loss: 3.457976\tLR: 0.000010\n",
      "Loss: 3.459756\tLR: 0.000009\n",
      "Loss: 3.434072\tLR: 0.000009\n",
      "Loss: 3.465003\tLR: 0.000009\n",
      "Loss: 3.460945\tLR: 0.000009\n",
      "Loss: 3.462182\tLR: 0.000009\n",
      "Loss: 3.430357\tLR: 0.000008\n",
      "Loss: 3.441674\tLR: 0.000008\n",
      "Loss: 3.425543\tLR: 0.000008\n",
      "Loss: 3.454682\tLR: 0.000008\n",
      "Loss: 3.420530\tLR: 0.000008\n",
      "Loss: 3.439780\tLR: 0.000007\n",
      "Loss: 3.419329\tLR: 0.000007\n",
      "Loss: 3.418274\tLR: 0.000007\n",
      "Loss: 3.426663\tLR: 0.000007\n",
      "Loss: 3.416382\tLR: 0.000007\n",
      "Loss: 3.428823\tLR: 0.000006\n",
      "Loss: 3.431574\tLR: 0.000006\n",
      "Loss: 3.418932\tLR: 0.000006\n",
      "Loss: 3.400946\tLR: 0.000006\n",
      "Loss: 3.455234\tLR: 0.000006\n",
      "Loss: 3.409028\tLR: 0.000006\n",
      "Loss: 3.408782\tLR: 0.000006\n",
      "Loss: 3.399523\tLR: 0.000006\n",
      "Loss: 3.413436\tLR: 0.000006\n",
      "Loss: 3.402602\tLR: 0.000006\n",
      "Loss: 3.429005\tLR: 0.000005\n",
      "Loss: 3.406008\tLR: 0.000005\n",
      "Loss: 3.413403\tLR: 0.000005\n",
      "Loss: 3.434261\tLR: 0.000005\n",
      "Loss: 3.382000\tLR: 0.000005\n",
      "Loss: 3.405024\tLR: 0.000005\n",
      "Loss: 3.411009\tLR: 0.000005\n",
      "Loss: 3.387883\tLR: 0.000005\n",
      "Loss: 3.403985\tLR: 0.000005\n",
      "Loss: 3.406252\tLR: 0.000005\n",
      "Loss: 3.393673\tLR: 0.000004\n",
      "Loss: 3.392995\tLR: 0.000004\n",
      "Loss: 3.358718\tLR: 0.000004\n",
      "Loss: 3.373202\tLR: 0.000004\n",
      "Loss: 3.367771\tLR: 0.000004\n",
      "Loss: 3.386777\tLR: 0.000004\n",
      "Loss: 3.397286\tLR: 0.000004\n",
      "Loss: 3.374347\tLR: 0.000004\n",
      "Loss: 3.384551\tLR: 0.000004\n",
      "Loss: 3.383759\tLR: 0.000004\n",
      "Loss: 3.386719\tLR: 0.000003\n",
      "Loss: 3.381050\tLR: 0.000003\n",
      "Loss: 3.407068\tLR: 0.000003\n",
      "Loss: 3.386394\tLR: 0.000003\n",
      "Loss: 3.364739\tLR: 0.000003\n",
      "Loss: 3.392733\tLR: 0.000003\n",
      "Loss: 3.377923\tLR: 0.000003\n",
      "Loss: 3.381244\tLR: 0.000003\n",
      "Loss: 3.362088\tLR: 0.000003\n",
      "Loss: 3.383790\tLR: 0.000003\n",
      "Loss: 3.378273\tLR: 0.000003\n",
      "Loss: 3.366207\tLR: 0.000003\n",
      "Loss: 3.376859\tLR: 0.000003\n",
      "Loss: 3.372305\tLR: 0.000003\n",
      "Loss: 3.366313\tLR: 0.000003\n",
      "Loss: 3.371725\tLR: 0.000003\n",
      "Loss: 3.346520\tLR: 0.000003\n",
      "Loss: 3.380338\tLR: 0.000003\n",
      "Loss: 3.367829\tLR: 0.000003\n",
      "Loss: 3.385921\tLR: 0.000003\n",
      "Loss: 3.383005\tLR: 0.000002\n",
      "Loss: 3.356741\tLR: 0.000002\n",
      "Loss: 3.379975\tLR: 0.000002\n",
      "Loss: 3.357671\tLR: 0.000002\n",
      "Loss: 3.370261\tLR: 0.000002\n",
      "Loss: 3.352137\tLR: 0.000002\n",
      "Loss: 3.393527\tLR: 0.000002\n",
      "Loss: 3.385445\tLR: 0.000002\n",
      "Loss: 3.349547\tLR: 0.000002\n",
      "Loss: 3.368403\tLR: 0.000002\n",
      "Loss: 3.362796\tLR: 0.000002\n",
      "Loss: 3.355630\tLR: 0.000002\n",
      "Loss: 3.342365\tLR: 0.000002\n",
      "Loss: 3.397518\tLR: 0.000002\n",
      "Loss: 3.328733\tLR: 0.000002\n",
      "Loss: 3.361254\tLR: 0.000002\n",
      "Loss: 3.348973\tLR: 0.000002\n",
      "Loss: 3.376067\tLR: 0.000002\n",
      "Loss: 3.341676\tLR: 0.000002\n",
      "Loss: 3.363222\tLR: 0.000002\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch, sample_spacings)\n",
    "    print(f\"Loss: {output[1]:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    # print(output[-1])\n",
    "    output[1].backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "coords_h = torch.arange(4)\n",
    "coords_w = torch.arange(4)\n",
    "coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "coords_flatten = torch.flatten(coords, 1)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += 4 - 1\n",
    "relative_coords[:, :, 1] += 4 - 1\n",
    "relative_coords[:, :, 0] *= 2 * 4 - 1\n",
    "relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "window_size = (4, 4, 4)\n",
    "relative_limits = (7, 7, 7)\n",
    "\n",
    "coords = get_coords_grid(window_size)\n",
    "coords_flatten = rearrange(coords, \"three_dimensional d h w -> three_dimensional (d h w)\", three_dimensional=3)\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords[:, :, 0] += window_size[0] - 1\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "relative_coords[:, :, 2] += window_size[2] - 1\n",
    "relative_position_index: torch.Tensor = (\n",
    "    relative_coords[:, :, 0] * relative_limits[1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 1] * relative_limits[2]\n",
    "    + relative_coords[:, :, 2]\n",
    ")\n",
    "\n",
    "relative_position_index.min(), relative_position_index.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "window_size = (2, 2, 2)\n",
    "relative_limits = (2 * window_size[0] - 1, 2 * window_size[1] - 1, 2 * window_size[2] - 1)\n",
    "\n",
    "relative_coords_table = get_coords_grid(relative_limits)\n",
    "relative_coords_table[0] -= window_size[0] - 1\n",
    "relative_coords_table[1] -= window_size[1] - 1\n",
    "relative_coords_table[2] -= window_size[2] - 1\n",
    "relative_coords_table = relative_coords_table.permute(1, 2, 3, 0).contiguous()\n",
    "relative_coords_table[0, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
