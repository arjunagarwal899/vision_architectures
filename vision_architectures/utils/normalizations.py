# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/utils/03_normalizations.ipynb.

# %% auto 0
__all__ = ['DyT', 'get_norm_layer']

# %% ../../nbs/utils/03_normalizations.ipynb 2
import torch
from torch import nn

# %% ../../nbs/utils/03_normalizations.ipynb 4
class DyT(nn.Module):
    # As proposed in Transformers without Normalization: https://arxiv.org/pdf/2503.10622

    def __init__(self, normalized_shape: int | list[int], alpha0: float = 0.5):
        super().__init__()

        if isinstance(normalized_shape, int):
            normalized_shape = (normalized_shape,)

        self.normalized_shape = normalized_shape
        self.alpha0 = alpha0

        self.alpha = nn.Parameter(torch.tensor([alpha0]))
        self.weight = nn.Parameter(torch.ones(normalized_shape, dtype=torch.float32))
        self.bias = nn.Parameter(torch.zeros(normalized_shape, dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = torch.tanh(self.alpha * x)
        x = x * self.weight + self.bias
        return x

    def extra_repr(self) -> str:
        return f"normalized_shape={self.normalized_shape}, alpha0={self.alpha0}"

# %% ../../nbs/utils/03_normalizations.ipynb 7
def get_norm_layer(normalization_name: str, *args, **kwargs):
    if normalization_name == "layernorm":
        norm_layer = nn.LayerNorm(*args, **kwargs)
    elif normalization_name == "batchnorm" or normalization_name == "batchnorm1d":
        norm_layer = nn.BatchNorm1d(*args, **kwargs)
    elif normalization_name == "batchnorm2d":
        norm_layer = nn.BatchNorm2d(*args, **kwargs)
    elif normalization_name == "batchnorm3d":
        norm_layer = nn.BatchNorm3d(*args, **kwargs)
    elif normalization_name == "dyt":
        norm_layer = DyT(*args, **kwargs)
    else:
        raise ValueError(f"Normalization {normalization_name} not implemented")

    return norm_layer
