# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/utils/10_pipeline_parallelism.ipynb.

# %% auto 0
__all__ = ['get_device', 'move_to_device', 'parallelize_module', 'paralellize_pipeline']

# %% ../../nbs/utils/10_pipeline_parallelism.ipynb 1
from collections import defaultdict

import torch
from torch import nn

# %% ../../nbs/utils/10_pipeline_parallelism.ipynb 3
def get_device(device: torch.device | str) -> torch.device:
    """Convert to torch.device object."""
    if isinstance(device, str):
        return torch.device(device)
    return device

# %% ../../nbs/utils/10_pipeline_parallelism.ipynb 4
def move_to_device(data, device):
    """
    Move data to the specified device.

    Args:
        data: The data to move.
        device: The device to move the data to.

    Returns:
        The data moved to the specified device.
    """
    device = get_device(device)
    if isinstance(data, (list, tuple, set)):
        return type(data)(move_to_device(d, device) for d in data)
    elif isinstance(data, dict):
        return {k: move_to_device(v, device) for k, v in data.items()}
    elif isinstance(data, torch.Tensor):
        return data.to(device)
    else:
        return data

# %% ../../nbs/utils/10_pipeline_parallelism.ipynb 7
def parallelize_module(
    module: nn.Module, processing_device: torch.device | str, output_device: torch.device | str | None = None
):
    # Get devices
    processing_device = get_device(processing_device)
    if output_device is None:
        output_device = processing_device
    output_device = get_device(output_device)

    # Move module to processing device
    module = module.to(processing_device)

    # Store module forward function without parallization
    module._forward_without_pipeline_parallelization = module.forward

    # Update with parallelized forward function
    def forward_with_pipeline_parallelization(*args, **kwargs):
        """Forward function that moves inputs to the processing device and outputs to the output device. To view the
        actual forward logic, use the `_forward_without_pipeline_parallelization` function."""
        args = move_to_device(args, processing_device)
        kwargs = move_to_device(kwargs, processing_device)
        output = module._forward_without_pipeline_parallelization(*args, **kwargs)
        return move_to_device(output, output_device)

    module.forward_with_pipeline_parallelization = forward_with_pipeline_parallelization
    module.forward = forward_with_pipeline_parallelization

    # Store module extra_repr function without parallization
    module._extra_repr_without_pipeline_parallelization = module.extra_repr

    # Update with extra_repr function
    def extra_repr_with_pipeline_parallelization(*args, **kwargs):
        """extra_repr function that also shows the processing device and the output device. To view the actual
        extra_repr function, use the `_extra_repr_without_pipeline_parallelization` function."""
        return (
            f"processing_device={processing_device}, output_device={output_device}"
            + module._extra_repr_without_pipeline_parallelization(*args, **kwargs)
        )

    module.extra_repr_with_pipeline_parallelization = extra_repr_with_pipeline_parallelization
    module.extra_repr = extra_repr_with_pipeline_parallelization

    # Add flag to module (although it's never used, it's useful for debugging)
    module._is_pipeline_parallelized = True

    return module

# %% ../../nbs/utils/10_pipeline_parallelism.ipynb 9
def paralellize_pipeline(
    model: nn.Module, module_to_device: dict[str, torch.device | str | list[torch.device | str]]
) -> nn.Module:
    """
    Parallelize a model across multiple devices.

    Args:
        model: The model to parallelize.
        module_to_device: A dictionary mapping module names to devices. Keys are modules names with nested modules
            separated by dots (e.g., "module.submodule"). Note that the parallelism is performed using Level Order
            Traversal (i.e. BFS) of the model. Therefore the device of the deepest module in the dictionary will be
            overwritten even if it's parent is also specified in the dictionary. Value is either a device or a 2-tuple
            of devices. The first device is the processing device, and the second device is the output device.

    Returns:
        The parallelized pipeline.
    """
    # Convert all dictionary values to lists
    for key, value in module_to_device.items():
        if not isinstance(value, list):
            module_to_device[key] = [value, value]

    children_submodules_to_device: dict[str, dict[str, torch.device | str]] = defaultdict(dict)
    for module_name in sorted(module_to_device.keys()):
        # Identify the devices
        devices = module_to_device[module_name]
        if isinstance(devices, list):
            processing_device, output_device = devices
        else:
            processing_device, output_device = devices, devices

        # Get the module's name and ensure it is present in the model
        module_name_split = module_name.split(".")
        child_name = module_name_split[0]
        if not hasattr(model, child_name):
            raise ValueError(f"Module {child_name} not found in the model.")

        if len(module_name_split) == 1:
            # If it is the module in question, parallelize it
            module = getattr(model, child_name)
            parallelize_module(module, processing_device, output_device)

            # This will always be the first element of the dictionary for every subtree, if at all. Update the
            # output_device of all the rest of they modules in the dictionary so that they are able to work together
            for key in module_to_device:
                if key.startswith(child_name + "."):
                    module_to_device[key][1] = processing_device
        else:
            # Add to the submodules_to_device
            children_submodules_to_device[child_name][".".join(module_name_split[1:])] = [
                processing_device,
                output_device,
            ]

    # Iterate over the submodules and assign them to the appropriate devices
    for child_name, submodules_to_device in children_submodules_to_device.items():
        module = getattr(model, child_name)
        paralellize_pipeline(module, submodules_to_device)

    return model
