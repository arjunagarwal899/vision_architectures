# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/utils/08_timesteps.ipynb.

# %% auto 0
__all__ = ['TimestepSampler']

# %% ../../nbs/utils/08_timesteps.ipynb 2
from typing import Literal

import torch
from torch import nn

from vision_architectures.layers.embeddings import (  # noqa: F401
    get_all_timestep_embeddings_1d,
    get_timestep_embeddings_1d,
)

# %% ../../nbs/utils/08_timesteps.ipynb 5
class TimestepSampler(nn.Module):
    """Sampel timesteps using a strategy.

    Args:
        total_timesteps: Total number of timesteps.
        strategy: Sampling strategy. Can be "gamma" or "uniform".
        gamma_alpha: Alpha parameter for the gamma distribution.
        gamma_beta: Beta parameter for the gamma distribution.
    """

    def __init__(
        self,
        total_timesteps: int,
        strategy: Literal["gamma", "uniform"],
        gamma_alpha: float = 1.0,
        gamma_beta: float = 1.0,
    ):
        super().__init__()

        self.total_timesteps = total_timesteps
        self.strategy = strategy

        if strategy == "uniform":
            self.distribution = torch.distributions.Uniform(0, 1)
        elif strategy == "gamma":
            self.distribution = torch.distributions.Gamma(gamma_alpha, gamma_beta)
        else:
            raise NotImplementedError(f"Sampling strategy {strategy} not implemented.")

    def forward(self, num_timesteps: int):
        """Sample timesteps based on the specified strategy.

        Args:
            num_timesteps: Number of timesteps to sample.
        """
        timesteps = self.distribution.sample((num_timesteps,))
        timesteps = timesteps / timesteps.max()
        timesteps = (timesteps * self.total_timesteps).long()
        timesteps = torch.clamp(timesteps, 0, self.total_timesteps - 1) + 1
        return timesteps
