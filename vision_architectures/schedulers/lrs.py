# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/schedulers/04_lrs.ipynb.

# %% auto 0
__all__ = ['ConstantLRWithWarmup']

# %% ../../nbs/schedulers/04_lrs.ipynb 2
from torch.optim.lr_scheduler import LambdaLR

from .cyclic import SineLR  # noqa:F401
from .sigmoid import SigmoidLR  # noqa:F401

# %% ../../nbs/schedulers/04_lrs.ipynb 5
class ConstantLRWithWarmup(LambdaLR):
    def __init__(self, optimizer, warmup_steps, min_lr_ratio: float = 0.001, *args, **kwargs):
        def lambda_fn(step):
            return min(1, min_lr_ratio + (1 - min_lr_ratio) * (step / warmup_steps))

        super().__init__(optimizer, lambda_fn, *args, **kwargs)
