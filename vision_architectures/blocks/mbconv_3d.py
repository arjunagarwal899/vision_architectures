# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/blocks/05_mbconv_3d.ipynb.

# %% auto 0
__all__ = ['MBConv3DConfig', 'MBConv3D']

# %% ../../nbs/blocks/05_mbconv_3d.ipynb 2
import torch
from einops import rearrange
from torch import nn

from .cnn import CNNBlock3D, CNNBlock3DConfig
from .se import SEBlock3D
from ..utils.activation_checkpointing import ActivationCheckpointing
from ..utils.custom_base_model import CustomBaseModel, model_validator
from ..utils.residuals import Residual

# %% ../../nbs/blocks/05_mbconv_3d.ipynb 4
class MBConv3DConfig(CustomBaseModel):
    dim: int
    expansion_ratio: float = 6.0
    se_reduction_ratio: float = 4.0
    kernel_size: int = 3
    padding: int = 1
    activation: str = "relu"
    normalization: str = "batchnorm3d"

    @property
    def hidden_dim(self):
        return int(self.expansion_ratio * self.dim)

    @model_validator(mode="after")
    def validate(self):
        super().validate()
        min_expansion_ratio = (self.dim + 1) / self.dim
        assert self.expansion_ratio > min_expansion_ratio, f"expansion_ratio must be greater than {min_expansion_ratio}"
        return self

# %% ../../nbs/blocks/05_mbconv_3d.ipynb 7
class MBConv3D(nn.Module):
    def __init__(self, config: MBConv3DConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__()

        self.config = MBConv3DConfig.model_validate(config | kwargs)

        dim = self.config.dim
        hidden_dim = self.config.hidden_dim
        kernel_size = self.config.kernel_size
        padding = self.config.padding
        se_reduction_ratio = self.config.se_reduction_ratio
        activation = self.config.activation
        normalization = self.config.normalization

        self.expand = CNNBlock3D(
            CNNBlock3DConfig(
                in_channels=dim,
                out_channels=hidden_dim,
                kernel_size=1,
                padding=0,
                activation=activation,
                normalization=normalization,
            )
        )
        self.depthwise_conv = CNNBlock3D(
            CNNBlock3DConfig(
                in_channels=hidden_dim,
                out_channels=hidden_dim,
                kernel_size=kernel_size,
                padding=padding,
                conv_kwargs=dict(groups=hidden_dim),
                activation=activation,
                normalization=normalization,
            )
        )
        self.se = SEBlock3D(dim=hidden_dim, r=se_reduction_ratio)
        self.pointwise_conv = CNNBlock3D(
            CNNBlock3DConfig(
                in_channels=hidden_dim,
                out_channels=dim,
                kernel_size=1,
                padding=0,
                activation=None,
                normalization=normalization,
            )
        )

        self.residual = Residual()

        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)

    def _forward(self, x: torch.Tensor, channels_first: bool = True):
        # x: (b, [dim], z, y, x, [dim])

        if not channels_first:
            x = rearrange(x, "b z y x d -> b d z y x")

        # Now x is (b, dim, z, y, x)

        res_connection = x

        # Expand
        x = self.expand(x, channels_first=True)
        # (b, hidden_dim, z, y, x)

        # Depthwise Conv
        x = self.depthwise_conv(x, channels_first=True)
        # (b, hidden_dim, z, y, x)

        # SE
        x = self.se(x, channels_first=True)
        # (b, hidden_dim, z, y, x)

        # Pointwise Conv
        x = self.pointwise_conv(x, channels_first=True)
        # (b, dim, z, y, x)

        # Residual
        x = self.residual(x, res_connection)

        if not channels_first:
            x = rearrange(x, "b d z y x -> b z y x d")
            # (b, z, y, x, dim)

        return x

    def forward(self, x: torch.Tensor, channels_first: bool = True):
        return self.checkpointing_level2(self._forward, x, channels_first)
