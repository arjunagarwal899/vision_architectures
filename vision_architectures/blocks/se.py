# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/blocks/03_se_3d.ipynb.

# %% auto 0
__all__ = ['SEBlock3DConfig', 'SEBlock3D']

# %% ../../nbs/blocks/03_se_3d.ipynb 2
import torch
from torch import nn

from ..utils.activation_checkpointing import ActivationCheckpointing
from ..utils.activations import get_act_layer
from ..utils.custom_base_model import CustomBaseModel
from ..utils.normalizations import get_norm_layer
from ..utils.rearrange import rearrange_channels

# %% ../../nbs/blocks/03_se_3d.ipynb 4
class SEBlock3DConfig(CustomBaseModel):
    dim: int
    r: float
    normalization: str = "batchnorm3d"
    activation: str = "silu"

# %% ../../nbs/blocks/03_se_3d.ipynb 6
class SEBlock3D(nn.Module):
    def __init__(self, config: SEBlock3DConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__()

        self.config = SEBlock3DConfig.model_validate(config | kwargs)

        dim = self.config.dim
        r = self.config.r
        activation = self.config.activation
        normalization = self.config.normalization

        excitation_dim = int(dim // r)

        self.squeeze = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.excite = nn.Sequential(
            nn.Conv3d(
                dim, excitation_dim, kernel_size=1, bias=False if normalization.startswith("batchnorm") else True
            ),
            get_norm_layer(normalization, excitation_dim),
            get_act_layer(activation),
            nn.Conv3d(
                excitation_dim, dim, kernel_size=1, bias=False if normalization.startswith("batchnorm") else True
            ),
            get_norm_layer(normalization, dim),
            nn.Sigmoid(),
        )

        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)

    def _forward(self, x: torch.Tensor, channels_first: bool = True):
        # x: (b, [dim], z, y, x, [dim])

        x = rearrange_channels(x, channels_first, True)
        # Now x is (b, dim, z, y, x)

        p = self.squeeze(x)
        # (b, dim, 1, 1, 1)
        p = self.excite(p)
        # (b, dim, 1, 1, 1)
        x = x * p
        # (b, dim, z, y, x)

        x = rearrange_channels(x, True, channels_first)
        # (b, [dim], z, y, x, [dim])

        return x

    def forward(self, *args, **kwargs):
        return self.checkpointing_level2(self._forward, *args, **kwargs)
