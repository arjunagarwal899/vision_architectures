# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/blocks/04_cnn.ipynb.

# %% auto 0
__all__ = ['possible_sequences', 'CNNBlockConfig', 'MultiResCNNBlockConfig', 'CNNBlock3D', 'CNNBlock2D', 'MultiResCNNBlock3D',
           'MultiResCNNBlock2D', 'TensorSplittingConv', 'add_tsp_to_module', 'remove_tsp_from_module']

# %% ../../nbs/blocks/04_cnn.ipynb 2
from functools import cache
from itertools import chain, permutations
from typing import Any, Literal

import torch
from loguru import logger
from torch import nn
from torch.nn import functional as F

from ..utils.activation_checkpointing import ActivationCheckpointing
from ..utils.activations import get_act_layer
from ..utils.custom_base_model import CustomBaseModel, Field, field_validator, model_validator
from ..utils.normalizations import get_norm_layer
from ..utils.rearrange import rearrange_channels
from ..utils.residuals import Residual
from ..utils.splitter_merger import Splitter

# %% ../../nbs/blocks/04_cnn.ipynb 4
possible_sequences = ["".join(p) for p in chain.from_iterable(permutations("ACDN", r) for r in range(5)) if "C" in p]


class CNNBlockConfig(CustomBaseModel):
    in_channels: int
    out_channels: int
    kernel_size: int | tuple[int, ...]
    padding: int | tuple[int, ...] | str = "same"
    stride: int = 1
    conv_kwargs: dict[str, Any] = {}
    transposed: bool = Field(False, description="Whether to perform ConvTranspose instead of Conv")

    normalization: str | None = "batchnorm3d"
    normalization_pre_args: list = []
    normalization_post_args: list = []
    normalization_kwargs: dict = {}
    activation: str | None = "relu"
    activation_kwargs: dict = {}

    sequence: Literal[tuple(possible_sequences)] = "CNA"

    drop_prob: float = 0.0

    @model_validator(mode="after")
    def validate(self):
        super().validate()
        if self.normalization is None and "N" in self.sequence:
            self.sequence = self.sequence.replace("N", "")
        if self.normalization is not None and "N" not in self.sequence:
            raise ValueError("Add N to the sequence or set normalization=None.")
        if self.activation is None and "A" in self.sequence:
            self.sequence = self.sequence.replace("A", "")
        if self.activation is not None and "A" not in self.sequence:
            raise ValueError("Add A to the sequence or set activation=None.")
        if self.drop_prob == 0.0 and "D" in self.sequence:
            self.sequence = self.sequence.replace("D", "")
        if self.drop_prob > 0.0 and "D" not in self.sequence:
            raise ValueError("Add D to the sequence or set drop_prob=0.")
        return self


class MultiResCNNBlockConfig(CNNBlockConfig):
    kernel_sizes: tuple[int | tuple[int, ...], ...] = (3, 5, 7)
    filter_ratios: tuple[float, ...] = Field(
        (1, 2, 3), description="Ratio of filters to out_channels for each conv layer. Will be scaled to sum to 1."
    )
    padding: Literal["same"] = "same"

    kernel_size: int = 3

    @field_validator("filter_ratios", mode="after")
    @classmethod
    def scale_filter_ratios(cls, filter_ratios):
        filter_ratios = tuple(ratio / sum(filter_ratios) for ratio in filter_ratios)
        return filter_ratios

    @model_validator(mode="after")
    def validate(self):
        super().validate()
        assert self.kernel_sizes == (3, 5, 7), "Only kernel sizes of (3, 5, 7) are supported for MultiResCNNBlock"
        assert self.kernel_size == 3, "only kernel_size = 3 is supported for MultiResCNNBlock"
        assert len(self.kernel_sizes) == len(
            self.filter_ratios
        ), "kernel_sizes and filter_ratios must have the same length"
        return self

# %% ../../nbs/blocks/04_cnn.ipynb 7
class _CNNBlock(nn.Module):
    def __init__(
        self, spatial_dims: Literal[2, 3], config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs
    ):
        super().__init__()

        self.config = CNNBlockConfig.model_validate(config | kwargs)

        normalization = self.config.normalization
        activation = self.config.activation
        drop_prob = self.config.drop_prob
        sequence = self.config.sequence

        bias = True
        if normalization is not None and normalization.startswith("batchnorm") and "CN" in sequence:
            bias = False

        match spatial_dims, self.config.transposed:
            case 2, False:
                conv_module = nn.Conv2d
            case 2, True:
                conv_module = nn.ConvTranspose2d
            case 3, False:
                conv_module = nn.Conv3d
            case 3, True:
                conv_module = nn.ConvTranspose3d
            case _:
                raise ValueError(f"Unsupported spatial dimensions: {spatial_dims}")

        self.conv = conv_module(
            in_channels=self.config.in_channels,
            out_channels=self.config.out_channels,
            kernel_size=self.config.kernel_size,
            padding=self.config.padding,
            stride=self.config.stride,
            bias=bias,
            **self.config.conv_kwargs,
        )

        self.norm = None
        self.act = None
        self.dropout = None

        norm_channels = self.config.out_channels
        if "N" in sequence.split("C")[0]:
            norm_channels = self.config.in_channels

        if "N" in sequence:
            self.norm = get_norm_layer(
                normalization,
                *self.config.normalization_pre_args,
                norm_channels,
                *self.config.normalization_post_args,
                **self.config.normalization_kwargs,
            )
        if "A" in sequence:
            self.act = get_act_layer(activation, **self.config.activation_kwargs)
        if "D" in sequence:
            self.dropout = nn.Dropout(drop_prob)

        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)

    def _forward(self, x: torch.Tensor, channels_first: bool = True):
        # x: (b, [in_channels], [z], y, x, [in_channels])

        x = rearrange_channels(x, channels_first, True)
        # Now x is (b, in_channels, [z], y, x)

        for layer in self.config.sequence:
            if layer == "C":
                x = self.conv(x)
            if layer == "A":
                x = self.act(x)
            elif layer == "D":
                x = self.dropout(x)
            elif layer == "N":
                x = self.norm(x)
        # (b, out_channels, [z], y, x)

        x = rearrange_channels(x, True, channels_first)
        # (b, [out_channels], [z], y, x, [out_channels])

        return x

    def forward(self, *args, **kwargs):
        return self.checkpointing_level1(self._forward, *args, **kwargs)

# %% ../../nbs/blocks/04_cnn.ipynb 8
class CNNBlock3D(_CNNBlock):
    def __init__(self, config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__(3, config, checkpointing_level, **kwargs)

# %% ../../nbs/blocks/04_cnn.ipynb 11
class CNNBlock2D(_CNNBlock):
    def __init__(self, config: CNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__(2, config, checkpointing_level, **kwargs)

# %% ../../nbs/blocks/04_cnn.ipynb 15
class _MultiResCNNBlock(nn.Module):
    def __init__(
        self, spatial_dims: Literal[2, 3], config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs
    ):
        super().__init__()

        self.config = MultiResCNNBlockConfig.model_validate(config | kwargs)

        assert self.config.kernel_sizes == (3, 5, 7), "Only kernel sizes of (3, 5, 7) are supported for now"

        all_out_channels = [max(1, int(self.config.out_channels * ratio)) for ratio in self.config.filter_ratios[:-1]]
        last_out_channels = self.config.out_channels - sum(all_out_channels)
        all_out_channels.append(last_out_channels)
        if last_out_channels <= 0:
            raise ValueError(
                f"These filter values ({self.config.filter_ratios}) won't work with the given out_channels. Please "
                f"adjust them. The out_channels of each conv layer is coming out to be {all_out_channels}."
            )
        all_in_channels = [self.config.in_channels] + all_out_channels[:-1]

        self.convs = nn.ModuleList(
            [
                _CNNBlock(
                    spatial_dims,
                    self.config.model_dump(),
                    checkpointing_level,
                    in_channels=in_channels,
                    out_channels=out_channels,
                    kernel_size=3,
                )
                for in_channels, out_channels in zip(all_in_channels, all_out_channels)
            ]
        )

        self.residual_conv = _CNNBlock(
            spatial_dims,
            self.config.model_dump(),
            checkpointing_level,
            in_channels=self.config.in_channels,
            out_channels=self.config.out_channels,
            kernel_size=1,
        )

        self.residual = Residual()

        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)

    def _forward(self, x: torch.Tensor, channels_first: bool = True):
        # x: (b, [in_channels], [z], y, x, [in_channels])

        x = rearrange_channels(x, channels_first, True)
        # (b, in_channels, [z], y, x)

        residual = self.residual_conv(x)
        # (b, out_channels, [z], y, x)

        conv_outputs = []
        for conv in self.convs:
            conv_input = conv_outputs[-1] if conv_outputs else x
            conv_output = conv(conv_input)
            conv_outputs.append(conv_output)
            # (b, one_of_all_out_channels, [z], y, x)

        x = torch.cat(conv_outputs, dim=1)
        # (b, out_channels, [z], y, x)

        x = self.residual(x, residual)
        # (b, out_channels, [z], y, x)

        x = rearrange_channels(x, True, channels_first)
        # (b, [out_channels], [z], y, x, [out_channels])

        return x

    def forward(self, *args, **kwargs):
        return self.checkpointing_level2(self._forward, *args, **kwargs)

# %% ../../nbs/blocks/04_cnn.ipynb 16
class MultiResCNNBlock3D(_MultiResCNNBlock):
    def __init__(self, config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__(3, config, checkpointing_level, **kwargs)

# %% ../../nbs/blocks/04_cnn.ipynb 18
class MultiResCNNBlock2D(_MultiResCNNBlock):
    def __init__(self, config: MultiResCNNBlockConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__(2, config, checkpointing_level, **kwargs)

# %% ../../nbs/blocks/04_cnn.ipynb 22
class TensorSplittingConv(nn.Module):
    """Convolution layer that operates on splits of a tensor on desired device and concatenates the results to give a
    lossless output. This is useful for large tensors that cannot fit in memory."""

    def __init__(self, conv: nn.Module, num_splits: int | tuple[int, ...], optimize_num_splits: bool = True):
        super().__init__()

        if isinstance(conv, nn.Conv2d):
            self.spatial_dims = 2
        elif isinstance(conv, nn.Conv3d):
            self.spatial_dims = 3
        else:
            raise ValueError("Unsupported convolution type. Only Conv2d and Conv3d are supported.")

        assert conv.stride == (1,) * self.spatial_dims, "Stride must be 1 for tensor splitting convolution."
        assert conv.padding == "same" or torch.allclose(
            torch.tensor(conv.padding), (torch.tensor(conv.kernel_size) - 1) // 2
        ), "Padding must be 'same' for tensor splitting convolution."

        if isinstance(num_splits, int):
            num_splits = (num_splits,) * self.spatial_dims
        assert len(num_splits) == self.spatial_dims, "num_splits must be a tuple of length equal to spatial_dims"

        self.conv = conv
        self.num_splits = num_splits
        self.optimize_num_splits = optimize_num_splits

    @cache
    def get_receptive_field(self) -> tuple[int, ...]:
        """Calculate the receptive field of the convolution layer."""
        kernel_size = torch.tensor(self.conv.kernel_size)
        dilation = torch.tensor(self.conv.dilation)
        receptive_field = dilation * (kernel_size - 1) + 1
        return tuple(receptive_field.tolist())

    @cache
    def get_edge_context(self):
        """Calculate the context size required to eliminate edge effects when merging the conv outputs into one."""
        receptive_field = self.get_receptive_field()
        context = torch.tensor(receptive_field) // 2
        return tuple(context.tolist())

    def get_input_shape(self, input_shape: tuple[int, ...] | torch.Size | torch.Tensor) -> tuple[int, ...]:
        """Get the input shape of the convolution layer."""
        if isinstance(input_shape, torch.Tensor):
            input_shape = input_shape.shape
        if isinstance(input_shape, torch.Size):
            input_shape = tuple(input_shape)
        input_shape = input_shape[-self.spatial_dims :]
        if len(input_shape) != self.spatial_dims:
            raise ValueError(f"Input shape must be of length {self.spatial_dims}. Got {len(input_shape)}.")
        return input_shape

    def get_optimized_num_splits(self, input_shape: tuple[int, ...] | torch.Size | torch.Tensor) -> tuple[int, ...]:
        """Optimize the number of splits for each dimension based on the input shape and number of splits

        Example:
            Let's say input shape is (110, 110) and num_splits is (12, 12). The input will first be padded to (120, 120)
            and then split into splits of size (10+overlap, 10+overlap) each. However, if you notice, the padding
            that was also equal to 10, and therefore was completely unnecessary as the same result can be achieved by
            using num_splits = (11, 11) and reducing 144-121=23 splits to be processed.

        Args:
            input_shape: Shape of the input tensor. If a tensor is provided, its shape will be used.

        Returns:
            Tuple of optimized number of splits for each dimension.
        """
        input_shape = self.get_input_shape(input_shape)

        num_splits = list(self.num_splits)
        for i in range(self.spatial_dims):
            while True:
                padding_required = (num_splits[i] - (input_shape[i] % num_splits[i])) % num_splits[i]
                split_size = (input_shape[i] + padding_required) // num_splits[i]
                if padding_required >= split_size:
                    num_splits[i] -= 1
                else:
                    break

        return tuple(num_splits)

    def pad_input_for_divisibility(self, x: torch.Tensor, num_splits: tuple[int, ...] = None) -> torch.Tensor:
        """Pad the input at the end of every spatial dimension such that it is perfectly divisible by the number of
        splits."""
        if num_splits is None:
            num_splits = self.num_splits
        padding = [0, 0] * (x.ndim - self.spatial_dims)
        for i in range(self.spatial_dims):
            dim = i + 2
            padding_required = (num_splits[i] - (x.shape[dim] % num_splits[i])) % num_splits[i]
            padding.extend([padding_required, 0])
        x = F.pad(x, list(reversed(padding)))
        return x

    def get_split_size(
        self, input_shape: tuple[int, ...] | torch.Size | torch.Tensor, num_splits: tuple[int, ...] = None
    ) -> tuple[int, ...]:
        """Calculate the split size for each dimension based on the input shape and number of splits.

        Args:
            input_shape: Shape of the input tensor. If a tensor is provided, its shape will be used.

        Returns:
            Tuple of split sizes for each dimension.
        """
        input_shape = self.get_input_shape(input_shape)
        if num_splits is None:
            num_splits = self.num_splits

        context = self.get_edge_context()

        split_size = []
        for i in range(self.spatial_dims):
            split_size.append(input_shape[i] // num_splits[i] + 2 * context[i])
        split_size = tuple(split_size)
        return split_size

    def get_split_stride(
        self, input_shape: tuple[int, ...] | torch.Size | torch.Tensor, num_splits: tuple[int, ...] = None
    ) -> tuple[int, ...]:
        """Calculate the split stride for each dimension based on the input shape and context size."""
        input_shape = self.get_input_shape(input_shape)
        if num_splits is None:
            num_splits = self.num_splits
        context = self.get_edge_context()
        split_size = self.get_split_size(input_shape, num_splits)
        split_stride = [split_size[i] - 2 * context[i] for i in range(self.spatial_dims)]
        assert all(
            split_stride[i] > 0 for i in range(self.spatial_dims)
        ), "Split stride must be greater than 0 for all dimensions."
        return split_stride

    def pad_input_for_context(self, x: torch.Tensor) -> torch.Tensor:
        """Pad the input with the context size for consistent merging."""
        context = self.get_edge_context()
        padding = [0, 0] * (x.ndim - self.spatial_dims)
        for i in range(self.spatial_dims):
            padding.extend([context[i], context[i]])
        x = F.pad(x, list(reversed(padding)))
        return x

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the convolution layer with tensor splitting parallelism. Main convolution occurs on it's
             device, but the output is built on the input tensor's device.

        Args:
            x: Input tensor of shape (batch_size, in_channels, [z], y, x).

        Returns:
            Output tensor of shape (batch_size, out_channels, [z], y, x).
        """
        input_device = x.device
        B, DIMS = x.shape[0], x.shape[2:]  # (batch_size, in_channels, [z], y, x)

        # Optimize num_splits
        num_splits = self.num_splits
        if self.optimize_num_splits:
            num_splits = self.get_optimized_num_splits(x)

        # Pad input such that it is divisible by the number of splits
        x = self.pad_input_for_divisibility(x, num_splits)

        # Calculate split size
        split_size = self.get_split_size(x, num_splits)

        # Identify the stride required to split the input tensor such that overlapping regions can be counted only once
        split_stride = self.get_split_stride(x, num_splits)

        # Pad the input
        x = self.pad_input_for_context(x)

        # Split the input tensor
        splitter = Splitter(
            split_dims=self.spatial_dims,
            split_size=split_size,
            stride=split_stride,
            extend_mode=None,  # Padding has been handled here for better control
        )
        positions = splitter.get_positions(x)
        x = splitter(x)
        # (num_splits, batch_size, in_channels, [z1], y1, x1)

        # Run the convolution on each split
        outputs: list[torch.Tensor] = []
        for x_split in x:
            x_split = x_split.to(self.conv.weight.device)
            x_split = self.conv(x_split)
            x_split = x_split.to(input_device)
            outputs.append(x_split)
        # list of len num_splits, each of shape (batch_size, out_channels, [z1], y1, x1)

        # Merge the outputs
        context = self.get_edge_context()
        merged = torch.empty((B, outputs[0].shape[1], *DIMS), device=input_device)
        for output, position in zip(outputs, positions):
            merged_slices = [slice(None), slice(None)]  # To track the coordinates where the output will be placed
            output_slices = [slice(None), slice(None)]  # To track the actual output that should be placed
            for i in range(self.spatial_dims):
                dim = i + 2
                merged_slice = slice(position[i], min(position[i] + split_stride[i], DIMS[i]))
                output_slice = slice(context[i], -context[i] if context[i] != 0 else None)

                merged_indices = merged_slice.indices(merged.shape[dim])
                output_indices = output_slice.indices(output.shape[dim])
                len_merged_slice = merged_indices[1] - merged_indices[0]
                len_output_slice = output_indices[1] - output_indices[0]
                if len_output_slice > len_merged_slice:
                    output_slice = slice(output_slice.start, output_slice.start + len_merged_slice)

                merged_slices.append(merged_slice)
                output_slices.append(output_slice)

            merged[tuple(merged_slices)] = output[tuple(output_slices)]

        return merged

    def extra_repr(self):
        return f"num_splits={self.num_splits}"

# %% ../../nbs/blocks/04_cnn.ipynb 25
def add_tsp_to_module(
    module: nn.Module,
    num_splits_2d: int | tuple[int, int] | None = None,
    num_splits_3d: int | tuple[int, int, int] = None,
    strict: bool = True,
) -> nn.Module:
    if num_splits_2d is None and num_splits_3d is None:
        raise ValueError("At least one of num_splits_2d or num_splits_3d must be provided.")
    for name, child in module.named_children():
        if isinstance(child, TensorSplittingConv):
            continue
        if num_splits_2d is not None and isinstance(child, nn.Conv2d):
            try:
                setattr(module, name, TensorSplittingConv(child, num_splits_2d).to(child.weight.device))
            except Exception as e:
                if strict:
                    raise e
                else:
                    logger.debug(f"Could not convert {name} to TensorSplittingConv. Error: {e}")

        if num_splits_3d is not None and isinstance(child, nn.Conv3d):
            try:
                setattr(module, name, TensorSplittingConv(child, num_splits_3d).to(child.weight.device))
            except Exception as e:
                if strict:
                    raise e
                else:
                    logger.debug(f"Could not convert {name} to TensorSplittingConv. Error: {e}")
        else:
            add_tsp_to_module(child, num_splits_2d, num_splits_3d, strict)
    return module

# %% ../../nbs/blocks/04_cnn.ipynb 26
def remove_tsp_from_module(module: nn.Module) -> nn.Module:
    for name, child in module.named_children():
        if isinstance(child, TensorSplittingConv):
            setattr(module, name, child.conv.to(child.conv.weight.device))
        else:
            remove_tsp_from_module(child)
    return module
