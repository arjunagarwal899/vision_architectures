# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_swinv2_3d_with_flash_attention.ipynb.

# %% auto 0
__all__ = ['SwinV23DMHSA', 'SwinV23DLayer', 'SwinV23DBlock', 'SwinV23DStage', 'SwinV23DEncoder', 'SwinV23DModel']

# %% ../nbs/04_swinv2_3d_with_flash_attention.ipynb 2
import torch
import numpy as np
from einops import rearrange, repeat
from torch import nn
import torch.nn.functional as F

from vision_architectures.swinv2_3d import (
    populate_and_validate_config,
    get_coords_grid,
    SwinV23DMHSA as SwinV23DMHSAWithoutFlashAttention,
    SwinV23DLayerMLP,
    SwinV23DLayer as SwinV23DLayerWithoutFlashAttention,
    SwinV23DBlock as SwinV23DBlockWithoutFlashAttention,
    SwinV23DPatchMerging,
    SwinV23DStage as SwinV23DStageWithoutFlashAttention,
    SwinV23DEncoder as SwinV23DEncoderWithoutFlashAttention,
    SwinV23DPatchEmbeddings,
    get_3d_position_embeddings,
    embed_spacings_in_position_embeddings,
    SwinV23DEmbeddings,
    SwinV23DModel as SwinV23DModelWithoutFlashAttention,
    SwinV23DMIMDecoder,
    SwinV23DMIM as SwinV23DMIMWithoutFlashAttention,
)

# %% ../nbs/04_swinv2_3d_with_flash_attention.ipynb 4
class SwinV23DMHSA(SwinV23DMHSAWithoutFlashAttention):
    def __init__(
        self,
        dim,
        num_heads,
        window_size,
        use_relative_position_bias,
        attn_drop_prob=0.0,
        proj_drop_prob=0.0,
    ):
        super().__init__(dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob)

        # Remove attention dropout layer as that is handled automatically, but store the dropout for later
        del self.attn_drop
        self.attn_drop_prob = attn_drop_prob

    def forward(self, hidden_states: torch.Tensor):
        # hidden_states: (windowed_b, window_size_z window_size_y window_size_x, dim)
        _, num_patches_z, num_patches_y, num_patches_x, _ = hidden_states.shape

        query, key, value = rearrange(
            self.W_qkv(hidden_states),
            "b nz ny nx (n num_heads d) -> n b num_heads (nz ny nx) d",
            n=3,
            num_heads=self.num_heads,
        )
        # num_patches = window_size_z * window_size_y * window_size_x
        # Each is (windowed_b, num_heads, num_patches, per_head_dim)

        query_normalized = F.normalize(query, dim=-1)
        key_normalized = F.normalize(key, dim=-1)

        relative_position_bias = None
        if self.use_relative_position_bias:
            relative_position_bias = self.calculate_relative_position_bias()

        logit_scale = torch.clamp(self.logit_scale, max=np.log(1.0 / 0.01)).exp()

        context = F.scaled_dot_product_attention(
            query_normalized,
            key_normalized,
            value,
            attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias
            dropout_p=self.attn_drop_prob,
            is_causal=False,
            # scale=logit_scale, # TODO: Allow logit scaling, otherwise it won't work
        )
        # (windowed_b, num_heads, num_patches, per_head_dim)
        context = rearrange(
            context,
            "b num_heads (num_patches_z num_patches_y num_patches_x) d -> "
            "b num_patches_z num_patches_y num_patches_x (num_heads d)",
            num_patches_z=num_patches_z,
            num_patches_y=num_patches_y,
            num_patches_x=num_patches_x,
        )
        # (windowed_b, window_size_z window_size_y window_size_x, dim)

        context = self.proj(context)
        context = self.proj_drop(context)
        # (windowed_b, window_size_z window_size_y window_size_x, dim)

        return context

# %% ../nbs/04_swinv2_3d_with_flash_attention.ipynb 7
class SwinV23DLayer(SwinV23DLayerWithoutFlashAttention):
    def __init__(
        self,
        dim,
        num_heads,
        intermediate_ratio,
        layer_norm_eps,
        window_size,
        use_relative_position_bias,
        attn_drop_prob=0.0,
        proj_drop_prob=0.0,
        mlp_drop_prob=0.0,
    ):
        super().__init__(
            dim,
            num_heads,
            intermediate_ratio,
            layer_norm_eps,
            window_size,
            use_relative_position_bias,
            attn_drop_prob,
            proj_drop_prob,
            mlp_drop_prob,
        )

        self.mhsa = SwinV23DMHSA(
            dim, num_heads, window_size, use_relative_position_bias, attn_drop_prob, proj_drop_prob
        )

# %% ../nbs/04_swinv2_3d_with_flash_attention.ipynb 8
class SwinV23DBlock(SwinV23DBlockWithoutFlashAttention):
    def __init__(self, stage_config):
        super().__init__(stage_config)

        self.stage_config = stage_config
        self.w_layer = SwinV23DLayer(
            stage_config["_out_dim"],
            stage_config["num_heads"],
            stage_config["intermediate_ratio"],
            stage_config["layer_norm_eps"],
            stage_config["window_size"],
            stage_config["use_relative_position_bias"],
            stage_config.get("attn_drop_prob", 0.0),
            stage_config.get("proj_drop_prob", 0.0),
            stage_config.get("mlp_drop_prob", 0.0),
        )
        self.sw_layer = SwinV23DLayer(
            stage_config["_out_dim"],
            stage_config["num_heads"],
            stage_config["intermediate_ratio"],
            stage_config["layer_norm_eps"],
            stage_config["window_size"],
            stage_config["use_relative_position_bias"],
            stage_config.get("attn_drop_prob", 0.0),
            stage_config.get("proj_drop_prob", 0.0),
            stage_config.get("mlp_drop_prob", 0.0),
        )

# %% ../nbs/04_swinv2_3d_with_flash_attention.ipynb 9
class SwinV23DStage(SwinV23DStageWithoutFlashAttention):
    def __init__(self, stage_config):
        super().__init__(stage_config)

        self.blocks = nn.ModuleList(
            [SwinV23DBlock(stage_config) for _ in range(stage_config["depth"])],
        )

# %% ../nbs/04_swinv2_3d_with_flash_attention.ipynb 10
class SwinV23DEncoder(SwinV23DEncoderWithoutFlashAttention):
    def __init__(self, config):
        super().__init__(config)

        self.stages = nn.ModuleList([SwinV23DStage(stage_config) for stage_config in config["stages"]])

# %% ../nbs/04_swinv2_3d_with_flash_attention.ipynb 11
class SwinV23DModel(SwinV23DModelWithoutFlashAttention):
    def __init__(self, config):
        super().__init__(config)

        self.encoder = SwinV23DEncoder(config)
