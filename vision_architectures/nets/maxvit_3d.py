# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/nets/07_maxvit_3d.ipynb.

# %% auto 0
__all__ = ['MaxViT3DStem0Config', 'MaxViT3DBlockConfig', 'MaxViT3DStemConfig', 'MaxViT3DEncoderConfig', 'MaxViT3DStem0',
           'MaxViT3DBlockAttention', 'MaxViT3DGridAttention', 'MaxViT3DBlock', 'MaxViT3DStem', 'MaxViT3DEncoder']

# %% ../../nbs/nets/07_maxvit_3d.ipynb 2
import torch
from einops import rearrange
from torch import nn

from ..blocks.cnn import CNNBlock3D, CNNBlock3DConfig
from ..blocks.mbconv_3d import MBConv3D, MBConv3DConfig
from ..blocks.transformer import Attention3DWithMLPConfig
from .swinv2_3d import SwinV23DLayer
from ..utils.activation_checkpointing import ActivationCheckpointing
from ..utils.custom_base_model import CustomBaseModel, model_validator

# %% ../../nbs/nets/07_maxvit_3d.ipynb 4
class MaxViT3DStem0Config(CustomBaseModel):
    in_channels: int
    dim: int
    depth: int = 2


class MaxViT3DBlockConfig(MBConv3DConfig, Attention3DWithMLPConfig):
    is_last: bool = False
    window_size: tuple[int, int, int]
    out_dim_ratio: int = 2  # Used only if is_last is True


class MaxViT3DStemConfig(MaxViT3DBlockConfig):
    depth: int


class MaxViT3DEncoderConfig(CustomBaseModel):
    stem0: MaxViT3DStem0Config
    stems: list[MaxViT3DStemConfig]

    @model_validator(mode="after")
    def validate(self):
        super().validate()
        assert self.stem0.dim == self.stems[0].dim, "Stem0 dim should be equal to the first stem dim"
        for i in range(1, len(self.stems)):
            assert (
                self.stems[i - 1].dim * self.stems[i - 1].out_dim_ratio == self.stems[i].dim
            ), "Stem dims should match"
        return self

# %% ../../nbs/nets/07_maxvit_3d.ipynb 6
class MaxViT3DStem0(nn.Module):
    def __init__(self, config: MaxViT3DStem0Config = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__()

        self.config = MaxViT3DStem0Config.model_validate(config | kwargs)

        self.layers = nn.ModuleList()
        self.layers.append(
            CNNBlock3D(
                CNNBlock3DConfig(
                    in_channels=self.config.in_channels,
                    out_channels=self.config.dim,
                    kernel_size=3,
                    stride=2,
                    padding=1,
                    normalization="batchnorm3d",
                    activation="gelu",
                ),
                checkpointing_level,
            )
        )
        for i in range(self.config.depth - 1):
            self.layers.append(
                CNNBlock3D(
                    CNNBlock3DConfig(
                        in_channels=self.config.dim,
                        out_channels=self.config.dim,
                        kernel_size=3,
                        stride=1,
                        padding="same",
                        normalization="batchnorm3d" if i < self.config.depth - 1 else None,
                        activation="gelu" if i < self.config.depth - 1 else None,
                    ),
                    checkpointing_level,
                )
            )

        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)

    def _forward(self, x: torch.Tensor, channels_first: bool = True):
        # x: (b, [in_channels], z, y, x, [in_channels])

        if not channels_first:
            x = rearrange(x, "b z y x d -> b d z y x")

        # Now x is (b, in_channels, z, y, x)

        for layer in self.layers:
            x = layer(x)
            # (b, dim, z1, y1, x1)

        if not channels_first:
            x = rearrange(x, "b d z y x -> b z y x d")
            # (b, z1, y1, x1, dim)

        return x

    def forward(self, x: torch.Tensor, channels_first: bool = True):
        return self.checkpointing_level2(self._forward, x, channels_first)

# %% ../../nbs/nets/07_maxvit_3d.ipynb 8
class MaxViT3DBlockAttention(SwinV23DLayer):
    pass

# %% ../../nbs/nets/07_maxvit_3d.ipynb 10
class MaxViT3DGridAttention(SwinV23DLayer):
    @staticmethod
    def _get_rearrange_patterns():
        forward_pattern = (
            "b (window_size_z num_windows_z) (window_size_y num_windows_y) (window_size_x num_windows_x) dim -> "
            "(b num_windows_z num_windows_y num_windows_x) window_size_z window_size_y window_size_x dim "
        )
        reverse_pattern = (
            "(b num_windows_z num_windows_y num_windows_x) window_size_z window_size_y window_size_x dim -> "
            "b (window_size_z num_windows_z) (window_size_y num_windows_y) (window_size_x num_windows_x) dim"
        )
        return forward_pattern, reverse_pattern

# %% ../../nbs/nets/07_maxvit_3d.ipynb 12
class MaxViT3DBlock(nn.Module):
    def __init__(self, config: MaxViT3DBlockConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__()

        self.config = MaxViT3DBlockConfig.model_validate(config | kwargs)

        mbconv_kwargs = {}
        out_dim = self.config.dim
        if self.config.is_last:
            out_dim = self.config.dim * self.config.out_dim_ratio
            mbconv_kwargs["stride"] = 2
            mbconv_kwargs["padding"] = 1
            mbconv_kwargs["out_dim"] = out_dim

        self.mbconv = MBConv3D(self.config.model_dump(), checkpointing_level=checkpointing_level, **mbconv_kwargs)

        # modify dim in the config
        self.config = MaxViT3DBlockConfig.model_validate(self.config | {"dim": out_dim})

        self.block_attention = MaxViT3DBlockAttention(self.config.model_dump(), checkpointing_level=checkpointing_level)
        self.grid_attention = MaxViT3DGridAttention(self.config.model_dump(), checkpointing_level=checkpointing_level)

        self.checkpointing_level3 = ActivationCheckpointing(3, checkpointing_level)

    def _forward(self, x: torch.Tensor, channels_first: bool = True):
        # x: (b, [dim], z, y, x, [dim])

        x = self.mbconv(x, channels_first)
        # (b, [dim], z1, y1, x1, [dim])

        if channels_first:
            x = rearrange(x, "b d z y x -> b z y x d")

        x = self.block_attention(x)
        # (b, dim, z1, y1, x1)
        x = self.grid_attention(x)
        # (b, dim, z1, y1, x1)

        if channels_first:
            x = rearrange(x, "b z y x d -> b d z y x")

        return x

    def forward(self, x: torch.Tensor, channels_first: bool = True):
        return self.checkpointing_level3(self._forward, x, channels_first)

# %% ../../nbs/nets/07_maxvit_3d.ipynb 15
class MaxViT3DStem(nn.Module):
    def __init__(self, config: MaxViT3DStemConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__()

        self.config = MaxViT3DStemConfig.model_validate(config | kwargs)

        self.blocks = nn.ModuleList(
            MaxViT3DBlock(
                self.config.model_dump(),
                checkpointing_level=checkpointing_level,
                is_last=True if i == self.config.depth - 1 else False,
            )
            for i in range(self.config.depth)
        )

        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)

    def _forward(self, x: torch.Tensor, channels_first: bool = True):
        # x: (b, [dim], z, y, x, [dim])
        for layer in self.blocks:
            x = layer(x, channels_first)
        return x

    def forward(self, x: torch.Tensor, channels_first: bool = True):
        return self.checkpointing_level4(self._forward, x, channels_first)

# %% ../../nbs/nets/07_maxvit_3d.ipynb 17
class MaxViT3DEncoder(nn.Module):
    def __init__(self, config: MaxViT3DEncoderConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__()

        self.config = MaxViT3DEncoderConfig.model_validate(config | kwargs)

        self.stems = nn.ModuleList([])
        self.stems.append(MaxViT3DStem0(self.config.stem0, checkpointing_level))
        for stem_config in self.config.stems:
            self.stems.append(MaxViT3DStem(stem_config, checkpointing_level))

        self.checkpointing_level5 = ActivationCheckpointing(5, checkpointing_level)

    def _forward(self, x: torch.Tensor, channels_first: bool = True):
        # x: (b, [in_channels], z, y, x, [in_channels])
        for stem in self.stems:
            x = stem(x, channels_first)
        return x

    def forward(self, x: torch.Tensor, channels_first: bool = True):
        return self.checkpointing_level5(self._forward, x, channels_first)
