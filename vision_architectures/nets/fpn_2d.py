# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/nets/11_fpn_2d.ipynb.

# %% auto 0
__all__ = ['FPN2DBlockConfig', 'FPN2DConfig', 'FPN2DBlock', 'FPN2D']

# %% ../../nbs/nets/11_fpn_2d.ipynb 2
from typing import Literal

import torch
from huggingface_hub import PyTorchModelHubMixin
from torch import nn
from torch.nn import functional as F

from ..blocks.cnn import CNNBlock2D, CNNBlockConfig
from ..utils.activation_checkpointing import ActivationCheckpointing
from ..utils.custom_base_model import CustomBaseModel, Field, model_validator

# %% ../../nbs/nets/11_fpn_2d.ipynb 4
class FPN2DBlockConfig(CNNBlockConfig):
    dim: int
    kernel_size: int = 3
    skip_conn_dim: int
    is_deepest: bool = Field(False, description="True if this is the deepest block in the FPN, else False")
    interpolation_mode: str = "bilinear"
    merge_method: Literal["add", "concat"] = "add"

    normalization: str = "batchnorm2d"

    in_channels: None = Field(None, description="Calculated based on other parameters")
    out_channels: None = Field(None, description="Calculated based on other parameters")


class FPN2DConfig(CustomBaseModel):
    blocks: list[FPN2DBlockConfig]

    @property
    def dim(self):
        return self.blocks[0].dim

    @model_validator(mode="before")
    @classmethod
    def validate_before(cls, data):
        if isinstance(data, dict):
            # Add skip_conn_dim first
            if "skip_conn_dims" in data:
                # Assume blocks are to be built from scratch
                assert "blocks" not in data, "Cannot provide both skip_conn_dims and blocks"
                skip_conn_dims = data.pop("skip_conn_dims")
                blocks: list[dict] = []
                for i, skip_conn_dim in enumerate(skip_conn_dims):
                    blocks.append({"skip_conn_dim": skip_conn_dim, "is_deepest": (i == len(skip_conn_dims) - 1)})
                data.setdefault("blocks", blocks)

            # Add the remaining
            for key, value in data.items():
                for block in data.get("blocks", []):
                    block.setdefault(key, value)
        return data

    @model_validator(mode="after")
    def validate(self):
        for block in self.blocks:
            assert block.dim == self.dim, "All blocks must have the same dim"
        return self

# %% ../../nbs/nets/11_fpn_2d.ipynb 8
class FPN2DBlock(nn.Module):
    def __init__(self, config: FPN2DBlockConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__()

        self.config = FPN2DBlockConfig.model_validate(config | kwargs | {"kernel_size": 3})

        self.skip_conn_conv = CNNBlock2D(
            self.config,
            in_channels=self.config.skip_conn_dim,
            out_channels=self.config.dim,
            kernel_size=1,
            checkpointing_level=checkpointing_level,
        )

        if not self.config.is_deepest:
            self.out_conv = CNNBlock2D(
                self.config,
                in_channels=self.config.dim if self.config.merge_method == "add" else 2 * self.config.dim,
                out_channels=self.config.dim,
                checkpointing_level=checkpointing_level,
            )

        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)

    def _forward(self, skip_conn_features: torch.Tensor, features: torch.Tensor | None):
        # skip_conn_features: (b, skip_conn_dim, h1, w1)
        # features: (b, dim, h2, w2)

        skip_conn_features = self.skip_conn_conv(skip_conn_features)
        # (b, dim, h1, w1)

        if not self.config.is_deepest:
            features = F.interpolate(
                features,
                size=skip_conn_features.shape[2:],
                mode=self.config.interpolation_mode,
                align_corners=False,
            )
            # (b, dim, h1, w1)

            if self.config.merge_method == "add":
                merged_features = skip_conn_features + features
                # (b, dim, h1, w1)
            elif self.config.merge_method == "concat":
                merged_features = torch.cat((skip_conn_features, features), dim=1)
                # (b, 2 * dim, h1, w1)

            merged_features = self.out_conv(merged_features)
            # (b, dim, h1, w1)
        else:
            merged_features = skip_conn_features

        return merged_features

    def forward(self, *args, **kwargs):
        return self.checkpointing_level2(self._forward, *args, **kwargs)

# %% ../../nbs/nets/11_fpn_2d.ipynb 12
class FPN2D(nn.Module, PyTorchModelHubMixin):
    def __init__(self, config: FPN2DConfig = {}, checkpointing_level: int = 0, **kwargs):
        super().__init__()

        self.config = FPN2DConfig.model_validate(config | kwargs)

        self.blocks = nn.ModuleList()
        for i in range(len(self.config.blocks)):
            self.blocks.append(FPN2DBlock(self.config.blocks[i], checkpointing_level))

        self.checkpointing_level4 = ActivationCheckpointing(4, checkpointing_level)

    def _forward(self, features: list[torch.Tensor]):
        # features: [
        #   (b, in_dim1, h1, w1),
        #   (b, in_dim2, h2, w2),
        #   ...
        # ]

        outputs = [None]
        for i in reversed(range(len(features))):
            outputs.append(self.blocks[i](features[i], outputs[-1]))
        outputs = outputs[1:]

        return outputs

    def forward(self, *args, **kwargs):
        return self.checkpointing_level4(self._forward, *args, **kwargs)
