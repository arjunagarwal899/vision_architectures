# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/layers/01_attention.ipynb.

# %% auto 0
__all__ = ['Attention1DConfig', 'Attention3DConfig', 'Attention1DMLPConfig', 'Attention3DMLPConfig', 'Attention1DWithMLPConfig',
           'Attention3DWithMLPConfig', 'Attention1D', 'Attention3D', 'Attention1DMLP', 'Attention3DMLP',
           'Attention1DWithMLP', 'Attention3DWithMLP']

# %% ../../nbs/layers/01_attention.ipynb 2
from functools import partial
from typing import Literal

import torch
import torch.nn.functional as F
from einops import rearrange
from torch import nn

from .embeddings import RelativePositionEmbeddings
from ..utils.activations import get_act_layer
from ..utils.custom_base_model import CustomBaseModel, Field, computed_field, model_validator

# %% ../../nbs/layers/01_attention.ipynb 4
class Attention1DConfig(CustomBaseModel):
    dim: int | tuple[int, int]
    num_heads: int  = Field(..., description="Number of query heads")
    ratio_q_to_kv_heads: int = 1
    relative_position_bias: RelativePositionEmbeddings | None = None
    logit_scale: float | None = None
    logit_scale_learnable: bool = False
    attn_drop_prob: float = 0.0
    proj_drop_prob: float = 0.0

    @computed_field
    @property
    def num_kv_heads(self) -> int:
        return self.num_heads // self.ratio_q_to_kv_heads

    @computed_field
    @property
    def gqa_mqa_enabled(self) -> bool:
        return self.ratio_q_to_kv_heads != 1

    @computed_field
    @property
    def dim_qk(self) -> int:
        if isinstance(self.dim, tuple):
            return self.dim[0]
        return self.dim

    @computed_field
    @property
    def dim_v(self) -> int:
        if isinstance(self.dim, tuple):
            return self.dim[1]
        return self.dim

    @computed_field
    @property
    def per_head_dim_qk(self) -> int:
        return self.dim_qk // self.num_heads

    @model_validator(mode="after")
    def validate(self):
        super().validate()
        if self.gqa_mqa_enabled:
            assert torch.__version__ >= "2.5", "Need PyTorch version >= 2.5 for GQA and MQA"

        assert self.dim_qk % self.num_heads == 0, "dimension must be divisible by number of heads"
        assert (
            self.num_heads % self.num_kv_heads == 0
        ), "number of query heads must be divisible by number of key and value heads"

        return self


class Attention3DConfig(Attention1DConfig):
    pass


class Attention1DMLPConfig(CustomBaseModel):
    dim: int
    mlp_ratio: int = 4
    activation: str = "gelu"
    mlp_drop_prob: float = 0.0


class Attention3DMLPConfig(Attention1DMLPConfig):
    pass


class Attention1DWithMLPConfig(Attention1DConfig, Attention1DMLPConfig):
    dim: int | tuple[int, int]
    norm_location: Literal["pre", "post"] = "post"
    layer_norm_eps: float = 1e-6


class Attention3DWithMLPConfig(Attention3DConfig, Attention3DMLPConfig):
    dim: int | tuple[int, int]
    norm_location: Literal["pre", "post"] = "post"
    layer_norm_eps: float = 1e-6

# %% ../../nbs/layers/01_attention.ipynb 6
class Attention1D(nn.Module):
    """
    Performs attention (MHA, GQA, and MQA) on 1D sequences
    Parameters:
        - dim: Input dimensions of k, q, and v. Attention happens at dim=dim_qk.
            int: dimension of q, k, and v
            tuple[int, int]: first int is dimension of q and k, second int is dimension of v
        - num_heads: number of query heads
        - ratio_q_to_kv_heads: number of query heads per key and value head
        - relative_position_bias: RelativePositionEmbeddings object
        - logit_scale: scale of the logits. Defaults to 1/sqrt(d)
        - logit_scale_learnable: whether the logit scale is learnable
        - attn_drop_prob: dropout probability for attention weights
        - proj_drop_prob: dropout probability for projection layer
    """

    def __init__(self, config: Attention1DConfig | None = None, **kwargs):
        super().__init__()

        self.config = Attention1DConfig.model_validate(config or kwargs)

        dim_qk = self.config.dim_qk
        dim_v = self.config.dim_v
        ratio_q_to_kv_heads = self.config.ratio_q_to_kv_heads
        per_head_dim = self.config.per_head_dim_qk
        relative_position_bias = self.config.relative_position_bias
        logit_scale = self.config.logit_scale
        logit_scale_learnable = self.config.logit_scale_learnable
        attn_drop_prob = self.config.attn_drop_prob
        proj_drop_prob = self.config.proj_drop_prob

        self.W_q = nn.Linear(dim_qk, dim_qk)
        self.W_k = nn.Linear(dim_qk, dim_qk // ratio_q_to_kv_heads)
        self.W_v = nn.Linear(dim_v, dim_qk // ratio_q_to_kv_heads)
        self.attn_drop_prob = attn_drop_prob
        self.proj = nn.Linear(dim_qk, dim_qk)
        self.proj_drop = nn.Dropout(proj_drop_prob)

        if logit_scale is None:
            self.logit_scale = nn.Parameter(torch.tensor([per_head_dim**-0.5]), requires_grad=logit_scale_learnable)
        else:
            self.logit_scale = logit_scale

        self.relative_position_bias = relative_position_bias

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
        """
        Parameters: T => number of tokens, b => batch size
            - query: (b, T_q, dim_qk)
            - key: (b, T_kv, dim_qk)
            - value: (b, T_kv, dim_v)
        """

        query = self.W_q(query)
        key = self.W_k(key)
        value = self.W_v(value)

        rearrange_partial = partial(rearrange, pattern="b T (num_heads d) -> b num_heads T d")
        query = rearrange_partial(query, num_heads=self.config.num_heads)
        key = rearrange_partial(key, num_heads=self.config.num_kv_heads)
        value = rearrange_partial(value, num_heads=self.config.num_kv_heads)
        # query: (b, num_heads, T, per_head_dim)
        # key: (b, num_kv_heads, T, per_head_dim)
        # value: (b, num_kv_heads, T, per_head_dim)

        if isinstance(self.logit_scale, nn.Module):
            logit_scale = self.logit_scale()
        else:
            logit_scale = self.logit_scale

        query_normalized = F.normalize(query, dim=-1)
        key_normalized = F.normalize(key, dim=-1)

        query_normalized_and_scaled = query_normalized * logit_scale  # Scale the query beforehand

        relative_position_bias = None
        if self.relative_position_bias is not None:
            relative_position_bias = self.relative_position_bias()

        output = F.scaled_dot_product_attention(
            query_normalized_and_scaled,
            key_normalized,
            value,
            attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias
            dropout_p=self.attn_drop_prob,
            is_causal=False,
            scale=1.0,  # Already scaled the vectors
            enable_gqa=self.config.gqa_mqa_enabled,
        )
        # (b, num_heads, T, per_head_dim)

        output = rearrange(output, "b num_heads T d -> b T (num_heads d)")
        # (b, T, dim_qk)

        output = self.proj(output)
        output = self.proj_drop(output)
        # (b, T, dim_qk)

        return output

# %% ../../nbs/layers/01_attention.ipynb 8
class Attention3D(Attention1D):
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, channels_first: bool = True):
        """
        Parameters: z => depth, y => height, x => width, b => batch size
            - query: (b, [dim_qk], z_q, y_q, x_q, [dim_qk])
            - key: (b, [dim_qk], z_k, y_k, x_k, [dim_qk])
            - value: (b, [dim_v], z_k, y_k, x_k, [dim_v])
            - channels_first: if True, BCDHW expected, else BDHWC

        Constraints:
            - d_q * h_q * w_q = d_k * h_k * w_k
        """

        if channels_first:
            z_q, y_q, x_q = query.shape[2:5]
            forward_pattern = "b d z y x -> b (z y x) d"
            reverse_pattern = "b (z y x) d -> b d z y x"
        else:
            z_q, y_q, x_q = query.shape[1:4]
            forward_pattern = "b z y x d -> b (z y x) d"
            reverse_pattern = "b (z y x) d -> b z y x d"

        query = rearrange(query, forward_pattern)
        key = rearrange(key, forward_pattern)
        value = rearrange(value, forward_pattern)

        output = super().forward(query, key, value)

        output = rearrange(output, reverse_pattern, z=z_q, y=y_q, x=x_q)

        return output

# %% ../../nbs/layers/01_attention.ipynb 10
class Attention1DMLP(nn.Module):
    def __init__(self, config: Attention1DMLPConfig | None = None, **kwargs):
        super().__init__()

        self.config = Attention1DMLPConfig.model_validate(config or kwargs)

        dim = self.config.dim
        mlp_ratio = self.config.mlp_ratio
        activation = self.config.activation
        mlp_drop_prob = self.config.mlp_drop_prob

        self.dense1 = nn.Linear(dim, dim * mlp_ratio)

        if isinstance(activation, nn.Module):
            self.act = activation
        else:
            self.act = get_act_layer(activation)

        self.dense2 = nn.Linear(dim * mlp_ratio, dim)
        self.dropout = nn.Dropout(mlp_drop_prob)

    def forward(self, hidden_states: torch.Tensor):
        # hidden_states: (b, T, dim)
        hidden_states = self.dense1(hidden_states)
        hidden_states = self.act(hidden_states)
        hidden_states = self.dense2(hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states

# %% ../../nbs/layers/01_attention.ipynb 12
class Attention3DMLP(Attention1DMLP):
    def forward(self, hidden_states: torch.Tensor, channels_first: bool = True):
        # hidden_states: (b, dim, z, y, x) or (b, z, y, x, dim)

        if channels_first:
            hidden_states = rearrange(hidden_states, "b d z y x -> b z y x d")

        hidden_states = super().forward(hidden_states)

        if channels_first:
            hidden_states = rearrange(hidden_states, "b z y x d -> b d z y x")

        return hidden_states

# %% ../../nbs/layers/01_attention.ipynb 14
class Attention1DWithMLP(nn.Module):
    def __init__(self, config: Attention1DWithMLPConfig | None = None, **kwargs):
        super().__init__()

        self.config = Attention1DWithMLPConfig.model_validate(config or kwargs)

        dim = self.config.dim
        dim_qk = self.config.dim_qk
        num_heads = self.config.num_heads
        ratio_q_to_kv_heads = self.config.ratio_q_to_kv_heads
        relative_position_bias = self.config.relative_position_bias
        logit_scale = self.config.logit_scale
        logit_scale_learnable = self.config.logit_scale_learnable
        attn_drop_prob = self.config.attn_drop_prob
        proj_drop_prob = self.config.proj_drop_prob
        mlp_ratio = self.config.mlp_ratio
        activation = self.config.activation
        mlp_drop_prob = self.config.mlp_drop_prob
        layer_norm_eps = self.config.layer_norm_eps

        self.attn = Attention1D(
            dim=dim,
            num_heads=num_heads,
            ratio_q_to_kv_heads=ratio_q_to_kv_heads,
            relative_position_bias=relative_position_bias,
            logit_scale=logit_scale,
            logit_scale_learnable=logit_scale_learnable,
            attn_drop_prob=attn_drop_prob,
            proj_drop_prob=proj_drop_prob,
        )
        self.layernorm1 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)
        self.mlp = Attention1DMLP(dim=dim_qk, mlp_ratio=mlp_ratio, activation=activation, mlp_drop_prob=mlp_drop_prob)
        self.layernorm2 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
        # Each is (b, T, dim)
        res_connection1 = query
        # (b, T, dim)

        if self.config.norm_location == "pre":
            query = self.layernorm1(query)
            key = self.layernorm1(key)
            value = self.layernorm1(value)
            # (b, T, dim)

        hidden_states = self.attn(query, key, value)
        # (b, T, dim)

        if self.config.norm_location == "post":
            hidden_states = self.layernorm1(hidden_states)
            # (b, T, dim)

        hidden_states = hidden_states + res_connection1
        res_connection2 = hidden_states
        # (b, T, dim)

        if self.config.norm_location == "pre":
            hidden_states = self.layernorm2(hidden_states)
            # (b, T, dim)

        hidden_states = self.mlp(hidden_states)
        # (b, T, dim)

        if self.config.norm_location == "post":
            hidden_states = self.layernorm2(hidden_states)
            # (b, T, dim)

        hidden_states = hidden_states + res_connection2
        # (b, T, dim)

        return hidden_states

# %% ../../nbs/layers/01_attention.ipynb 16
class Attention3DWithMLP(nn.Module):
    def __init__(self, config: Attention3DWithMLPConfig | None = None, **kwargs):
        super().__init__()

        self.config = Attention3DWithMLPConfig.model_validate(config or kwargs)

        dim = self.config.dim
        dim_qk = self.config.dim_qk
        num_heads = self.config.num_heads
        ratio_q_to_kv_heads = self.config.ratio_q_to_kv_heads
        relative_position_bias = self.config.relative_position_bias
        logit_scale = self.config.logit_scale
        logit_scale_learnable = self.config.logit_scale_learnable
        attn_drop_prob = self.config.attn_drop_prob
        proj_drop_prob = self.config.proj_drop_prob
        mlp_ratio = self.config.mlp_ratio
        activation = self.config.activation
        mlp_drop_prob = self.config.mlp_drop_prob
        layer_norm_eps = self.config.layer_norm_eps

        self.attn = Attention3D(
            dim=dim,
            num_heads=num_heads,
            ratio_q_to_kv_heads=ratio_q_to_kv_heads,
            relative_position_bias=relative_position_bias,
            logit_scale=logit_scale,
            logit_scale_learnable=logit_scale_learnable,
            attn_drop_prob=attn_drop_prob,
            proj_drop_prob=proj_drop_prob,
        )
        self.layernorm1 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)
        self.mlp = Attention3DMLP(dim=dim_qk, mlp_ratio=mlp_ratio, activation=activation, mlp_drop_prob=mlp_drop_prob)
        self.layernorm2 = nn.LayerNorm(dim_qk, eps=layer_norm_eps)

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, channels_first: bool = True):
        # Each is (b, [dim], tokens_z, tokens_y, tokens_x, [dim])

        if channels_first:
            query = rearrange(query, "b d z y x -> b z y x d")
            key = rearrange(key, "b d z y x -> b z y x d")
            value = rearrange(value, "b d z y x -> b z y x d")
            # (b, tokens_z, tokens_y, tokens_x, dim)

        res_connection1 = query
        # (b, tokens_z, tokens_y, tokens_x, dim)

        if self.config.norm_location == "pre":
            query = self.layernorm1(query)
            key = self.layernorm1(key)
            value = self.layernorm1(value)
            # (b, tokens_z, tokens_y, tokens_x, dim)

        hidden_states = self.attn(query, key, value, channels_first=False)
        # (b, tokens_z, tokens_y, tokens_x, dim)

        if self.config.norm_location == "post":
            hidden_states = self.layernorm1(hidden_states)
            # (b, tokens_z, tokens_y, tokens_x, dim)

        hidden_states = hidden_states + res_connection1
        res_connection2 = hidden_states
        # (b, tokens_z, tokens_y, tokens_x, dim)

        if self.config.norm_location == "pre":
            hidden_states = self.layernorm2(hidden_states)
            # (b, tokens_z, tokens_y, tokens_x, dim)

        hidden_states = self.mlp(hidden_states, channels_first=False)
        # (b, tokens_z, tokens_y, tokens_x, dim)

        if self.config.norm_location == "post":
            hidden_states = self.layernorm2(hidden_states)
            # (b, tokens_z, tokens_y, tokens_x, dim)

        hidden_states = hidden_states + res_connection2
        # (b, tokens_z, tokens_y, tokens_x, dim)

        if channels_first:
            hidden_states = rearrange(hidden_states, "b z y x d -> b d z y x")
            # (b, dim, tokens_z, tokens_y, tokens_x)

        return hidden_states
