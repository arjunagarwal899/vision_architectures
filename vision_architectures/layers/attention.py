# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/layers/01_attention.ipynb.

# %% auto #0
__all__ = ['Attention1DConfig', 'Attention3DConfig', 'Attention1D', 'Attention3D']

# %% ../../nbs/layers/01_attention.ipynb #70207962
from functools import partial, wraps
from typing import Literal

import torch
import torch.nn.functional as F
from einops import rearrange
from loguru import logger
from torch import nn

from ..docstrings import populate_docstring
from vision_architectures.layers.embeddings import (
    RelativePositionEmbeddings,
    RotaryPositionEmbeddings1D,
    RotaryPositionEmbeddings1DConfig,
    RotaryPositionEmbeddings3D,
    RotaryPositionEmbeddings3DConfig,
)
from ..utils.activation_checkpointing import ActivationCheckpointing
from ..utils.custom_base_model import CustomBaseModel, Field, model_validator
from ..utils.rearrange import rearrange_channels

# %% ../../nbs/layers/01_attention.ipynb #755efca9
class Attention1DConfig(CustomBaseModel):
    dim: int | tuple[int, int] = Field(
        ...,
        description=(
            "Dimension of the input features. If tuple, (dim_qk, dim_v). "
            "Otherwise it is assumed to be dim of both qk and v."
        ),
    )
    num_heads: int = Field(..., description="Number of query heads")
    ratio_q_to_kv_heads: int = Field(1, description="Ratio of query heads to key/value heads. Useful for MQA/GQA.")
    logit_scale_learnable: bool = Field(False, description="Whether the logit scale is learnable.")
    attn_drop_prob: float = Field(0.0, description="Dropout probability for attention weights.")
    proj_drop_prob: float = Field(0.0, description="Dropout probability for the projection layer.")
    max_attention_batch_size: int = Field(
        -1,
        description=(
            "Runs attention by splitting the inputs into chunks of this size. 0 means no chunking. "
            "Useful for large inputs during inference. (This happens along batch dimension)."
        ),
    )
    rotary_position_embeddings_config: RotaryPositionEmbeddings1DConfig | None = Field(
        None, description="Config for rotary position embeddings"
    )

    @property
    def num_q_heads(self) -> int:
        return self.num_heads

    @property
    def num_kv_heads(self) -> int:
        return self.num_heads // self.ratio_q_to_kv_heads

    @property
    def gqa_mqa_enabled(self) -> bool:
        return self.ratio_q_to_kv_heads != 1

    @property
    def dim_qk(self) -> int:
        if isinstance(self.dim, tuple):
            return self.dim[0]
        return self.dim

    @property
    def dim_v(self) -> int:
        if isinstance(self.dim, tuple):
            return self.dim[1]
        return self.dim

    @property
    def per_head_dim_qk(self) -> int:
        return self.dim_qk // self.num_heads

    @model_validator(mode="after")
    def validate(self):
        super().validate()
        if self.gqa_mqa_enabled:
            assert torch.__version__ >= "2.5", "Need PyTorch version >= 2.5 for GQA and MQA"

        assert self.dim_qk % self.num_heads == 0, "dimension must be divisible by number of heads"
        assert (
            self.num_heads % self.num_kv_heads == 0
        ), "number of query heads must be divisible by number of key and value heads"

        return self


class Attention3DConfig(Attention1DConfig):
    rotary_position_embeddings_config: RotaryPositionEmbeddings3DConfig | None = Field(
        None, description="Config for rotary position embeddings"
    )

# %% ../../nbs/layers/01_attention.ipynb #64813808
@populate_docstring
class _Attention(nn.Module):
    # This class is written for the 1D case, but the forward method also takes in 3D inputs to allow for 3D rotary
    # position embeddings. For 1D usage, use the Attention1D class, for 3D usage, use the Attention3D class.
    """Performs attention (MHA, GQA, and MQA) on 1D sequences. {CLASS_DESCRIPTION_1D_DOC}"""

    _warn_relative_position_bias: bool = True
    _warn_mismatched_extra_tokens = True
    _input_dimensionality: Literal["1d", "3d"] = ...

    @populate_docstring
    def __init__(
        self,
        config: Attention1DConfig = {},
        relative_position_bias: RelativePositionEmbeddings | None = None,
        logit_scale: float | None = None,
        checkpointing_level: int = 0,
        **kwargs,
    ):
        """Initializes the Attention1D module.

        Args:
            config: {CONFIG_INSTANCE_DOC}
            relative_position_bias: Relative position embeddings to be considered during attention. Should be callable.
            logit_scale: Logit scale to be used for attention. If None, it will be initialized based on per-head
                dimension.
            checkpointing_level: {CHECKPOINTING_LEVEL_DOC}
            **kwargs: {CONFIG_KWARGS_DOC}
        """
        super().__init__()

        self.config = Attention1DConfig.model_validate(config | kwargs)

        self.W_q = nn.Linear(self.config.dim_qk, self.config.dim_qk)
        self.W_k = nn.Linear(self.config.dim_qk, self.config.dim_qk // self.config.ratio_q_to_kv_heads)
        self.W_v = nn.Linear(self.config.dim_v, self.config.dim_qk // self.config.ratio_q_to_kv_heads)
        self.proj = nn.Linear(self.config.dim_qk, self.config.dim_qk)
        self.proj_drop = nn.Dropout(self.config.proj_drop_prob)

        if logit_scale is None:
            self.logit_scale = nn.Parameter(
                torch.tensor([self.config.per_head_dim_qk**-0.5]),
                requires_grad=self.config.logit_scale_learnable,
            )
        else:
            self.logit_scale = logit_scale

        if _Attention._warn_relative_position_bias and relative_position_bias is not None:
            logger.warning(
                "Warning: Relative position bias is not used in Attention1D. "
                "Use Attention3D for relative position bias."
            )
        self.relative_position_bias = relative_position_bias

        self.rotary_position_embeddings = None
        if self.config.rotary_position_embeddings_config is not None:
            self.rotary_position_embeddings = RotaryPositionEmbeddings1D(self.config.rotary_position_embeddings_config)

        self.checkpointing_level1 = ActivationCheckpointing(1, checkpointing_level)
        self.checkpointing_level2 = ActivationCheckpointing(2, checkpointing_level)

    @populate_docstring
    def _forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        query_grid_shape: tuple[int, int, int] | None,
        key_grid_shape: tuple[int, int, int] | None,
    ):
        """Forward pass of the Attention1D module.

        Terminology: T => number of tokens, b => batch size

        Args:
            query: Tensor of shape (b, T_q, dim_qk) representing the input to the query matrix.
            key: Tensor of shape (b, T_kv, dim_qk) representing the input to the key matrix.
            value: Tensor of shape (b, T_kv, dim_v) representing the input to the value matrix.
            query_grid_shape: {ROTARY_POSITION_EMBEDDINGS_GRID_SHAPE_DOC}
            key_grid_shape: {ROTARY_POSITION_EMBEDDINGS_GRID_SHAPE_DOC}

        Returns:
            Tensor of shape (b, T_q, dim_qk) representing output tokens.
        """
        input_mode: Literal[
            "true_3d",  # input data is truly 3D
            "true_1d",  # input data is truly 1D
            "3d_as_1d",  # input data is 3D but is flattened and appears to be 1D
        ] = ...

        if query.ndim == 3:
            if self._input_dimensionality == "1d":
                input_mode = "true_1d"
            else:
                input_mode = "3d_as_1d"
                if self.rotary_position_embeddings is not None:
                    if query_grid_shape is None:
                        raise ValueError("query_grid_shape must be provided if 3D tokens are provided as 1D")
                    if key_grid_shape is None:
                        key_grid_shape = query_grid_shape

            forward_rearrange_partial = partial(rearrange, pattern="b T (num_heads d) -> b num_heads T d")
            backward_rearrange_partial = partial(rearrange, pattern="b num_heads T d -> b T (num_heads d)")
        elif query.ndim == 5:
            input_mode = "true_3d"

            z_q, y_q, x_q = query.shape[1:4]
            forward_rearrange_partial = partial(rearrange, pattern="b z y x (num_heads d) -> b num_heads (z y x) d")
            backward_rearrange_partial = partial(
                rearrange, pattern="b num_heads (z y x) d -> b z y x (num_heads d)", z=z_q, y=y_q, x=x_q
            )
        else:
            raise NotImplementedError

        def get_final_query_key_value(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
            """Computing query, key, and value tokens after passing to the weight matrices. Useful for activation
            checkpointing"""
            query = self.W_q(query)
            key = self.W_k(key)
            value = self.W_v(value)

            if self.rotary_position_embeddings is not None:
                kwargs = {}
                if input_mode in {"true_3d", "3d_as_1d"}:
                    kwargs["channels_first"] = False

                if input_mode in {"3d_as_1d"}:
                    # 3D tokens have been provided as 1D (probably to include class tokens etc.), so we need to
                    # rearrange them temporarily for rotary position embeddings

                    T_q = query.shape[1]
                    T_k = key.shape[1]
                    z_q, y_q, x_q = query_grid_shape
                    z_k, y_k, x_k = key_grid_shape
                    N_q = z_q * y_q * x_q
                    N_k = z_k * y_k * x_k

                    extra_query_tokens, query = query.split([T_q - N_q, N_q], dim=1)
                    extra_key_tokens, key = key.split([T_k - N_k, N_k], dim=1)

                    if self._warn_mismatched_extra_tokens and extra_query_tokens.shape[1] != extra_key_tokens.shape[1]:
                        logger.warning(
                            f"Query was provided with {extra_query_tokens.shape[1]} extra tokens, whereas key was "
                            f"provided with {extra_key_tokens.shape[1]} extra tokens. This may fail silently. Please "
                            "ensure this is as expected."
                        )
                        self._warn_mismatched_extra_tokens = False  # Warn only once

                    query = rearrange(
                        query, "b (z_q y_q x_q) d -> b z_q y_q x_q d", z_q=z_q, y_q=y_q, x_q=x_q
                    ).contiguous()
                    key = rearrange(key, "b (z_k y_k x_k) d -> b z_k y_k x_k d", z_k=z_k, y_k=y_k, x_k=x_k).contiguous()

                query = self.rotary_position_embeddings(query, **kwargs)
                key = self.rotary_position_embeddings(key, **kwargs)

                if input_mode in {"3d_as_1d"}:
                    # Revert back to the original structure
                    query = torch.cat(
                        [
                            extra_query_tokens,
                            rearrange(
                                query, "b z_q y_q x_q d -> b (z_q y_q x_q) d", z_q=z_q, y_q=y_q, x_q=x_q
                            ).contiguous(),
                        ],
                        dim=1,
                    )
                    key = torch.cat(
                        [
                            extra_key_tokens,
                            rearrange(
                                key, "b z_k y_k x_k d -> b (z_k y_k x_k) d", z_k=z_k, y_k=y_k, x_k=x_k
                            ).contiguous(),
                        ],
                        dim=1,
                    )

            query = forward_rearrange_partial(query, num_heads=self.config.num_heads).contiguous()
            key = forward_rearrange_partial(key, num_heads=self.config.num_kv_heads).contiguous()
            value = forward_rearrange_partial(value, num_heads=self.config.num_kv_heads).contiguous()
            # query: (b, num_heads, T, per_head_dim)
            # key: (b, num_kv_heads, T, per_head_dim)
            # value: (b, num_kv_heads, T, per_head_dim)

            if isinstance(self.logit_scale, nn.Module):
                logit_scale = self.logit_scale()
            else:
                logit_scale = self.logit_scale

            query_normalized = F.normalize(query, dim=-1)
            key_normalized = F.normalize(key, dim=-1)

            query_normalized_and_scaled = query_normalized * logit_scale  # Scale the query beforehand

            return query_normalized_and_scaled, key_normalized, value

        query_normalized_and_scaled, key_normalized, value = self.checkpointing_level1(
            get_final_query_key_value, query, key, value
        )

        relative_position_bias = None
        if self.relative_position_bias is not None:
            relative_position_bias = self.relative_position_bias()

        # Split tensors into batches and perform attention
        output = []
        chunk_size = self.config.max_attention_batch_size
        if chunk_size == -1:
            chunk_size = query_normalized_and_scaled.size(0)
        for query_normalized_and_scaled_chunk, key_normalized_chunk, value_chunk in zip(
            torch.split(query_normalized_and_scaled, chunk_size, dim=0),
            torch.split(key_normalized, chunk_size, dim=0),
            torch.split(value, chunk_size, dim=0),
        ):
            torch250plus_kwargs = {}
            if torch.__version__ >= "2.5":
                torch250plus_kwargs["enable_gqa"] = self.config.gqa_mqa_enabled

            output_chunk = F.scaled_dot_product_attention(
                query_normalized_and_scaled_chunk,
                key_normalized_chunk,
                value_chunk,
                attn_mask=relative_position_bias,  # Use this as a way to introduce relative position bias
                dropout_p=self.config.attn_drop_prob,
                is_causal=False,
                scale=1.0,  # Already scaled the vectors
                **torch250plus_kwargs,
            )
            output.append(output_chunk)
            # (chunk_size, num_heads, T, per_head_dim)
        output = torch.cat(output, dim=0)
        # (b, num_heads, T, per_head_dim)

        output = backward_rearrange_partial(output).contiguous()
        # (b, T, dim_qk)

        def get_final_output(output):
            """Computing final output after projection. Useful for activation checkpointing"""
            output = self.proj(output)
            output = self.proj_drop(output)
            return output

        output = self.checkpointing_level1(get_final_output, output)
        # (b, T, dim_qk)

        return output

    @wraps(_forward)
    def forward(self, *args, **kwargs):
        return self.checkpointing_level2(self._forward, *args, **kwargs)

# %% ../../nbs/layers/01_attention.ipynb #82ce8250
class Attention1D(_Attention):
    _input_dimensionality: Literal["1d", "3d"] = "1d"

    def _forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
        """Forward pass of the Attention1D module.

        Terminology: T => number of tokens, b => batch size

        Args:
            query: Tensor of shape (b, T_q, dim_qk) representing the input to the query matrix.
            key: Tensor of shape (b, T_kv, dim_qk) representing the input to the key matrix.
            value: Tensor of shape (b, T_kv, dim_v) representing the input to the value matrix.

        Returns:
            Tensor of shape (b, T_q, dim_qk) representing output tokens.
        """
        return super()._forward(query, key, value, None, None)

# %% ../../nbs/layers/01_attention.ipynb #760eb158
@populate_docstring
class Attention3D(_Attention):
    """Performs attention (MHA, GQA, and MQA) on 3D sequences. {CLASS_DESCRIPTION_3D_DOC}"""

    _warn_relative_position_bias: bool = False
    _input_dimensionality: Literal["1d", "3d"] = "3d"

    def __init__(
        self,
        config: Attention3DConfig = {},
        relative_position_bias: RelativePositionEmbeddings | None = None,
        logit_scale: float | None = None,
        checkpointing_level: int = 0,
        **kwargs
    ):
        self.config = Attention3DConfig.model_validate(config | kwargs)

        rotary_position_embeddings_config = self.config.rotary_position_embeddings_config
        self.config.rotary_position_embeddings_config = None
        super().__init__(self.config, relative_position_bias, logit_scale, checkpointing_level)
        self.config.rotary_position_embeddings_config = rotary_position_embeddings_config

        self.rotary_position_embeddings = None
        if self.config.rotary_position_embeddings_config is not None:
            self.rotary_position_embeddings = RotaryPositionEmbeddings3D(self.config.rotary_position_embeddings_config)

    @populate_docstring
    def _forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        channels_first: bool = True,
        query_grid_shape: tuple[int, int, int] | None = None,
        key_grid_shape: tuple[int, int, int] | None = None,
    ):
        """Forward pass of the Attention3D module.

        Terminology: z => depth, y => height, x => width, b => batch size

        Args:
            query: Tensor of shape (b, [dim_qk], z_q, y_q, x_q, [dim_qk]) or (b, T_q, dim_qk) representing the input to
                the query matrix.
            key: Tensor of shape (b, [dim_qk], z_kv, y_kv, x_kv, [dim_qk]) or (b, T_kv, dim_qk) representing the input
                to the key matrix.
            value: Tensor of shape (b, [dim_v], z_kv, y_kv, x_kv, [dim_v]) or (b, T_kv, dim_v) representing the input
                to the value matrix.
            channels_first: {CHANNELS_FIRST_DOC}
            query_grid_shape: {ROTARY_POSITION_EMBEDDINGS_GRID_SHAPE_DOC}
            key_grid_shape: {ROTARY_POSITION_EMBEDDINGS_GRID_SHAPE_DOC}

        Returns:
            Tensor of shape (b, [dim_qk], z_q, y_q, x_q, [dim_qk]) or (b, T_q, dim_qk) representing output tokens.
        """
        if query.ndim == 3:
            # Each is (b, T, d)
            pass
        elif query.ndim == 5:
            query = rearrange_channels(query, channels_first, False)
            key = rearrange_channels(key, channels_first, False)
            value = rearrange_channels(value, channels_first, False)
            # Each is now (b, z, y, x, d)
        else:
            raise ValueError("Input tensors must have 3 or 5 dimensions")

        output = super()._forward(query, key, value, query_grid_shape, key_grid_shape)
        # (b, z, y, x, d)

        if output.ndim == 5:
            output = rearrange_channels(output, False, channels_first)
            # (b, [d], z, y, x, [d])

        return output

    @wraps(_forward)
    def forward(self, *args, **kwargs):
        return self.checkpointing_level2(self._forward, *args, **kwargs)
