# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/metrics/01_detection.ipynb.

# %% auto 0
__all__ = ['map_mar', 'mean_average_precision_recall', 'MeanAveragePrecisionRecall', 'mean_average_precision_mean_average_recall',
           'MeanAveragePrecisionMeanAverageRecall', 'MeanAveragePrecision', 'MeanAverageRecall']

# %% ../../nbs/metrics/01_detection.ipynb 2
from typing import Literal

import torch
from monai.data.box_utils import box_iou
from torchmetrics import Metric

# %% ../../nbs/metrics/01_detection.ipynb 6
def mean_average_precision_mean_average_recall(
    pred_bboxes: list[torch.Tensor],
    pred_objectness_probabilities: list[torch.Tensor] | None,
    pred_class_probabilities: list[torch.Tensor],
    target_bboxes: list[torch.Tensor],
    target_classes: list[torch.Tensor],
    iou_thresholds: list[float] = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95],
    average_precision_num_points: int = 101,
    min_confidence_threshold: float = 0.0,
    max_bboxes_per_image: int | None = 100,
    return_intermediates: bool = False,
) -> tuple[float, float] | tuple[float, float, dict[float, dict[int, float]], dict[float, dict[int, float]]]:
    """Calculate the COCO mean average precision (mAP) for object detection.

    Args:
        pred_bboxes: A list of length B containing tensors of shape (NP, 4) or (NP, 6) containing the predicted bounding
            box parameters in xyxy or xyzxyz format.
        pred_objectness_probabilities: A list of length B containing tensors of shape (NP,) containing the predicted
            objectness probabilities for the corresponding bounding boxes. This can be set to None in which case only
            the class probabilities are considered.
        pred_class_probabilities: A list of length B containing tensors of shape (NP, num_classes) containing the
            predicted class probabilities for the corresponding bounding boxes.
        target_bboxes: A list of length B containing tensors of shape (NT, 4) or (NT, 6) containing the target bounding
            box parameters in xyxy or xyzxyz format.
        target_classes: A list of length B containing tensors of shape (NT,) containing the target class labels for the
            objects in the image.
        iou_thresholds: A list of IoU thresholds to use for calculating mAP and mAR.
        average_precision_num_points: Number of points over which to calculate average precision.
        min_confidence_threshold: Minimum confidence probability threshold to consider a prediction.
        max_bboxes_per_image: Maximum number of bounding boxes to consider per image. If more are present, only the top
            `max_bboxes_per_image` boxes based on confidence scores are considered. If set to None, all bounding boxes
            are considered.
        return_intermediates: If True, return intermediate values used to calculate mAP and mAR.

    Returns:
        The mean average precision (mAP) and mean average recall (mAR) across all classes and IoU thresholds for the
        entire dataset.
        If `return_intermediates` is True, also returns two dictionaries containing the average precision and average
        recall for each class at each IoU threshold.
    """
    # Set some globaly used variables
    B = len(pred_bboxes)
    num_classes = pred_class_probabilities[0].shape[-1]

    if pred_objectness_probabilities is None:
        pred_objectness_probabilities = [
            torch.ones_like(pred_class_probability[:, 0]) for pred_class_probability in pred_class_probabilities
        ]

    # Some basic tests
    assert (
        len(pred_bboxes)
        == len(pred_objectness_probabilities)
        == len(pred_class_probabilities)
        == len(target_bboxes)
        == len(target_classes)
        == B
    ), (
        f"All input lists must have the same length. Got lengths: {len(pred_bboxes)}, "
        f"{len(pred_objectness_probabilities)}, {len(pred_class_probabilities)}, {len(target_bboxes)}, "
        f"{len(target_classes)}."
    )
    assert all(
        pred_bbox.shape[0] == pred_objectness_probability.shape[0] == pred_class_probability.shape[0]
        for pred_bbox, pred_objectness_probability, pred_class_probability in zip(
            pred_bboxes, pred_objectness_probabilities, pred_class_probabilities
        )
    ), "Each prediction input list element must have the same number of bounding boxes."
    assert all(
        pred_bbox.shape[1] == 4 or pred_bbox.shape[1] == 6 for pred_bbox in pred_bboxes
    ), "Prediction bounding boxes must have shape (NP, 4) or (NP, 6)."
    assert all(
        pred_class_probability.shape[1] == num_classes for pred_class_probability in pred_class_probabilities
    ), "Prediction class probabilities must have shape (NP, num_classes)."
    assert all(
        target_bbox.shape[0] == target_class.shape[0]
        for target_bbox, target_class in zip(target_bboxes, target_classes)
    ), "Each target must have the same number of bounding boxes."

    # Split everything based on different classes
    pred_bboxes_by_class = [[] for _ in range(num_classes)]
    pred_objectness_probabilities_by_class = [[] for _ in range(num_classes)]
    pred_class_probabilities_by_class = [[] for _ in range(num_classes)]
    target_bboxes_by_class = [[] for _ in range(num_classes)]
    for b in range(B):
        pred_classes = torch.argmax(pred_class_probabilities[b], dim=-1)
        # (NP,)
        for c in range(num_classes):
            pred_classes_mask = pred_classes == c
            # (NP,)
            target_classes_mask = target_classes[b] == (c + 1)
            # (NT,)

            pred_bboxes_by_class[c].append(pred_bboxes[b][pred_classes_mask])
            pred_objectness_probabilities_by_class[c].append(pred_objectness_probabilities[b][pred_classes_mask])
            pred_class_probabilities_by_class[c].append(pred_class_probabilities[b][pred_classes_mask])
            # (NP,)

            target_bboxes_by_class[c].append(target_bboxes[b][target_classes_mask])
            # (NT,)

    # Helper function to sort a tensor in descending order based on values in first column
    def _sort_by_first_column_descending(tensor: torch.Tensor) -> torch.Tensor:
        return tensor[torch.argsort(tensor[:, 0], descending=True)]

    # Calculate IOUs for all prediction and target bounding box pairs for each class
    # Also calculate confidence scores for each prediction bounding box along with index
    # Also track number of target boxes for each class
    ious = [[] for _ in range(num_classes)]
    confidence_scores = [[] for _ in range(num_classes)]
    num_target_boxes = [0 for _ in range(num_classes)]
    for b in range(B):
        for c in range(num_classes):
            _ious = box_iou(pred_bboxes_by_class[c][b], target_bboxes_by_class[c][b])
            # (NP, NT)

            _confidence_scores = (
                pred_objectness_probabilities_by_class[c][b] * pred_class_probabilities_by_class[c][b][:, c]
            )
            _batch_index = torch.full_like(_confidence_scores, b)
            _offset_index = torch.arange(len(_confidence_scores), device=_confidence_scores.device)
            _confidence_scores = torch.stack([_confidence_scores, _batch_index, _offset_index], dim=-1)
            # (NP, 3) -> (confidence_score, batch_index, offset_index)

            ious[c].append(_ious)
            confidence_scores[c].append(_confidence_scores)
            num_target_boxes[c] += target_bboxes_by_class[c][b].shape[0]

        # Limit number of bounding boxes per image if applicable
        if max_bboxes_per_image is not None:
            all_confidences = torch.cat(
                [
                    torch.cat(
                        [
                            confidence_scores[c][b],  # (NC, 3)
                            torch.full_like(confidence_scores[c][b][:, :1], c),  # (NC, 1)
                        ],
                        dim=-1,
                    )  # (NC, 4)
                    for c in range(num_classes)
                ],
                dim=0,
            )
            if all_confidences.shape[0] > max_bboxes_per_image:
                all_confidences = _sort_by_first_column_descending(all_confidences)
                topk_confidences = all_confidences[:max_bboxes_per_image]
                # (max_bboxes_per_image, 4)
                for c in range(num_classes):
                    class_mask = topk_confidences[:, 3] == c
                    # (max_bboxes_per_image,)
                    confidence_scores[c][b] = topk_confidences[class_mask][:, :3]
                    # (N_class, 3)

    # Concatenate confidence scores and sort them in descending order for each class
    for c in range(num_classes):
        confidence_scores[c] = torch.cat(confidence_scores[c], dim=0)
        confidence_scores[c] = _sort_by_first_column_descending(confidence_scores[c])

    # For each IOU threshold, calculate average precision and average recall
    average_precisions = {}
    average_recalls = {}
    for iou_threshold in iou_thresholds:
        # For each class calculate average precision and average recall
        class_average_precisions = {}
        class_average_recalls = {}
        for c in range(num_classes):
            if num_target_boxes[c] == 0:
                # If no target boxes for this class, skip it
                class_average_precisions[c + 1] = float("nan")
                class_average_recalls[c + 1] = float("nan")
                continue

            matched_target_indices = [set() for _ in range(B)]
            tps, fps, fns = 0, 0, num_target_boxes[c]
            precisions, recalls = [], []

            # In descending order, update tp, fp, fn and calculate precision and recall at each step
            for confidence_score, b, pred_offset in confidence_scores[c]:
                if confidence_score < min_confidence_threshold:
                    # Do not consider this and following predictions if confidence score is below threshold
                    break

                b, pred_offset = int(b), int(pred_offset)

                pred_ious = ious[c][b][pred_offset]
                # (NT,)

                if pred_ious.numel() > 0:
                    if matched_target_indices[b]:
                        # Exclude already matched target boxes
                        pred_ious[list(matched_target_indices[b])] = -1.0
                    # Exclude target boxes below IOU threshold
                    pred_ious[pred_ious < iou_threshold] = -1.0

                if pred_ious.numel() == 0 or pred_ious.amax() < 0.0:
                    # No valid target box to match with
                    fps += 1
                else:
                    target_offset = pred_ious.argmax().item()
                    matched_target_indices[b].add(target_offset)
                    tps += 1
                    fns -= 1

                precisions.append(tps / (tps + fps) if (tps + fps) > 0 else 1.0)
                recalls.append(tps / (tps + fns) if (tps + fns) > 0 else 0.0)

            precisions = torch.tensor(precisions)
            recalls = torch.tensor(recalls)

            # Precision envelope: P_interp(r) = max_{r' >= r} P(r')
            enveloped_precisions = precisions.clone()
            for i in range(len(enveloped_precisions) - 2, -1, -1):
                if enveloped_precisions[i] < enveloped_precisions[i + 1]:
                    enveloped_precisions[i] = enveloped_precisions[i + 1]

            # Calculate average precision using step-wise interpolation
            recall_samples = torch.linspace(0, 1, average_precision_num_points, device=recalls.device)
            idxs = torch.searchsorted(recalls, recall_samples, side="left")
            valid = idxs < enveloped_precisions.numel()
            enveloped_precisions_at_t = torch.zeros_like(recall_samples)
            enveloped_precisions_at_t[valid] = enveloped_precisions[idxs[valid]]
            class_average_precisions[c + 1] = enveloped_precisions_at_t.mean().item()

            # Calculate average recall i.e. maximum recall achieved at this IoU threshold
            class_average_recalls[c + 1] = recalls.max().item() if recalls.numel() > 0 else 0.0

        average_precisions[iou_threshold] = class_average_precisions
        average_recalls[iou_threshold] = class_average_recalls

    map_metric = torch.nanmean(
        torch.stack([torch.tensor(ap) for iou_aps in average_precisions.values() for ap in iou_aps.values()])
    ).item()
    mar_metric = torch.nanmean(
        torch.stack([torch.tensor(ar) for iou_ars in average_recalls.values() for ar in iou_ars.values()])
    ).item()

    if return_intermediates:
        return map_metric, mar_metric, average_precisions, average_recalls
    return map_metric, mar_metric


# Create aliases
map_mar = mean_average_precision_mean_average_recall
mean_average_precision_recall = mean_average_precision_mean_average_recall

# %% ../../nbs/metrics/01_detection.ipynb 12
class MeanAveragePrecisionMeanAverageRecall(Metric):
    """Calculate the COCO mean average precision (mAP) and mean average recall (mAR) for object detection."""

    def __init__(
        self,
        iou_thresholds: list[float] = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95],
        average_precision_num_points: int = 101,
        min_confidence_threshold: float = 0.0,
        max_bboxes_per_image: int | None = 100,
    ):
        """Initialize the MeanAveragePrecisionMeanAverageRecall metric.

        Args:
            num_classes: Number of classes in the dataset.
            iou_thresholds: A list of IoU thresholds to use for calculating mAP and mAR.
            average_precision_num_points: Number of points over which to calculate average precision.
            min_confidence_threshold: Minimum confidence score threshold to consider a prediction.
            max_bboxes_per_image: Maximum number of bounding boxes to consider per image. If more are present, only the
                top `max_bboxes_per_image` boxes based on confidence scores are considered.
        """
        super().__init__()

        self.iou_thresholds = iou_thresholds
        self.average_precision_num_points = average_precision_num_points
        self.min_confidence_threshold = min_confidence_threshold
        self.max_bboxes_per_image = max_bboxes_per_image

        self.add_state("pred_bboxes", [], dist_reduce_fx=None, persistent=False)
        self.add_state("pred_objectness_probabilities", [], dist_reduce_fx=None, persistent=False)
        self.add_state("pred_class_probabilities", [], dist_reduce_fx=None, persistent=False)
        self.add_state("target_bboxes", [], dist_reduce_fx=None, persistent=False)
        self.add_state("target_classes", [], dist_reduce_fx=None, persistent=False)

    def update(
        self,
        pred_bboxes: list[torch.Tensor],
        pred_objectness_probabilities: list[torch.Tensor],
        pred_class_probabilities: list[torch.Tensor],
        target_bboxes: list[torch.Tensor],
        target_classes: list[torch.Tensor],
    ):
        self.pred_bboxes.extend(pred_bboxes)
        self.pred_objectness_probabilities.extend(pred_objectness_probabilities)
        self.pred_class_probabilities.extend(pred_class_probabilities)
        self.target_bboxes.extend(target_bboxes)
        self.target_classes.extend(target_classes)

    def compute(self):
        return mean_average_precision_mean_average_recall(
            self.pred_bboxes,
            self.pred_objectness_probabilities,
            self.pred_class_probabilities,
            self.target_bboxes,
            self.target_classes,
            iou_thresholds=self.iou_thresholds,
            average_precision_num_points=self.average_precision_num_points,
            min_confidence_threshold=self.min_confidence_threshold,
            max_bboxes_per_image=self.max_bboxes_per_image,
        )

    def forward(self, *args, return_metrics: Literal["map_only", "mar_only", "map_mar"] = "map_mar", **kwargs):
        map, mar = super().forward(*args, **kwargs)
        if return_metrics == "map_only":
            return map
        elif return_metrics == "mar_only":
            return mar
        return map, mar


# Aliases
MeanAveragePrecisionRecall = MeanAveragePrecisionMeanAverageRecall

# %% ../../nbs/metrics/01_detection.ipynb 14
class MeanAveragePrecision(MeanAveragePrecisionMeanAverageRecall):
    """Calculate the COCO mean average precision (mAP) for object detection."""

    def forward(self, *args, **kwargs):
        return super().forward(*args, return_metrics="map_only", **kwargs)

# %% ../../nbs/metrics/01_detection.ipynb 15
class MeanAverageRecall(MeanAveragePrecisionMeanAverageRecall):
    """Calculate the COCO mean average recall (mAR) for object detection."""

    def forward(self, *args, **kwargs):
        return super().forward(*args, return_metrics="mar_only", **kwargs)
